"""
ä½œè€…ï¼še.e.
å¾®ä¿¡ï¼šEnkidu_lin
æ—¥æœŸï¼š2025.08.24
"""

import time
import os
import sys
import logging
import numpy as np
import pandas as pd
from config import RECORDS_PATH, REGION_LIST
from machine_lib_ee import get_alphas, set_alpha_properties, batch_set_alpha_properties, load_user_config, load_digging_config
from session_client import get_session
from datetime import datetime, timedelta
from collections import defaultdict
import json
from alpha_record_manager import (
    is_alpha_in_records
)

# å¯¼å…¥æ•°æ®åº“ç®¡ç†å™¨
try:
    from database.db_manager import FactorDatabaseManager
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæ·»åŠ è·¯å¾„
    sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
    from database.db_manager import FactorDatabaseManager

# å¯¼å…¥ç›¸å…³æ€§æ£€æŸ¥å™¨ï¼ˆå·²ç§»åŠ¨åˆ°ç‹¬ç«‹è„šæœ¬ï¼‰
# from correlation_checker_independent import IndependentCorrelationChecker


brain_api_url = os.environ.get("BRAIN_API_URL", "https://api.worldquantbrain.com")

class OptimizedChecker:
    def __init__(self, mode=None, batch_size=50, sharpe_threshold=None, fitness_threshold=None, start_date=None):
        # é…ç½®ä¸“ç”¨çš„logger
        self.logger = logging.getLogger('check_optimized')
        
        # åŠ è½½é…ç½®
        self.config = load_digging_config()
        
        # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå…¶æ¬¡ä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œæœ€åä½¿ç”¨é»˜è®¤å€¼
        self.mode = mode or self.config.get('mode', "PPAC")
        
        # éªŒè¯å¹¶è®¾ç½®ç”¨æˆ·æŒ‡å®šçš„èµ·å§‹æ—¥æœŸ
        self.override_start_date = None
        if start_date:
            try:
                # éªŒè¯æ—¥æœŸæ ¼å¼
                datetime.strptime(start_date, '%Y-%m-%d')
                self.override_start_date = start_date
                self.logger.info(f"ğŸ“… ç”¨æˆ·æŒ‡å®šèµ·å§‹æ—¥æœŸ: {start_date}")
            except ValueError:
                self.logger.error(f"âŒ æ— æ•ˆçš„æ—¥æœŸæ ¼å¼: {start_date}ï¼ŒæœŸæœ›æ ¼å¼: YYYY-MM-DD")
                raise ValueError(f"æ— æ•ˆçš„æ—¥æœŸæ ¼å¼: {start_date}ï¼ŒæœŸæœ›æ ¼å¼: YYYY-MM-DD")
        
        # æ ¹æ®æ¨¡å¼è®¾ç½®é»˜è®¤é˜ˆå€¼
        mode_defaults = {
            "CONSULTANT": {"sharpe": 1.58, "fitness": 1.0},
            "USER": {"sharpe": 1.25, "fitness": None},
            "PPAC": {"sharpe": 1.0, "fitness": None}
        }
        
        default_config = mode_defaults.get(self.mode, mode_defaults["PPAC"])
        
        # é˜ˆå€¼ä¼˜å…ˆçº§ï¼šä¼ å…¥å‚æ•° > é…ç½®æ–‡ä»¶ > æ¨¡å¼é»˜è®¤å€¼
        self.sharpe_threshold = sharpe_threshold or self.config.get('sharpe_threshold', default_config["sharpe"])
        self.fitness_threshold = fitness_threshold or self.config.get('fitness_threshold', default_config["fitness"])
        
        self.batch_size = int(self.config.get('check_batch_size', batch_size))  # æ‰¹é‡å¤„ç†å¤§å°
        
        # ä»é…ç½®æ–‡ä»¶è¯»å–åŸºæœ¬é…ç½®
        self.enable_smart_delay = self.config.get('enable_smart_delay', True)  # å¯ç”¨æ™ºèƒ½å»¶è¿Ÿ
        self.smart_retry_enabled = self.config.get('smart_retry_enabled', True)  # æ™ºèƒ½é‡è¯•å¼€å…³
        self.exponential_backoff_max = int(self.config.get('exponential_backoff_max', 60))  # æŒ‡æ•°é€€é¿æœ€å¤§å»¶è¿Ÿ
        
        self.session = None
        
        self.api_delay = float(self.config.get('api_retry_delay', 1))
        self.max_retries = int(self.config.get('api_max_retries', 3))
        
        # æ‰“å°é…ç½®ä¿¡æ¯
        self.logger.info(f"âœ… ç®€åŒ–ç‰ˆæ£€æŸ¥å™¨é…ç½®:")
        self.logger.info(f"  ğŸ¯ æ£€æŸ¥æ¨¡å¼: {self.mode}")
        self.logger.info(f"  ğŸ“Š Sharpeé˜ˆå€¼: {self.sharpe_threshold}")
        self.logger.info(f"  ğŸ“ˆ Fitnessé˜ˆå€¼: {self.fitness_threshold if self.fitness_threshold is not None else 'ä¸ä½¿ç”¨'}")
        self.logger.info(f"  ğŸ“Š æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        self.logger.info(f"  ğŸ§  æ™ºèƒ½å»¶è¿Ÿ: {'å¯ç”¨' if self.enable_smart_delay else 'ç¦ç”¨'}")
        self.logger.info(f"  ğŸ”„ æ™ºèƒ½é‡è¯•: {'å¯ç”¨' if self.smart_retry_enabled else 'ç¦ç”¨'}")
        self.logger.info(f"  â° æŒ‡æ•°é€€é¿ä¸Šé™: {self.exponential_backoff_max}s")
        
    def initialize_session(self):
        """åˆå§‹åŒ–ä¼šè¯ï¼ˆä½¿ç”¨ç»Ÿä¸€ä¼šè¯ç®¡ç†å™¨ï¼‰"""
        if self.session is None:
            try:
                self.session = get_session()
                self.logger.info("âœ… ä¼šè¯åˆå§‹åŒ–å®Œæˆ (ä½¿ç”¨ç»Ÿä¸€ä¼šè¯ç®¡ç†å™¨)")
            except Exception as e:
                self.logger.error(f"âŒ ç»Ÿä¸€ä¼šè¯ç®¡ç†å™¨å¤±è´¥: {e}")
                # ä½¿ç”¨SessionClient
                try:
                    from session_client import get_session
                    self.session = get_session()
                    self.logger.info("âœ… ä¼šè¯åˆå§‹åŒ–å®Œæˆ (ä½¿ç”¨SessionClient)")
                except Exception as e2:
                    self.logger.error(f"âŒ SessionClientå¤±è´¥: {e2}")
                    self.logger.error("ğŸ’¡ è¯·ç¡®ä¿SessionKeeperæ­£åœ¨è¿è¡Œå¹¶ç»´æŠ¤æœ‰æ•ˆä¼šè¯")
                    raise

    def batch_check_alphas(self, alphas, submitable_alpha_file):
        """æ‰¹é‡æ£€æŸ¥Alpha - æµå¼å¤„ç†æ¨¡å¼"""
        self.logger.info(f"\nğŸ” å¼€å§‹æµå¼æ£€æŸ¥ {len(alphas)} ä¸ªAlpha...")
        
        # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        db_path = os.path.join(os.path.dirname(RECORDS_PATH), 'database', 'factors.db')
        db = FactorDatabaseManager(db_path)
        
        # 1. è¿‡æ»¤å·²æ£€æŸ¥çš„Alpha
        # æ³¨æ„ï¼šé¢œè‰²çŠ¶æ€è¿‡æ»¤å·²åœ¨APIè·å–é˜¶æ®µç»Ÿä¸€å¤„ç†ï¼Œè¿™é‡Œåªéœ€å¤„ç†æ£€æŸ¥è®°å½•
        valid_alphas = []
        skipped_checked = 0
        
        for alpha in alphas:
            alpha_id = alpha['id']
            tags = alpha['tags']
            tag = tags[0] if len(tags) == 1 else ''
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»æ£€æŸ¥è¿‡ - ä½¿ç”¨alpha_record_manager
            if is_alpha_in_records(alpha_id, tag, "checked"):
                skipped_checked += 1
                self.logger.info(f"  â­ï¸  Alpha {alpha_id}: å·²æ£€æŸ¥è¿‡ï¼Œè·³è¿‡")
                continue
                
            valid_alphas.append(alpha)
            self.logger.info(f"  ğŸ“ Alpha {alpha_id}: å¾…æ£€æŸ¥ (æ ‡ç­¾: {tag})")
        
        self.logger.info(f"\nğŸ“Š ä¸šåŠ¡é€»è¾‘è¿‡æ»¤ç»Ÿè®¡:")
        self.logger.info(f"  â­ï¸  å·²æ£€æŸ¥è·³è¿‡: {skipped_checked} ä¸ª")
        self.logger.info(f"  âœ… éœ€è¦æ£€æŸ¥: {len(valid_alphas)} ä¸ª")
        self.logger.info(f"  ğŸ’¡ é¢œè‰²çŠ¶æ€è¿‡æ»¤å·²åœ¨APIè·å–é˜¶æ®µç»Ÿä¸€å¤„ç†")
        
        if not valid_alphas:
            self.logger.info(f"ğŸ“ æ²¡æœ‰éœ€è¦æ£€æŸ¥çš„Alpha")
            return
        
        # 2. æµå¼å¤„ç†ï¼šåˆ†æ‰¹è·å–ç›¸å…³æ€§å¹¶ç«‹å³å¤„ç†ç»“æœ
        self.logger.info(f"\nğŸ”„ å¼€å§‹æµå¼å¤„ç†æ¨¡å¼...")
        batch_num = 0
        
        # æŒ‰æ‰¹æ¬¡å¤„ç†Alpha
        for i in range(0, len(valid_alphas), self.batch_size):
            batch_alphas = valid_alphas[i:i + self.batch_size]
            batch_num += 1
            
            try:
                self.logger.info(f"\n{'='*60}")
                self.logger.info(f"ğŸ“¦ æµå¼å¤„ç†æ‰¹æ¬¡ {batch_num}: {len(batch_alphas)} ä¸ªAlpha")
                self.logger.info(f"ğŸ“‹ æ‰¹æ¬¡è¿›åº¦: {i+1}-{min(i+len(batch_alphas), len(valid_alphas))}/{len(valid_alphas)}")
                self.logger.info(f"{'='*60}")
                
                # å¤„ç†å½“å‰æ‰¹æ¬¡ï¼ˆä¸å†ä¾èµ–è¿”å›å€¼ï¼‰
                self.process_alpha_batch(batch_alphas, submitable_alpha_file, batch_num)
                
                self.logger.info(f"\nğŸ“Š æ‰¹æ¬¡ {batch_num} å¤„ç†å®Œæˆ")
                
                # æ‰¹æ¬¡é—´ä¼‘æ¯
                if i + self.batch_size < len(valid_alphas):
                    batch_delay = 3.0
                    self.logger.info(f"\nâ¸ï¸  æ‰¹æ¬¡é—´ä¼‘æ¯...")
                    for remaining in range(int(batch_delay), 0, -1):
                        progress = f"[æ‰¹æ¬¡{batch_num+1}å‡†å¤‡ä¸­] ä¼‘æ¯ {remaining} ç§’..."
                        print(f"  {progress}", end='\r', flush=True)
                        time.sleep(1)
                    print(f"                                          ", end='\r')
                
            except KeyboardInterrupt:
                self.logger.info(f"\nâš ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†ï¼Œå·²å¤„ç† {batch_num} ä¸ªæ‰¹æ¬¡")
                raise
            except Exception as e:
                self.logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} å¤„ç†å¼‚å¸¸: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        # æœ€ç»ˆç»Ÿè®¡
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"ğŸ‰ æµå¼æ£€æŸ¥å®Œæˆ:")
        self.logger.info(f"  ğŸ“Š å¤„ç†æ‰¹æ¬¡: {batch_num} ä¸ª")
        self.logger.info(f"  ğŸ“Š å¤„ç†Alpha: {len(valid_alphas)} ä¸ª")
        self.logger.info(f"  ğŸ”„ APIå¤±è´¥çš„Alphaå°†åœ¨ä¸‹è½®å¾ªç¯ä¸­é‡è¯•")
        self.logger.info(f"{'='*60}")

    def process_alpha_batch(self, batch_alphas, submitable_alpha_file, batch_num):
        """å¤„ç†å•ä¸ªAlphaæ‰¹æ¬¡çš„ç›¸å…³æ€§æ£€æŸ¥å’Œç»“æœä¿å­˜"""
        self.logger.info(f"ğŸ” ç¬¬ä¸‰é˜¶æ®µ: å¤„ç†æ‰¹æ¬¡ {batch_num} ({len(batch_alphas)} ä¸ªAlpha)...")
        
        # åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        db_path = os.path.join(os.path.dirname(RECORDS_PATH), 'database', 'factors.db')
        db = FactorDatabaseManager(db_path)
        
        submitable_alphas = []
        # ç®€åŒ–ååªæœ‰å¯æäº¤çš„Alphaï¼Œç›¸å…³æ€§æ£€æµ‹ç”±submitå¤„ç†
        
        for alpha in batch_alphas:
            alpha_id = alpha['id']
            # è®¾ç½®é»˜è®¤ç›¸å…³æ€§å€¼ï¼Œè¡¨ç¤ºéœ€è¦submitæ£€æŸ¥
            alpha['self_corr'] = 999.0  # è¡¨ç¤ºéœ€è¦æ£€æŸ¥è‡ªç›¸å…³æ€§
            alpha['prod_corr'] = 999.0  # è¡¨ç¤ºéœ€è¦æ£€æŸ¥ç”Ÿäº§ç›¸å…³æ€§
            alpha['aggressive_mode'] = False  # ä¸å†ä½¿ç”¨æ­¤å­—æ®µåˆ¤æ–­
            # è®¾ç½®colorä¸ºYELLOWï¼Œè¡¨ç¤ºç­‰å¾…ç›¸å…³æ€§æ£€æŸ¥
            alpha['color'] = 'YELLOW'
            submitable_alphas.append(alpha)
        
        # ç«‹å³ä¿å­˜ç»“æœ
        self.logger.info(f"\nğŸ’¾ ç¬¬å››é˜¶æ®µ: ä¿å­˜æ‰¹æ¬¡ç»“æœ...")
        self.logger.info(f"  âœ… å¯æäº¤: {len(submitable_alphas)} ä¸ª")
        self.logger.info(f"  ğŸ“ æ³¨æ„: ç›¸å…³æ€§æ£€æµ‹å·²å§”æ‰˜ç»™submitå¤„ç†")
        
        # 1. æ›´æ–°å¯æäº¤Alphaæ•°æ®åº“
        if submitable_alphas:
            self.logger.info(f"    ğŸ“Š å‡†å¤‡æ›´æ–°å¯æäº¤Alphaæ•°æ®åº“...")
            
            # è¿‡æ»¤å·²å­˜åœ¨çš„å› å­ï¼Œé¿å…é‡å¤æ’å…¥
            new_submitable_alphas = []
            skipped_existing = 0
            
            for alpha in submitable_alphas:
                alpha_id = alpha['id']
                if db.is_alpha_submitable(alpha_id):
                    skipped_existing += 1
                    self.logger.info(f"    â­ï¸  Alpha {alpha_id}: å·²å­˜åœ¨äºå¯æäº¤æ•°æ®åº“ä¸­ï¼Œè·³è¿‡æ’å…¥")
                else:
                    new_submitable_alphas.append(alpha)
            
            self.logger.info(f"    ğŸ“Š é‡å¤æ£€æŸ¥ç»“æœ: {len(submitable_alphas)} ä¸ªæ£€æŸ¥é€šè¿‡ï¼Œ{skipped_existing} ä¸ªå·²å­˜åœ¨ï¼Œ{len(new_submitable_alphas)} ä¸ªéœ€è¦æ’å…¥")
            
            # ä½¿ç”¨æ•°æ®åº“äº‹åŠ¡ä¿è¯æ•°æ®ä¸€è‡´æ€§
            try:
                if new_submitable_alphas:
                    # è½¬æ¢ä¸ºDataFrameæ ¼å¼
                    submitable_df = pd.DataFrame(new_submitable_alphas)
                    
                    # æ‰¹é‡æ·»åŠ åˆ°æ•°æ®åº“
                    success_count = 0
                    for _, row in submitable_df.iterrows():
                        alpha_data = row.to_dict()
                        
                        # å­—æ®µåæ˜ å°„ï¼šAPIé©¼å³°å‘½å -> æ•°æ®åº“ä¸‹åˆ’çº¿å‘½å
                        field_mapping = {
                            'id': 'alpha_id',
                            'instrumentType': 'instrument_type',
                            'unitHandling': 'unit_handling', 
                            'nanHandling': 'nan_handling',
                            'operatorCount': 'operator_count',
                            'dateCreated': 'date_created',
                            'dateSubmitted': 'date_submitted', 
                            'dateModified': 'date_modified',
                            'bookSize': 'book_size',
                            'longCount': 'long_count',
                            'shortCount': 'short_count',
                            'startDate': 'start_date'
                        }
                        
                        # åº”ç”¨å­—æ®µåæ˜ å°„
                        for old_name, new_name in field_mapping.items():
                            if old_name in alpha_data:
                                alpha_data[new_name] = alpha_data.pop(old_name)
                        
                        # åªä¿ç•™æ ¸å¿ƒå­—æ®µï¼Œé¿å…å­˜å‚¨è¿‡å¤šå¤æ‚æ•°æ®
                        core_fields = {
                            'alpha_id', 'type', 'author', 'instrument_type', 'region', 'universe',
                            'delay', 'decay', 'neutralization', 'truncation', 'pasteurization',
                            'unit_handling', 'nan_handling', 'language', 'visualization', 'code',
                            'description', 'operator_count', 'date_created', 'date_submitted',
                            'date_modified', 'name', 'favorite', 'hidden', 'color', 'category',
                            'tags', 'grade', 'stage', 'status', 'pnl', 'book_size', 'long_count',
                            'short_count', 'turnover', 'returns', 'drawdown', 'margin', 'fitness',
                            'sharpe', 'start_date', 'aggressive_mode', 'self_corr', 'prod_corr'
                        }
                        
                        # è¿‡æ»¤å­—æ®µ
                        filtered_data = {}
                        for key, value in alpha_data.items():
                            if key in core_fields:
                                filtered_data[key] = value
                        
                        # æ•°æ®ç±»å‹è½¬æ¢ï¼šå¤æ‚å¯¹è±¡è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                        complex_fields = ['tags']  # åªå¤„ç†tagså­—æ®µï¼Œå…¶ä»–å¤æ‚å­—æ®µå·²è¿‡æ»¤æ‰
                        for field in complex_fields:
                            if field in filtered_data and filtered_data[field] is not None:
                                if isinstance(filtered_data[field], (list, dict)):
                                    filtered_data[field] = json.dumps(filtered_data[field], ensure_ascii=False)
                                elif not isinstance(filtered_data[field], str):
                                    filtered_data[field] = str(filtered_data[field])
                        
                        # å¤„ç†Noneå€¼å’Œå¸ƒå°”å€¼
                        for key in list(filtered_data.keys()):
                            if filtered_data[key] is None:
                                filtered_data[key] = ''
                            elif isinstance(filtered_data[key], bool):
                                filtered_data[key] = 1 if filtered_data[key] else 0
                            elif isinstance(filtered_data[key], (int, float)):
                                filtered_data[key] = filtered_data[key]
                            else:
                                filtered_data[key] = str(filtered_data[key])
                        
                        if db.add_submitable_alpha(filtered_data):
                            success_count += 1
                    
                    if success_count == len(new_submitable_alphas):
                        self.logger.info(f"    ğŸ“Š æ•°æ®åº“æ›´æ–°æˆåŠŸ: æ·»åŠ äº† {success_count} ä¸ªæ–°çš„å¯æäº¤Alpha")
                        
                        # è·å–å½“å‰æ•°æ®åº“ä¸­çš„æ€»æ•°
                        current_df = db.get_submitable_alphas()
                        total_count = len(current_df)
                        self.logger.info(f"    ğŸ“Š æ•°æ®åº“ä¸­å½“å‰å…±æœ‰ {total_count} ä¸ªå¯æäº¤Alpha")
                    else:
                        self.logger.info(f"    âš ï¸  éƒ¨åˆ†æ•°æ®åº“æ›´æ–°å¤±è´¥: æˆåŠŸ {success_count}/{len(new_submitable_alphas)}")
                else:
                    self.logger.info(f"    ğŸ“ æ‰€æœ‰å› å­éƒ½å·²å­˜åœ¨äºæ•°æ®åº“ä¸­ï¼Œæ— éœ€æ’å…¥æ–°æ•°æ®")
                    
            except Exception as e:
                self.logger.info(f"    âŒ æ•°æ®åº“æ“ä½œå¼‚å¸¸: {e}")
                self.logger.info(f"    ğŸ”„ è·³è¿‡æœ¬æ‰¹æ¬¡å¯æäº¤Alphaä¿å­˜ï¼Œç­‰å¾…ä¸‹è½®é‡è¯•")
            
            # æ‰¹é‡è®¾ç½®Alphaä¸ºYELLOWï¼ˆåŒ…æ‹¬æ–°æ’å…¥çš„å’Œå·²å­˜åœ¨çš„å› å­ï¼‰
            # å¯¹äºå·²å­˜åœ¨çš„å› å­ï¼Œä¹Ÿéœ€è¦æ ‡è®°ä¸ºYELLOWï¼Œå› ä¸ºå¯èƒ½ä¹‹å‰æ ‡è®°å¤±è´¥äº†
            yellow_ids = [alpha['id'] for alpha in submitable_alphas]  # æ‰€æœ‰é€šè¿‡æ£€æŸ¥çš„å› å­
            if skipped_existing > 0:
                self.logger.info(f"    ğŸ¨ è®¾ç½® {len(yellow_ids)} ä¸ªAlphaä¸ºYELLOW (åŒ…æ‹¬ {len(new_submitable_alphas)} ä¸ªæ–°æ’å…¥çš„å’Œ {skipped_existing} ä¸ªå·²å­˜åœ¨çš„)...")
            else:
                self.logger.info(f"    ğŸ¨ è®¾ç½® {len(yellow_ids)} ä¸ªAlphaä¸ºYELLOW...")
            self.batch_set_alpha_properties(yellow_ids, color='YELLOW')
        
    def batch_set_alpha_properties(self, alpha_ids, **properties):
        """æ‰¹é‡è®¾ç½®Alphaå±æ€§ - æ™ºèƒ½é€‰æ‹©API"""
        if not alpha_ids:
            return 0, 0
            
        self.logger.info(f"      ğŸ¨ å¼€å§‹æ‰¹é‡è®¾ç½® {len(alpha_ids)} ä¸ªAlphaå±æ€§: {properties}")
        
        # æ£€æŸ¥æ˜¯å¦åªè®¾ç½®é¢œè‰²ï¼ˆæ‰¹é‡APIåªæ”¯æŒé¢œè‰²è®¾ç½®ï¼‰
        is_color_only = len(properties) == 1 and 'color' in properties
        
        if is_color_only:
            self.logger.info(f"      ğŸ“‹ æ£€æµ‹åˆ°ä»…è®¾ç½®é¢œè‰²ï¼Œä½¿ç”¨æ‰¹é‡API...")
            
            # å‡†å¤‡æ‰¹é‡APIæ•°æ®æ ¼å¼
            alpha_data = [{"id": alpha_id, "color": properties['color']} for alpha_id in alpha_ids]
            
            try:
                result = batch_set_alpha_properties(self.session, alpha_data, max_batch_size=50)
                
                success_count = result["success"]
                failed_count = result["failed"]
                
                self.logger.info(f"      ğŸ“Š æ‰¹é‡è®¾ç½®å®Œæˆ:")
                self.logger.info(f"        âœ… æˆåŠŸ: {success_count}/{len(alpha_ids)} ä¸ª")
                
                if failed_count > 0:
                    self.logger.info(f"        âŒ å¤±è´¥: {failed_count}/{len(alpha_ids)} ä¸ª")
                
                # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
                for detail in result["details"]:
                    self.logger.info(f"        ğŸ“‹ {detail}")
                
                return success_count, failed_count
                
            except Exception as e:
                self.logger.error(f"      âŒ æ‰¹é‡APIå¼‚å¸¸: {e}")
                self.logger.info(f"      ğŸ”„ å›é€€åˆ°å•ä¸ªè®¾ç½®æ¨¡å¼...")
                
                # å›é€€åˆ°å•ä¸ªè®¾ç½®
                return self._fallback_individual_set(alpha_ids, **properties)
        else:
            # è®¾ç½®äº†å…¶ä»–å±æ€§ï¼ˆnameã€tagsç­‰ï¼‰ï¼Œå¿…é¡»ä½¿ç”¨å•ä¸ªAPI
            self.logger.info(f"      ğŸ“‹ æ£€æµ‹åˆ°å¤æ‚å±æ€§è®¾ç½®ï¼Œä½¿ç”¨å•ä¸ªAPI...")
            return self._fallback_individual_set(alpha_ids, **properties)
    
    def _fallback_individual_set(self, alpha_ids, **properties):
        """å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨å•ä¸ªAPIè®¾ç½®"""
        success_count = 0
        failed_count = 0
        
        for i, alpha_id in enumerate(alpha_ids):
            try:
                success = self._set_alpha_properties_with_retry(alpha_id, **properties)
                if success:
                    success_count += 1
                else:
                    failed_count += 1
                
                if i % 10 == 0 or i == len(alpha_ids) - 1:
                    self.logger.info(f"      âœ… å•ä¸ªè®¾ç½®è¿›åº¦: {i+1}/{len(alpha_ids)} (æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count})")
                
                # é€‚å½“å»¶è¿Ÿé¿å…APIé™åˆ¶
                if i < len(alpha_ids) - 1:
                    time.sleep(1.0)
                    
            except Exception as e:
                failed_count += 1
                self.logger.error(f"      âŒ Alpha {alpha_id} è®¾ç½®å¼‚å¸¸: {e}")
        
        return success_count, failed_count
    def _set_alpha_properties_with_retry(self, alpha_id, **properties):
        """å¸¦é‡è¯•æœºåˆ¶çš„Alphaå±æ€§è®¾ç½®
        æ­£ç¡®å¤„ç†HTTP 429é€Ÿç‡é™åˆ¶ï¼šé€šè¿‡ç­‰å¾…è€Œä¸æ˜¯é‡æ–°ç™»å½•
        """
        max_retries = 3
        base_delay = 60  # åŸºç¡€ç­‰å¾…æ—¶é—´60ç§’
        
        for attempt in range(max_retries + 1):
            try:
                # å°è¯•è®¾ç½®Alphaå±æ€§
                result = set_alpha_properties(self.session, alpha_id, **properties)
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºé€Ÿç‡é™åˆ¶
                if result == 'RATE_LIMITED':
                    if attempt < max_retries:
                        # è®¡ç®—ç­‰å¾…æ—¶é—´ï¼šæŒ‡æ•°é€€é¿ç­–ç•¥
                        wait_time = base_delay * (2 ** attempt)  # 60s, 120s, 240s
                        self.logger.info(f"      â° Alpha {alpha_id} é‡åˆ°é€Ÿç‡é™åˆ¶ (å°è¯• {attempt + 1}/{max_retries + 1})")
                        self.logger.info(f"      â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        
                        # å€’è®¡æ—¶ç­‰å¾…
                        for remaining in range(int(wait_time), 0, -1):
                            minutes, seconds = divmod(remaining, 60)
                            if minutes > 0:
                                time_str = f"{minutes}:{seconds:02d}"
                            else:
                                time_str = f"{seconds}s"
                            print(f"      â³ é€Ÿç‡é™åˆ¶ç­‰å¾…: {time_str}", end='\r', flush=True)
                            time.sleep(1)
                        print(f"                                    ", end='\r')  # æ¸…é™¤å€’è®¡æ—¶
                        
                        self.logger.info(f"      ğŸ”„ ç­‰å¾…å®Œæˆï¼Œé‡è¯• Alpha {alpha_id}...")
                        continue
                    else:
                        self.logger.warning(f"      âŒ Alpha {alpha_id} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä»ç„¶é€Ÿç‡é™åˆ¶")
                        return False
                
                # è¿”å›æˆåŠŸæˆ–å¤±è´¥ç»“æœ
                return result == True
                
            except Exception as e:
                if attempt < max_retries:
                    self.logger.warning(f"      âš ï¸  Alpha {alpha_id} è®¾ç½®å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries + 1}): {e}")
                    self.logger.info(f"      â³ ç­‰å¾… 30 ç§’åé‡è¯•...")
                    time.sleep(30)
                    continue
                else:
                    self.logger.error(f"      âŒ Alpha {alpha_id} è®¾ç½®å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e}")
                    return False
        
        return False

    def run_check_cycle(self):
        """è¿è¡Œä¸€æ¬¡æ£€æŸ¥å‘¨æœŸ"""
        try:
            self.logger.info(f"ğŸš€ åˆå§‹åŒ–æ£€æŸ¥ç¯å¢ƒ...")
            self.initialize_session()
            
            start_date_file = os.path.join(RECORDS_PATH, 'start_date.txt')
            submitable_alpha_file = os.path.join(RECORDS_PATH, 'submitable_alpha.csv')
            
            # ç”Ÿæˆæ£€æŸ¥æ—¶é—´æ®µ - ä¼˜å…ˆçº§ï¼šå‘½ä»¤è¡Œå‚æ•° > æ•°æ®åº“ > æ–‡ä»¶ > é»˜è®¤å€¼
            db_path = os.path.join(os.path.dirname(RECORDS_PATH), 'database', 'factors.db')
            db = FactorDatabaseManager(db_path)
            
            if self.override_start_date:
                # ç”¨æˆ·æŒ‡å®šäº†èµ·å§‹æ—¥æœŸï¼Œä½¿ç”¨æ­¤æ—¥æœŸå¹¶æ›´æ–°åˆ°æ•°æ®åº“
                start_date = self.override_start_date
                self.logger.info(f"ğŸ“… ä½¿ç”¨ç”¨æˆ·æŒ‡å®šå¼€å§‹æ—¥æœŸ: {start_date}")
                try:
                    # å°†ç”¨æˆ·æŒ‡å®šçš„æ—¥æœŸå†™å…¥æ•°æ®åº“
                    db.set_system_config('start_date', start_date)
                    self.logger.info(f"ğŸ“… å·²å°†ç”¨æˆ·æŒ‡å®šæ—¥æœŸæ›´æ–°åˆ°æ•°æ®åº“")
                    # æ¸…é™¤ç”¨æˆ·æŒ‡å®šæ—¥æœŸï¼Œé¿å…å½±å“åç»­å¾ªç¯
                    self.override_start_date = None
                    self.logger.info(f"ğŸ“… å·²æ¸…é™¤ç”¨æˆ·æŒ‡å®šæ—¥æœŸï¼Œåç»­å¾ªç¯å°†ä½¿ç”¨æ•°æ®åº“æ—¥æœŸ")
                except Exception as e:
                    self.logger.warning(f"ğŸ“… æ›´æ–°æ•°æ®åº“å¤±è´¥: {e}")
            else:
                # ç”¨æˆ·æœªæŒ‡å®šæ—¥æœŸï¼Œä»æ•°æ®åº“æˆ–æ–‡ä»¶è¯»å–
                try:
                    start_date = db.get_system_config('start_date')
                    if start_date:
                        self.logger.info(f"ğŸ“… ä»æ•°æ®åº“è¯»å–å¼€å§‹æ—¥æœŸ: {start_date}")
                    else:
                        # å¦‚æœæ•°æ®åº“ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»æ–‡ä»¶è¯»å–
                        try:
                            with open(start_date_file, 'r') as f:
                                start_date = f.read().strip()
                            self.logger.info(f"ğŸ“… ä»æ–‡ä»¶è¯»å–å¼€å§‹æ—¥æœŸ: {start_date}")
                            # å°†è¯»å–çš„æ—¥æœŸå†™å…¥æ•°æ®åº“
                            db.set_system_config('start_date', start_date)
                        except FileNotFoundError:
                            start_date = '2024-10-07'
                            self.logger.info(f"ğŸ“… ä½¿ç”¨é»˜è®¤å¼€å§‹æ—¥æœŸ: {start_date}")
                            # å°†é»˜è®¤æ—¥æœŸå†™å…¥æ•°æ®åº“
                            db.set_system_config('start_date', start_date)
                except Exception as e:
                    self.logger.warning(f"ğŸ“… æ•°æ®åº“è¯»å–å¤±è´¥: {e}, ä½¿ç”¨æ–‡ä»¶è¯»å–")
                    try:
                        with open(start_date_file, 'r') as f:
                            start_date = f.read().strip()
                        self.logger.info(f"ğŸ“… ä»æ–‡ä»¶è¯»å–å¼€å§‹æ—¥æœŸ: {start_date}")
                    except FileNotFoundError:
                        start_date = '2024-10-07'
                        self.logger.info(f"ğŸ“… ä½¿ç”¨é»˜è®¤å¼€å§‹æ—¥æœŸ: {start_date}")
            
            end_date = (datetime.strptime(start_date, '%Y-%m-%d') + timedelta(days=1)).strftime('%Y-%m-%d')
            
            self.logger.info(f"ğŸ“… æ£€æŸ¥æ—¶é—´æ®µ: {start_date} åˆ° {end_date}")
            
            # ä½¿ç”¨åŠ¨æ€é…ç½®çš„é˜ˆå€¼
            sh_th = self.sharpe_threshold
            self.logger.info(f"ğŸ“Š æ£€æŸ¥æ¨¡å¼: {self.mode}, å¤æ™®æ¯”é˜ˆå€¼: {sh_th}")
            if self.fitness_threshold is not None:
                self.logger.info(f"ğŸ“ˆ Fitnessé˜ˆå€¼: {self.fitness_threshold}")
            else:
                self.logger.info(f"ğŸ“ˆ ä¸ä½¿ç”¨Fitnessé˜ˆå€¼è¿‡æ»¤")
            
            total_checked = 0
            total_regions = len(REGION_LIST)
            
            for region_idx, region in enumerate(REGION_LIST):
                self.logger.info(f"\n{'='*80}")
                self.logger.info(f"ğŸŒ [{region_idx+1}/{total_regions}] æ£€æŸ¥åœ°åŒº: {region}")
                self.logger.info(f"{'='*80}")
                
                region_start_time = time.time()
                
                # è·å–éœ€è¦æ£€æŸ¥çš„Alpha
                self.logger.info(f"ğŸ” è·å–åœ°åŒº {region} çš„Alphaåˆ—è¡¨...")
                # fitnessé˜ˆå€¼ï¼šå¦‚æœä¸ä½¿ç”¨åˆ™ä¼ å…¥Noneï¼Œè®©get_alphaså‡½æ•°ä¸æ·»åŠ fitnessè¿‡æ»¤
                fitness_th = self.fitness_threshold  # Noneè¡¨ç¤ºä¸ä½¿ç”¨fitnessè¿‡æ»¤
                # è·å–Alphaï¼ˆæ’é™¤REDï¼Œç„¶åç»Ÿä¸€è¿‡æ»¤å…¶ä»–å·²å¤„ç†çŠ¶æ€ï¼‰
                need_to_check = get_alphas(
                    start_date, end_date, sh_th, fitness_th, 10, 10,
                    region=region, universe="", delay='', instrumentType='',
                    alpha_num=9999, usage="submit", tag='', color_exclude='RED', s=self.session
                )
                
                # ç»Ÿä¸€è¿‡æ»¤æ‰€æœ‰å·²å¤„ç†çŠ¶æ€çš„Alphaï¼ˆGREENã€YELLOWã€BLUEã€PURPLEï¼‰
                if need_to_check['check']:
                    original_count = len(need_to_check['check'])
                    
                    # ç»Ÿè®¡å„ç§çŠ¶æ€çš„Alphaæ•°é‡ï¼ˆç”¨äºæ—¥å¿—ï¼‰
                    green_count = sum(1 for alpha in need_to_check['check'] if alpha.get('color') == 'GREEN')
                    yellow_count = sum(1 for alpha in need_to_check['check'] if alpha.get('color') == 'YELLOW')
                    blue_count = sum(1 for alpha in need_to_check['check'] if alpha.get('color') == 'BLUE')
                    purple_count = sum(1 for alpha in need_to_check['check'] if alpha.get('color') == 'PURPLE')
                    
                    # ç»Ÿä¸€è¿‡æ»¤å·²å¤„ç†çŠ¶æ€
                    need_to_check['check'] = [alpha for alpha in need_to_check['check'] 
                                             if alpha.get('color') not in ['GREEN', 'YELLOW', 'BLUE', 'PURPLE']]
                    filtered_count = len(need_to_check['check'])
                    
                    # æ˜¾ç¤ºè¿‡æ»¤ç»Ÿè®¡
                    if original_count > filtered_count:
                        self.logger.info(f"  ğŸ”½ APIè¿‡æ»¤ç»Ÿè®¡ ({original_count} â†’ {filtered_count}):")
                        if green_count > 0:
                            self.logger.info(f"    ğŸŸ¢ è¿‡æ»¤æ‰ {green_count} ä¸ªGREENçŠ¶æ€çš„Alpha")
                        if yellow_count > 0:
                            self.logger.info(f"    ğŸŸ¡ è¿‡æ»¤æ‰ {yellow_count} ä¸ªYELLOWçŠ¶æ€çš„Alpha")
                        if blue_count > 0:
                            self.logger.info(f"    ğŸ”µ è¿‡æ»¤æ‰ {blue_count} ä¸ªBLUEçŠ¶æ€çš„Alpha")
                        if purple_count > 0:
                            self.logger.info(f"    ğŸŸ£ è¿‡æ»¤æ‰ {purple_count} ä¸ªPURPLEçŠ¶æ€çš„Alphaï¼ˆå‚å­—å‹ï¼‰")
                        self.logger.info(f"  ğŸ“Š å‰©ä½™å¾…æ£€æŸ¥Alpha: {filtered_count} ä¸ª")
                
                if not need_to_check['check']:
                    self.logger.info(f"  ğŸ“ {region} åœ°åŒºæ²¡æœ‰éœ€è¦æ£€æŸ¥çš„Alpha")
                    continue
                
                region_alpha_count = len(need_to_check['check'])
                total_checked += region_alpha_count
                self.logger.info(f"  ğŸ“Š {region} åœ°åŒºæ‰¾åˆ° {region_alpha_count} ä¸ªAlphaéœ€è¦æ£€æŸ¥")
                
                # æ˜¾ç¤ºAlphaæ¦‚è§ˆ
                alpha_tags = defaultdict(int)
                for alpha in need_to_check['check']:
                    tags = alpha.get('tags', [])
                    tag = tags[0] if tags else 'Unknown'
                    alpha_tags[tag] += 1
                
                self.logger.info(f"  ğŸ“‹ Alphaæ ‡ç­¾åˆ†å¸ƒ:")
                for tag, count in alpha_tags.items():
                    self.logger.info(f"    {tag}: {count} ä¸ª")
                
                # æ‰¹é‡æ£€æŸ¥
                self.batch_check_alphas(need_to_check['check'], submitable_alpha_file)
                
                region_time = time.time() - region_start_time
                self.logger.info(f"  â±ï¸  åœ°åŒº {region} å¤„ç†å®Œæˆï¼Œè€—æ—¶: {region_time:.2f}s")
                
            # å¦‚æœstart_dateè·ç¦»å½“å‰æ—¥æœŸè¶…è¿‡2å¤©ï¼Œåˆ™æ›´æ–°start_date
            start_date_obj = datetime.strptime(start_date, '%Y-%m-%d').date()
            if (datetime.now().date() - start_date_obj).days > 2:
                new_date = (start_date_obj + timedelta(days=1)).strftime('%Y-%m-%d')
                
                # æ•°æ®åº“ç‰ˆæœ¬ï¼šæ›´æ–°start_dateåˆ°æ•°æ®åº“
                try:
                    db.set_system_config('start_date', new_date)
                    self.logger.info(f"ğŸ“… æ•°æ®åº“æ›´æ–°å¼€å§‹æ—¥æœŸä¸º: {new_date} (åŒºåŸŸå†…å‘å‰æ¨è¿›1å¤©)")
                except Exception as e:
                    self.logger.info(f"ğŸ“… æ•°æ®åº“æ›´æ–°å¤±è´¥: {e}ï¼Œå›é€€åˆ°æ–‡ä»¶æ›´æ–°")
                    # å¦‚æœæ•°æ®åº“æ›´æ–°å¤±è´¥ï¼Œå›é€€åˆ°æ–‡ä»¶å†™å…¥
                    try:
                        with open(start_date_file, 'w') as f:
                            f.write(new_date)
                        self.logger.info(f"ğŸ“… æ–‡ä»¶æ›´æ–°å¼€å§‹æ—¥æœŸä¸º: {new_date} (åŒºåŸŸå†…å‘å‰æ¨è¿›1å¤©)")
                    except Exception as fe:
                        self.logger.info(f"ğŸ“… æ–‡ä»¶æ›´æ–°ä¹Ÿå¤±è´¥: {fe}")
                
                # æ›´æ–°start_dateä»¥ä¾¿ä¸‹ä¸€ä¸ªåŒºåŸŸä½¿ç”¨
                start_date = new_date
            
            self.logger.info(f"\nğŸ“Š æœ¬æ¬¡æ£€æŸ¥å‘¨æœŸæ€»ç»“:")
            self.logger.info(f"  ğŸŒ æ£€æŸ¥åœ°åŒº: {total_regions} ä¸ª")
            self.logger.info(f"  ğŸ“‹ å¤„ç†Alpha: {total_checked} ä¸ª")
            
        except Exception as e:
            self.logger.error(f"âŒ æ£€æŸ¥å‘¨æœŸå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    import logging.handlers
    from datetime import datetime
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–ç‰ˆAlphaæ£€æŸ¥å™¨')
    parser.add_argument('--mode', choices=['CONSULTANT', 'USER', 'PPAC'], 
                       help='æ£€æŸ¥æ¨¡å¼: CONSULTANT(sharpeâ‰¥1.58,fitnessâ‰¥1), USER(sharpeâ‰¥1.25,æ— fitness), PPAC(sharpeâ‰¥1.0,fitnesså¯é€‰)')
    parser.add_argument('--sharpe-threshold', type=float, 
                       help='Sharpeé˜ˆå€¼ (è¦†ç›–æ¨¡å¼é»˜è®¤å€¼)')
    parser.add_argument('--fitness-threshold', type=float, 
                       help='Fitnessé˜ˆå€¼ (å¯é€‰ï¼Œä»…åœ¨æŒ‡å®šæ—¶ä½¿ç”¨)')
    parser.add_argument('--batch-size', type=int, default=50,
                       help='æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 50)')
    parser.add_argument('--start-date', type=str,
                       help='èµ·å§‹æ£€æŸ¥æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD, å¦‚: 2025-01-01)')
    
    args = parser.parse_args()
    
    # é…ç½®ä¸“ç”¨çš„loggerï¼Œé¿å…ä¸session_managerçš„loggingå†²çª
    logger = logging.getLogger('check_optimized')
    logger.setLevel(logging.INFO)
    
    # åªåœ¨æ²¡æœ‰handleræ—¶æ·»åŠ ï¼Œé¿å…é‡å¤
    if not logger.handlers:
        # æ£€æŸ¥æ˜¯å¦ä½œä¸ºå­è¿›ç¨‹è¿è¡Œï¼ˆé€šè¿‡æ£€æŸ¥stdoutæ˜¯å¦è¢«é‡å®šå‘ï¼‰
        import sys
        is_subprocess = not sys.stdout.isatty()
        
        if is_subprocess:
            # ä½œä¸ºå­è¿›ç¨‹è¿è¡Œï¼Œä½¿ç”¨ç®€å•çš„StreamHandlerè¾“å‡ºåˆ°stdout
            # è¿™äº›è¾“å‡ºä¼šè¢«çˆ¶è¿›ç¨‹é‡å®šå‘åˆ°æ—¥å¿—æ–‡ä»¶
            console_handler = logging.StreamHandler(sys.stdout)
            console_formatter = logging.Formatter('%(asctime)s | %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)
            
            logger.info(f"ğŸ“ ä¼˜åŒ–æ£€æŸ¥å™¨æ—¥å¿—ç³»ç»Ÿå·²å¯åŠ¨ (å­è¿›ç¨‹æ¨¡å¼)")
            logger.info(f"  ğŸ“¤ è¾“å‡ºé‡å®šå‘: é€šè¿‡çˆ¶è¿›ç¨‹ç®¡ç†")
            logger.info(f"  ğŸ†” è¿›ç¨‹ID: {os.getpid()}")
            logger.info(f"  ğŸ’¾ ç¼–ç : UTF-8")
        else:
            # ç‹¬ç«‹è¿è¡Œæ¨¡å¼ï¼Œåˆ›å»ºè‡ªå·±çš„æ—¥å¿—æ–‡ä»¶
            # ç¡®ä¿logsç›®å½•å­˜åœ¨
            log_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'logs')
            os.makedirs(log_dir, exist_ok=True)
            
            # ç”Ÿæˆå”¯ä¸€çš„æ—¥å¿—æ–‡ä»¶åï¼ˆåŸºäºå¯åŠ¨æ—¶é—´å’ŒPIDï¼‰
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file = os.path.join(log_dir, f"check_optimized_{timestamp}_{os.getpid()}.log")
            
            # åˆ›å»ºè½®è½¬æ–‡ä»¶å¤„ç†å™¨ï¼šæœ€å¤§10MBï¼Œä¿ç•™3ä¸ªæ–‡ä»¶
            file_handler = logging.handlers.RotatingFileHandler(
                log_file, 
                maxBytes=10*1024*1024,  # 10MB
                backupCount=3,          # ä¿ç•™3ä¸ªå¤‡ä»½æ–‡ä»¶
                encoding='utf-8'
            )
            
            # é‡å†™doRolloveræ–¹æ³•ï¼Œåœ¨è½®è½¬æ—¶è®°å½•ä¿¡æ¯
            original_doRollover = file_handler.doRollover
            def doRollover_with_log():
                original_doRollover()
                # è½®è½¬åè®°å½•ä¿¡æ¯ï¼ˆä½¿ç”¨æ–°æ–‡ä»¶ï¼‰
                file_handler.emit(file_handler.makeRecord(
                    logger.name, logging.INFO, __file__, 0, 
                    f"ğŸ”„ æ—¥å¿—æ–‡ä»¶å·²è½®è½¬ï¼Œå½“å‰æ–‡ä»¶: {os.path.basename(log_file)}", 
                    (), None
                ))
            file_handler.doRollover = doRollover_with_log
            
            # æ§åˆ¶å°å¤„ç†å™¨ï¼ˆç‹¬ç«‹æ¨¡å¼ä¸‹ä¹Ÿæ·»åŠ æ§åˆ¶å°è¾“å‡ºï¼‰
            console_handler = logging.StreamHandler(sys.stdout)
            
            # è®¾ç½®æ ¼å¼
            file_formatter = logging.Formatter('%(asctime)s | %(message)s', datefmt='%Y-%m-%d %H:%M:%S')
            console_formatter = logging.Formatter('%(message)s')
            file_handler.setFormatter(file_formatter)
            console_handler.setFormatter(console_formatter)
            
            # æ·»åŠ å¤„ç†å™¨
            logger.addHandler(file_handler)
            logger.addHandler(console_handler)
            
            # é˜²æ­¢æ—¥å¿—ä¼ æ’­åˆ°root loggerï¼Œé¿å…é‡å¤è¾“å‡º
            logger.propagate = False
            
            logger.info(f"ğŸ“ ä¼˜åŒ–æ£€æŸ¥å™¨æ—¥å¿—ç³»ç»Ÿå·²å¯åŠ¨ (ç‹¬ç«‹æ¨¡å¼)")
            logger.info(f"  ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
            logger.info(f"  ğŸ”„ è½®è½¬è®¾ç½®: 3ä¸ªæ–‡ä»¶ Ã— 10MB")
            logger.info(f"  ğŸ’¾ ç¼–ç : UTF-8")
        
        # è®¾ç½®é˜²æ­¢æ—¥å¿—ä¼ æ’­ï¼ˆä¸¤ç§æ¨¡å¼éƒ½éœ€è¦ï¼‰
        logger.propagate = False
    
    logger.info("ğŸš€ å¯åŠ¨ä¼˜åŒ–ç‰ˆAlphaæ£€æŸ¥å™¨...")
    
    # åˆ›å»ºæ£€æŸ¥å™¨å®ä¾‹ - ä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    checker = OptimizedChecker(
        mode=args.mode,
        batch_size=args.batch_size,
        sharpe_threshold=args.sharpe_threshold,
        fitness_threshold=args.fitness_threshold,
        start_date=args.start_date
    )
    
    # æŒç»­è¿è¡Œ
    while True:
        try:
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ•’ å¼€å§‹æ–°çš„æ£€æŸ¥å‘¨æœŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info(f"{'='*60}")
            
            start_time = time.time()
            
            checker.run_check_cycle()
            
            end_time = time.time()
            
            if end_time - start_time > 300:
                logger.info(f"ğŸ•’ æ£€æŸ¥å‘¨æœŸå®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f}sï¼Œè¶…è¿‡äº”åˆ†é’Ÿï¼Œè·³è¿‡äº”åˆ†é’Ÿç­‰å¾…")
                continue
            
            logger.info(f"\nâœ… æ£€æŸ¥å‘¨æœŸå®Œæˆï¼Œ{300 - (end_time - start_time):.2f}ç§’åå¼€å§‹æ–°çš„æ£€æŸ¥å‘¨æœŸ...")
            # 5åˆ†é’Ÿå€’è®¡æ—¶ç­‰å¾…
            time.sleep(300 - (end_time - start_time))
            logger.info(f"ğŸš€ å¼€å§‹æ–°çš„æ£€æŸ¥å‘¨æœŸ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            
        except KeyboardInterrupt:
            logger.info("\nğŸ‘‹ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
            break
        except Exception as e:
            logger.error(f"âŒ ä¸»å¾ªç¯å¼‚å¸¸: {e}")
            logger.info("â³ ç­‰å¾…100ç§’åé‡è¯•...")
            # 100ç§’å€’è®¡æ—¶ç­‰å¾…
            for remaining in range(100, 0, -1):
                minutes, seconds = divmod(remaining, 60)
                print(f"â³ å¼‚å¸¸æ¢å¤å€’è®¡æ—¶: {minutes:02d}:{seconds:02d}", end='\r', flush=True)
                time.sleep(1)
            print(f"                                         ", end='\r')  # æ¸…é™¤è¿›åº¦æ˜¾ç¤º
            logger.info(f"ğŸ”„ é‡æ–°å¯åŠ¨æ£€æŸ¥å™¨...")

if __name__ == '__main__':
    main() 