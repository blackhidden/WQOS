"""
PnLæ•°æ®ç®¡ç†å™¨ - è´Ÿè´£PnLæ•°æ®çš„è·å–å’Œç¼“å­˜
"""

import time
import pickle
import pandas as pd
from typing import List, Dict, Tuple, Optional
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor


class PnLManager:
    """PnLæ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, config_manager, session_service, logger):
        """åˆå§‹åŒ–PnLç®¡ç†å™¨"""
        self.config = config_manager
        self.session_service = session_service
        self.logger = logger
        
        # PnLæ•°æ®ç¼“å­˜æœºåˆ¶
        self.pnl_cache = {}  # {alpha_id: pd.Series} - ç¼“å­˜å·²ä¸‹è½½çš„PnLæ•°æ®
        self.cache_loaded = False
    
    def load_pnl_cache(self):
        """åŠ è½½PnLæ•°æ®ç¼“å­˜"""
        if self.cache_loaded:
            return
            
        try:
            if self.config.pnl_cache_file.exists():
                with open(self.config.pnl_cache_file, 'rb') as f:
                    self.pnl_cache = pickle.load(f)
                self.logger.info(f"ğŸ“‚ åŠ è½½PnLç¼“å­˜: {len(self.pnl_cache)} ä¸ªAlpha")
            else:
                self.pnl_cache = {}
                self.logger.info(f"ğŸ“‚ åˆå§‹åŒ–ç©ºPnLç¼“å­˜")
        except Exception as e:
            self.logger.warning(f"âš ï¸ åŠ è½½PnLç¼“å­˜å¤±è´¥: {e}ï¼Œä½¿ç”¨ç©ºç¼“å­˜")
            self.pnl_cache = {}
        
        self.cache_loaded = True
    
    def save_pnl_cache(self):
        """ä¿å­˜PnLæ•°æ®ç¼“å­˜"""
        try:
            with open(self.config.pnl_cache_file, 'wb') as f:
                pickle.dump(self.pnl_cache, f, pickle.HIGHEST_PROTOCOL)
            self.logger.debug(f"ğŸ’¾ ä¿å­˜PnLç¼“å­˜: {len(self.pnl_cache)} ä¸ªAlpha")
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜PnLç¼“å­˜å¤±è´¥: {e}")
    
    def cleanup_pnl_cache(self, keep_alpha_ids: List[str]):
        """æ¸…ç†PnLç¼“å­˜ï¼Œåªä¿ç•™é€šè¿‡æ£€æµ‹çš„Alphaæ•°æ®"""
        if not self.pnl_cache:
            return
            
        original_count = len(self.pnl_cache)
        # åªä¿ç•™åœ¨keep_alpha_idsä¸­çš„Alphaæ•°æ®
        self.pnl_cache = {aid: pnl for aid, pnl in self.pnl_cache.items() if aid in keep_alpha_ids}
        cleaned_count = original_count - len(self.pnl_cache)
        
        if cleaned_count > 0:
            self.logger.info(f"ğŸ§¹ æ¸…ç†PnLç¼“å­˜: ç§»é™¤ {cleaned_count} ä¸ªæœªé€šè¿‡æ£€æµ‹çš„Alphaï¼Œä¿ç•™ {len(self.pnl_cache)} ä¸ª")
            self.save_pnl_cache()
        else:
            self.logger.debug(f"ğŸ§¹ PnLç¼“å­˜æ— éœ€æ¸…ç†")
    
    def _get_alpha_pnl(self, alpha_id: str) -> pd.DataFrame:
        """è·å–æŒ‡å®šalphaçš„PnLæ•°æ®"""
        try:
            url = f"https://api.worldquantbrain.com/alphas/{alpha_id}/recordsets/pnl"
            response = self.session_service.wait_get(url, message=f"è·å–Alpha {alpha_id} pnlæ•°æ®")
            
            if response.status_code != 200:
                raise Exception(f"HTTP {response.status_code}: {response.text}")
            
            pnl_data = response.json()
            
            # æ£€æŸ¥å“åº”æ•°æ®ç»“æ„
            if 'records' not in pnl_data or 'schema' not in pnl_data:
                raise Exception(f"å“åº”æ•°æ®æ ¼å¼é”™è¯¯: {pnl_data}")
            
            if not pnl_data['records']:
                # å¦‚æœæ²¡æœ‰è®°å½•ï¼Œè¿”å›ç©ºDataFrame
                return pd.DataFrame(columns=['Date', alpha_id])
            
            # æ„å»ºDataFrame
            columns = [item['name'] for item in pnl_data['schema']['properties']]
            df = pd.DataFrame(pnl_data['records'], columns=columns)
            
            # é‡å‘½ååˆ—
            df = df.rename(columns={'date': 'Date', 'pnl': alpha_id})
            
            # ç¡®ä¿éœ€è¦çš„åˆ—å­˜åœ¨
            if 'Date' not in df.columns:
                raise Exception(f"å“åº”æ•°æ®ä¸­ç¼ºå°‘dateå­—æ®µ: {columns}")
            if alpha_id not in df.columns:
                raise Exception(f"å“åº”æ•°æ®ä¸­ç¼ºå°‘pnlå­—æ®µ: {columns}")
            
            df = df[['Date', alpha_id]]
            return df
            
        except Exception as e:
            raise Exception(f"è·å–Alpha {alpha_id} PnLæ•°æ®å¤±è´¥: {e}")
    
    def get_alpha_daily_pnl(self, alpha_id: str) -> pd.Series:
        """è·å–æŒ‡å®šalphaçš„æ¯æ—¥PnLæ•°æ®ï¼ˆç”¨äºè´¨é‡æ£€æŸ¥ï¼‰"""
        try:
            url = f"https://api.worldquantbrain.com/alphas/{alpha_id}/recordsets/daily-pnl"
            response = self.session_service.wait_get(url, message=f"è·å–Alpha {alpha_id} daily-pnlæ•°æ®")
            
            if response.status_code != 200:
                raise Exception(f"HTTP {response.status_code}: {response.text}")
            
            data = response.json()
            
            # æ£€æŸ¥å“åº”æ•°æ®ç»“æ„
            if 'records' not in data or 'schema' not in data:
                raise Exception(f"å“åº”æ•°æ®æ ¼å¼é”™è¯¯: {data}")
            
            if not data['records']:
                self.logger.warning(f"âš ï¸ Alpha {alpha_id} æ²¡æœ‰daily-pnlæ•°æ®")
                return pd.Series()
            
            # æ„å»ºDataFrame
            columns = [item['name'] for item in data['schema']['properties']]
            df = pd.DataFrame(data['records'], columns=columns)
            
            # è®¾ç½®ç´¢å¼•å’Œè¿”å›Series
            df['date'] = pd.to_datetime(df['date'])
            df = df.set_index('date').sort_index()
            
            return df['pnl']
            
        except Exception as e:
            self.logger.error(f"âŒ è·å–Alpha {alpha_id} daily-pnlæ•°æ®å¤±è´¥: {e}")
            return pd.Series()
    
    def get_alpha_pnls(self, alphas: List[Dict], 
                      alpha_pnls: Optional[pd.DataFrame] = None,
                      alpha_ids: Optional[Dict[str, List]] = None) -> Tuple[Dict[str, List], pd.DataFrame]:
        """è·å–alphaçš„PnLæ•°æ®ï¼Œå¹¶æŒ‰åŒºåŸŸåˆ†ç±»alphaçš„IDï¼ˆæ”¯æŒç¼“å­˜æœºåˆ¶ï¼‰"""
        if alpha_ids is None:
            alpha_ids = defaultdict(list)
        if alpha_pnls is None or alpha_pnls.empty:
            alpha_pnls = pd.DataFrame()
        
        # ç¡®ä¿ç¼“å­˜å·²åŠ è½½
        self.load_pnl_cache()
        
        # è¿‡æ»¤å‡ºæ–°çš„alphaï¼ˆæ—¢ä¸åœ¨ç°æœ‰DataFrameä¸­ï¼Œä¹Ÿä¸åœ¨ç¼“å­˜ä¸­ï¼‰
        existing_columns = alpha_pnls.columns.tolist() if not alpha_pnls.empty else []
        new_alphas = [item for item in alphas if item['id'] not in existing_columns]
        
        # è¿›ä¸€æ­¥åˆ†ç±»ï¼šä»ç¼“å­˜ä¸­å¯ä»¥è·å–çš„å’Œéœ€è¦é‡æ–°ä¸‹è½½çš„
        alphas_from_cache = []
        alphas_to_download = []
        
        for alpha in new_alphas:
            alpha_id = alpha['id']
            if alpha_id in self.pnl_cache:
                alphas_from_cache.append(alpha)
            else:
                alphas_to_download.append(alpha)
        
        if not new_alphas:
            return alpha_ids, alpha_pnls
        
        self.logger.info(f"ğŸ“¥ å¤„ç† {len(new_alphas)} ä¸ªæ–°Alphaçš„PnLæ•°æ®:")
        self.logger.info(f"  ğŸ’¾ ä»ç¼“å­˜è·å–: {len(alphas_from_cache)} ä¸ª")
        self.logger.info(f"  ğŸŒ éœ€è¦ä¸‹è½½: {len(alphas_to_download)} ä¸ª")
        
        # æŒ‰åŒºåŸŸåˆ†ç±»æ‰€æœ‰alpha
        for alpha in new_alphas:
            region = alpha['settings']['region']
            alpha_ids[region].append(alpha['id'])
        
        # é¦–å…ˆä»ç¼“å­˜ä¸­è·å–PnLæ•°æ®
        cached_results = []
        if alphas_from_cache:
            self.logger.info(f"ğŸ“‚ ä»ç¼“å­˜åŠ è½½ {len(alphas_from_cache)} ä¸ªAlphaçš„PnLæ•°æ®...")
            for alpha in alphas_from_cache:
                alpha_id = alpha['id']
                try:
                    cached_pnl = self.pnl_cache[alpha_id].copy()
                    cached_pnl.name = alpha_id
                    cached_df = cached_pnl.to_frame().T
                    cached_df.index = [alpha_id]
                    cached_df = cached_df.T
                    cached_results.append(cached_df)
                except Exception as e:
                    self.logger.warning(f"  âš ï¸ ä»ç¼“å­˜è·å–Alpha {alpha_id}å¤±è´¥: {e}ï¼Œå°†é‡æ–°ä¸‹è½½")
                    alphas_to_download.append(alpha)
        
        if not alphas_to_download:
            # æ‰€æœ‰æ•°æ®éƒ½ä»ç¼“å­˜è·å–
            if cached_results:
                if alpha_pnls.empty:
                    alpha_pnls = pd.concat(cached_results, axis=1)
                else:
                    alpha_pnls = pd.concat([alpha_pnls] + cached_results, axis=1)
                alpha_pnls.sort_index(inplace=True)
            return alpha_ids, alpha_pnls
        
        # å¹¶è¡Œè·å–éœ€è¦ä¸‹è½½çš„PnLæ•°æ®
        self.logger.info(f"ğŸŒ å¼€å§‹ä¸‹è½½ {len(alphas_to_download)} ä¸ªAlphaçš„PnLæ•°æ®...")
        
        def fetch_pnl_func(alpha_data):
            alpha_id = alpha_data['id']
            try:
                result = self._get_alpha_pnl(alpha_id).set_index('Date')
                return alpha_id, result, None
            except Exception as e:
                self.logger.error(f"âŒ è·å–Alpha {alpha_id} PnLå¤±è´¥: {e}")
                return alpha_id, pd.DataFrame(), str(e)
        
        # ä½¿ç”¨è¾ƒå°‘çš„å¹¶å‘æ•°ä»¥é¿å…APIé™åˆ¶
        max_workers = min(3, len(alphas_to_download))  # é™ä½å¹¶å‘æ•°
        results = []
        
        # åˆ†æ‰¹å¤„ç†ä»¥é¿å…APIé™åˆ¶
        batch_size = 10
        for i in range(0, len(alphas_to_download), batch_size):
            batch = alphas_to_download[i:i + batch_size]
            self.logger.info(f"ğŸ“¥ å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(alphas_to_download)-1)//batch_size + 1}: {len(batch)} ä¸ªAlpha")
            
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                batch_results = list(executor.map(fetch_pnl_func, batch))
                results.extend(batch_results)
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            if i + batch_size < len(alphas_to_download):
                self.logger.info(f"â¸ï¸ æ‰¹æ¬¡é—´ä¼‘æ¯2ç§’...")
                time.sleep(2)
        
        # å¤„ç†ä¸‹è½½ç»“æœå¹¶æ›´æ–°ç¼“å­˜
        downloaded_results = []
        failed_alphas = []
        newly_cached_count = 0
        
        for alpha_id, result, error in results:
            if not result.empty and error is None:
                downloaded_results.append(result)
                # å°†æˆåŠŸä¸‹è½½çš„PnLæ•°æ®æ·»åŠ åˆ°ç¼“å­˜
                self.pnl_cache[alpha_id] = result.iloc[:, 0]  # ä¿å­˜ä¸ºSeries
                newly_cached_count += 1
            else:
                failed_alphas.append(alpha_id)
        
        # ä¿å­˜æ›´æ–°çš„ç¼“å­˜
        if newly_cached_count > 0:
            self.save_pnl_cache()
            self.logger.info(f"ğŸ’¾ æ–°å¢ç¼“å­˜: {newly_cached_count} ä¸ªAlphaçš„PnLæ•°æ®")
        
        if failed_alphas:
            self.logger.warning(f"âš ï¸ {len(failed_alphas)} ä¸ªAlphaçš„PnLæ•°æ®è·å–å¤±è´¥ï¼Œè·³è¿‡è¿™äº›Alpha")
            self.logger.info(f"å¤±è´¥çš„Alpha ID: {failed_alphas[:10]}{'...' if len(failed_alphas) > 10 else ''}")
        
        # åˆå¹¶æ‰€æœ‰ç»“æœï¼ˆç¼“å­˜çš„ + æ–°ä¸‹è½½çš„ï¼‰
        all_results = cached_results + downloaded_results
        
        if all_results:
            if alpha_pnls.empty:
                alpha_pnls = pd.concat(all_results, axis=1)
            else:
                alpha_pnls = pd.concat([alpha_pnls] + all_results, axis=1)
            alpha_pnls.sort_index(inplace=True)
            
            # æ›´æ¸…æ¥šåœ°æ˜¾ç¤ºPnLæ•°æ®ä¿¡æ¯
            total_alphas = len(alphas_from_cache) + len(downloaded_results)
            self.logger.info(f"ğŸ“Š PnLæ•°æ®æ±‡æ€»: {total_alphas} ä¸ªAlpha ({len(alphas_from_cache)}ç¼“å­˜+{len(downloaded_results)}æ–°ä¸‹è½½), {len(alpha_pnls)} ä¸ªäº¤æ˜“æ—¥, {alpha_pnls.shape[1]} åˆ—")
        
        return alpha_ids, alpha_pnls
