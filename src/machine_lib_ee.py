"""
ä½œè€…ï¼še.e. ï¼ˆåŸºäºé‘«é‘«é‘«ä»£ç æ‹“å±•è¡¥å……ï¼‰
å¾®ä¿¡ï¼šEnkidu_lin
æ—¥æœŸï¼š2025.08.24
"""
import os

import requests
import time
import pandas as pd
from itertools import product
from collections import defaultdict
import aiohttp
import asyncio
import logging as logger
import math

# è®¾ç½®æ—¥å¿—çº§åˆ«ä¸º INFO
logger.basicConfig(
    level=logger.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# å¯¼å…¥è·¯å¾„é…ç½®
from config import ROOT_PATH, RECORDS_PATH

def load_user_config(txt_file=None):
    """ä»config/user_info.txtåŠ è½½ç”¨æˆ·é…ç½®"""
    if txt_file is None:
        txt_file = os.path.join(ROOT_PATH, 'config', 'user_info.txt')
    config = {}
    try:
        with open(txt_file, 'r') as f:
            data = f.read().strip().split('\n')
            for line in data:
                if ': ' in line:
                    key, value = line.split(': ', 1)
                    # ç§»é™¤å¼•å·
                    if value.startswith("'") and value.endswith("'"):
                        value = value[1:-1]
                    elif value.startswith('"') and value.endswith('"'):
                        value = value[1:-1]
                    config[key] = value
    except FileNotFoundError:
        logger.warning(f"é…ç½®æ–‡ä»¶ {txt_file} æœªæ‰¾åˆ°")
    except Exception as e:
        logger.warning(f"è¯»å–é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    return config

def load_digging_config(config_file=None, for_step=None):
    """
    ä»config/digging_config.txtåŠ è½½æŒ–æ˜é…ç½®
    æ”¯æŒåˆ†é˜¶æ®µæ•°æ®é›†é…ç½®ï¼Œæä¾›æ›´å¥½çš„çµæ´»æ€§å’Œæ‰©å±•æ€§
    
    :param config_file: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
    :param for_step: æŒ‡å®šæ­¥éª¤('step1', 'step2', 'step3'ç­‰)ï¼Œç”¨äºé€‰æ‹©å¯¹åº”çš„æ•°æ®é›†é…ç½®
    :return: é…ç½®å­—å…¸
    """
    if config_file is None:
        config_file = os.path.join(ROOT_PATH, 'config', 'digging_config.txt')
    
    config = {}
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # è·³è¿‡æ³¨é‡Šå’Œç©ºè¡Œ
                if line.startswith('#') or not line or ':' not in line:
                    continue
                
                key, value = line.split(':', 1)
                key = key.strip()
                value = value.strip()
                
                # ç±»å‹è½¬æ¢
                if key in ['delay', 'decay', 'n_jobs', 'api_max_retries', 'api_retry_delay',
                           'check_batch_size', 'check_interval',
                            'max_concurrent_checks', 'cache_cleanup_interval',
                           'exponential_backoff_max', 'daily_submit_limit']:
                    value = int(value)
                elif key in ['api_call_interval', 'api_burst_delay']:
                    value = float(value)
                elif key in ['use_recommended_fields', 'api_rate_limit_backoff', 'enable_smart_delay',
                            'smart_retry_enabled']:
                    value = value.lower() in ['true', 'yes', '1', 'on']
                
                config[key] = value
                
    except FileNotFoundError:
        logger.info(f"é…ç½®æ–‡ä»¶ {config_file} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        # é»˜è®¤é…ç½®
        config = {
            'priority_dataset': 'analyst4',
            'region': 'USA',
            'universe': 'TOP3000',
            'delay': 1,
            'decay': 6,
            'neutralization': 'SUBINDUSTRY',
            'n_jobs': 3,
            'use_recommended_fields': False,
            'api_max_retries': 3,
            'api_retry_delay': 5,
            'api_rate_limit_backoff': True,
            'daily_submit_limit': 0,
            'daily_limit_timezone': '-4',

        }
    except Exception as e:
        logger.warning(f"è¯»å–æŒ–æ˜é…ç½®æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        logger.info("ä½¿ç”¨é»˜è®¤é…ç½®")
        # å‘ç”Ÿä»»ä½•å…¶ä»–é”™è¯¯æ—¶ä¹Ÿä½¿ç”¨é»˜è®¤é…ç½®
        config = {
            'priority_dataset': 'analyst4',
            'region': 'USA',
            'universe': 'TOP3000',
            'delay': 1,
            'decay': 6,
            'neutralization': 'SUBINDUSTRY',
            'n_jobs': 3,
            'use_recommended_fields': False,
            'api_max_retries': 3,
            'api_retry_delay': 5,
            'api_rate_limit_backoff': True,
            'daily_submit_limit': 0,
            'daily_limit_timezone': '-4',

        }
    
    return config


def parse_timezone_offset(timezone_str):
    """
    è§£ææ—¶åŒºå­—ç¬¦ä¸²å¹¶è¿”å›UTCåç§»å°æ—¶æ•°
    
    :param timezone_str: æ—¶åŒºå­—ç¬¦ä¸² ('UTC', 'LOCAL', 'ET', '+8', '-4' ç­‰)
    :return: UTCåç§»å°æ—¶æ•° (æ­£æ•°è¡¨ç¤ºä¸œæ—¶åŒºï¼Œè´Ÿæ•°è¡¨ç¤ºè¥¿æ—¶åŒº)
    """
    import time
    from datetime import datetime
    
    timezone_str = timezone_str.upper().strip()
    
    if timezone_str == 'UTC':
        return 0
    elif timezone_str == 'LOCAL':
        # è·å–æœ¬åœ°æ—¶åŒºåç§»
        if time.daylight:
            return -time.altzone / 3600  # å¤ä»¤æ—¶åç§»
        else:
            return -time.timezone / 3600  # æ ‡å‡†æ—¶é—´åç§»
    elif timezone_str == 'ET':
        # ç¾å›½ä¸œéƒ¨æ—¶é—´ (UTC-4 å¤ä»¤æ—¶, UTC-5 æ ‡å‡†æ—¶é—´)
        # ç®€åŒ–å¤„ç†ï¼Œä½¿ç”¨UTC-4
        return -4
    elif timezone_str.startswith('+') or timezone_str.startswith('-'):
        # æ•°å­—åç§»æ ¼å¼ï¼Œå¦‚ +8, -4
        try:
            return int(timezone_str)
        except ValueError:
            return 0
    else:
        logger.warning(f"âš ï¸  æœªè¯†åˆ«çš„æ—¶åŒºæ ¼å¼: {timezone_str}ï¼Œä½¿ç”¨UTC")
        return 0


def get_current_date_with_timezone(timezone_str='UTC'):
    """
    æ ¹æ®æŒ‡å®šæ—¶åŒºè·å–å½“å‰æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
    
    :param timezone_str: æ—¶åŒºå­—ç¬¦ä¸²
    :return: æ—¥æœŸå­—ç¬¦ä¸² YYYY-MM-DD
    """
    from datetime import datetime, timedelta
    
    offset_hours = parse_timezone_offset(timezone_str)
    utc_now = datetime.utcnow()
    local_now = utc_now + timedelta(hours=offset_hours)
    
    return local_now.strftime('%Y-%m-%d')


def login():
    # ä»txtæ–‡ä»¶è§£å¯†å¹¶è¯»å–æ•°æ®
    # txtæ ¼å¼:
    # password: 'password'
    # username: 'username'
    def load_decrypted_data(txt_file=None):
        if txt_file is None:
            txt_file = os.path.join(ROOT_PATH, 'config', 'user_info.txt')
        with open(txt_file, 'r') as f:
            data = f.read()
            data = data.strip().split('\n')

            data = {line.split(': ')[0]: line.split(': ')[1] for line in data if ': ' in line}

        return data['username'][1:-1], data['password'][1:-1]

    username, password = load_decrypted_data()

    # Create a session to persistently store the headers
    s = requests.Session()

    # Save credentials into session
    s.auth = (username, password)

    # Send a POST request to the /authentication API
    response = s.post('https://api.worldquantbrain.com/authentication')

    info_ = response.content.decode('utf-8')
    logger.info(info_)
    if "INVALID_CREDENTIALS" in info_:
        raise Exception("ä½ çš„è´¦å·å¯†ç æœ‰è¯¯ï¼Œè¯·åœ¨ã€config/user_info.txtã€‘è¾“å…¥æ­£ç¡®çš„é‚®ç®±å’Œå¯†ç ï¼\n"
                        "Your username or password is incorrect. Please enter the correct email and password!")
    return s

pd.set_option('expand_frame_repr', False)
pd.set_option('display.max_rows', 1000)

brain_api_url = os.environ.get("BRAIN_API_URL", "https://api.worldquantbrain.com")

basic_ops = ["log", "sqrt", "reverse", "inverse", "rank", "zscore", "log_diff", "s_log_1p",
             'fraction', 'quantile', "normalize", "scale_down"]

ts_ops = ["ts_rank", "ts_zscore", "ts_delta", "ts_sum", "ts_product",
          "ts_ir", "ts_std_dev", "ts_mean", "ts_arg_min", "ts_arg_max", "ts_min_diff",
          "ts_max_diff", "ts_returns", "ts_scale", "ts_skewness", "ts_kurtosis",
          "ts_quantile"]

ts_not_use = ["ts_min", "ts_max", "ts_delay", "ts_median", ]

arsenal = ["ts_moment", "ts_entropy", "ts_min_max_cps", "ts_min_max_diff", "inst_tvr", 'sigmoid',
           "ts_decay_exp_window", "ts_percentage", "vector_neut", "vector_proj", "signed_power"]

twin_field_ops = ["ts_corr", "ts_covariance", "ts_co_kurtosis", "ts_co_skewness", "ts_theilsen"]

group_ops = ["group_neutralize", "group_rank", "group_normalize", "group_scale", "group_zscore"]

group_ac_ops = ["group_sum", "group_max", "group_mean", "group_median", "group_min", "group_std_dev", ]

vec_ops = ["vec_avg", "vec_sum", "vec_ir", "vec_max",
                   "vec_count", "vec_skewness", "vec_stddev", "vec_choose"]

ops_set = basic_ops + ts_ops + arsenal + group_ops

# å»¶è¿Ÿç™»å½•ï¼Œé¿å…æ¨¡å—å¯¼å…¥æ—¶å°±ç™»å½•
s = None

def init_session():
    """åˆå§‹åŒ–sessionï¼Œè·å–å¯ç”¨çš„æ“ä½œç¬¦"""
    import time
    global s, ts_ops, basic_ops, group_ops
    if s is None:
        try:
            # ä½¿ç”¨SessionClientè·å–ä¼šè¯
            from session_client import get_session
            s = get_session()
            logger.info("âœ… machine_lib_ee ä½¿ç”¨SessionClient")
        except Exception as e:
            logger.warning(f"âŒ SessionClientä¸å¯ç”¨: {e}")
            logger.warning("ğŸ’¡ è¯·ç¡®ä¿SessionKeeperæ­£åœ¨è¿è¡Œå¹¶ç»´æŠ¤æœ‰æ•ˆä¼šè¯")
            raise
        
        # ä¿å­˜åŸå§‹æ“ä½œç¬¦åˆ—è¡¨
        globals()['ts_ops_original'] = ts_ops.copy()
        globals()['basic_ops_original'] = basic_ops.copy()
        globals()['group_ops_original'] = group_ops.copy()
        
        # è·å–é…ç½®ä¸­çš„é‡è¯•å‚æ•°
        try:
            config = load_digging_config()
            max_retries = config.get('api_max_retries', 3)
            retry_delay = config.get('api_retry_delay', 5)
            use_backoff = config.get('api_rate_limit_backoff', True)
        except:
            # å¦‚æœé…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
            max_retries = 3
            retry_delay = 5
            use_backoff = True
        
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ” è·å–å¯ç”¨æ“ä½œç¬¦ (å°è¯• {attempt + 1}/{max_retries})...")
                res = s.get("https://api.worldquantbrain.com/operators")
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                if res.status_code != 200:
                    logger.warning(f"âš ï¸  API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {res.status_code}")
                    if attempt < max_retries - 1:
                        logger.info(f"â³ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                        continue
                    else:
                        logger.warning("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä½¿ç”¨é»˜è®¤æ“ä½œç¬¦åˆ—è¡¨")
                        return s
                
                # è§£æå“åº”
                response_data = res.json()
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯å“åº”
                if isinstance(response_data, dict) and 'message' in response_data:
                    error_msg = response_data['message']
                    logger.warning(f"âš ï¸  API è¿”å›é”™è¯¯: {error_msg}")
                    
                    if 'rate limit' in error_msg.lower():
                        if attempt < max_retries - 1:
                            if use_backoff:
                                wait_time = retry_delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                                logger.info(f"ğŸš« é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯• (æŒ‡æ•°é€€é¿)...")
                            else:
                                wait_time = retry_delay
                                logger.info(f"ğŸš« é‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                            time.sleep(wait_time)
                            continue
                        else:
                            logger.warning("âŒ é€Ÿç‡é™åˆ¶æŒç»­å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤æ“ä½œç¬¦åˆ—è¡¨")
                            return s
                    else:
                        logger.warning(f"âŒ API é”™è¯¯: {error_msg}")
                        if attempt < max_retries - 1:
                            logger.info(f"â³ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                            time.sleep(retry_delay)
                            continue
                        else:
                            logger.warning("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä½¿ç”¨é»˜è®¤æ“ä½œç¬¦åˆ—è¡¨")
                            return s
                
                # æ­£å¸¸å“åº”ï¼Œæ„é€  DataFrame
                if isinstance(response_data, list) and len(response_data) > 0:
                    aval = pd.DataFrame(response_data)['name'].tolist()
                    
                    # è¿‡æ»¤æ‰€æœ‰æ“ä½œç¬¦ç±»å‹
                    original_ts = len(ts_ops)
                    original_basic = len(basic_ops)
                    original_group = len(group_ops)
                    
                    ts_ops = [op for op in ts_ops if op in aval]
                    basic_ops = [op for op in basic_ops if op in aval]
                    group_ops = [op for op in group_ops if op in aval]
                    
                    # æ›´æ–°å…¨å±€å˜é‡
                    globals()['ts_ops'] = ts_ops
                    globals()['basic_ops'] = basic_ops  
                    globals()['group_ops'] = group_ops
                    
                    logger.info(f"âœ… æˆåŠŸè·å– {len(aval)} ä¸ªå¯ç”¨æ“ä½œç¬¦")
                    logger.info(f"ğŸ“Š æ—¶é—´åºåˆ—æ“ä½œç¬¦: {len(ts_ops)}/{original_ts} ä¸ª")
                    logger.info(f"ğŸ“Š åŸºç¡€æ“ä½œç¬¦: {len(basic_ops)}/{original_basic} ä¸ª")
                    logger.info(f"ğŸ“Š ç»„æ“ä½œç¬¦: {len(group_ops)}/{original_group} ä¸ª")
                    
                    # æ˜¾ç¤ºè¢«è¿‡æ»¤æ‰çš„æ“ä½œç¬¦
                    ts_ops_orig = globals().get('ts_ops_original', [])
                    basic_ops_orig = globals().get('basic_ops_original', [])
                    group_ops_orig = globals().get('group_ops_original', [])
                    
                    filtered_ts = [op for op in ts_ops_orig if op not in aval]
                    filtered_basic = [op for op in basic_ops_orig if op not in aval] 
                    filtered_group = [op for op in group_ops_orig if op not in aval]
                    
                    if filtered_ts or filtered_basic or filtered_group:
                        logger.info(f"âš ï¸  è¿‡æ»¤æ‰çš„ä¸å¯ç”¨æ“ä½œç¬¦:")
                        if filtered_ts:
                            logger.info(f"   ts_ops: {filtered_ts}")
                        if filtered_basic:
                            logger.info(f"   basic_ops: {filtered_basic}")
                        if filtered_group:
                            logger.info(f"   group_ops: {filtered_group}")
                    
                    break
                else:
                    logger.warning("âš ï¸  API è¿”å›ç©ºæ•°æ®æˆ–æ ¼å¼å¼‚å¸¸")
                    if attempt < max_retries - 1:
                        logger.info(f"â³ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                        continue
                    else:
                        logger.warning("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä½¿ç”¨é»˜è®¤æ“ä½œç¬¦åˆ—è¡¨")
                        return s
                        
            except Exception as e:
                logger.warning(f"âŒ è·å–æ“ä½œç¬¦æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                if attempt < max_retries - 1:
                    logger.info(f"â³ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                    time.sleep(retry_delay)
                    continue
                else:
                    logger.warning("âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œä½¿ç”¨é»˜è®¤æ“ä½œç¬¦åˆ—è¡¨")
                    return s
    
    return s


def get_available_ops():
    """è·å–å¯ç”¨çš„æ“ä½œç¬¦åˆ—è¡¨"""
    init_session()
    global group_ops, twin_field_ops, vec_ops, arsenal, aval
    if 'aval' not in globals():
        try:
            res = s.get("https://api.worldquantbrain.com/operators")
            if res.status_code == 200:
                response_data = res.json()
                if isinstance(response_data, list) and len(response_data) > 0:
                    aval = pd.DataFrame(response_data)['name'].tolist()
                    group_ops = [op for op in group_ops if op in aval]
                    twin_field_ops = [op for op in twin_field_ops if op in aval]
                else:
                    logger.warning("âš ï¸  get_available_ops: API è¿”å›ç©ºæ•°æ®ï¼Œä½¿ç”¨é»˜è®¤æ“ä½œç¬¦")
                    aval = []
            else:
                logger.warning(f"âš ï¸  get_available_ops: API è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {res.status_code}")
                aval = []
        except Exception as e:
            logger.warning(f"âš ï¸  get_available_ops: è·å–æ“ä½œç¬¦å¼‚å¸¸: {e}")
            aval = []
        arsenal = [op for op in arsenal if op in aval]
        vec_ops = [op for op in vec_ops if op in aval]
    return aval


def list_chuckation(field_list, num):
    list_chucked = []
    lens = len(field_list)
    i = 0
    while i + num <= lens:
        list_chucked.append(field_list[i:i + num])
        i += num
    list_chucked.append(field_list[i:lens])
    return list_chucked


def batch_set_alpha_properties(
        s,
        alpha_data: list,  # [{"id": "alpha_id", "color": "GREEN"}, ...]
        max_batch_size: int = 100
):
    """
    æ‰¹é‡è®¾ç½®Alphaé¢œè‰²ï¼ˆä½¿ç”¨æ–°çš„æ‰¹é‡API - PATCH /alphasï¼‰
    æ³¨æ„ï¼šæ­¤APIç›®å‰åªæ”¯æŒæ‰¹é‡è®¾ç½®é¢œè‰²ï¼Œä¸æ”¯æŒnameã€tagsç­‰å…¶ä»–å±æ€§
    
    Args:
        s: sessionå¯¹è±¡
        alpha_data: Alphaæ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å¿…é¡»åŒ…å«idå’Œcolorå­—æ®µ
                   ä¾‹ï¼š[{"id": "ZVO5aLY", "color": "GREEN"}, {"id": "QR09ZpG", "color": "BLUE"}]
        max_batch_size: æœ€å¤§æ‰¹æ¬¡å¤§å°
    Returns:
        dict: {"success": int, "failed": int, "details": [...]}
    """
    if not alpha_data:
        return {"success": 0, "failed": 0, "details": []}
    
    success_count = 0
    failed_count = 0
    details = []
    
    # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å•æ¬¡è¯·æ±‚è¿‡å¤§
    for i in range(0, len(alpha_data), max_batch_size):
        batch = alpha_data[i:i + max_batch_size]
        batch_num = i//max_batch_size + 1
        
        # é‡è¯•æœºåˆ¶ï¼šå¤„ç†é€Ÿç‡é™åˆ¶
        retry_count = 0
        max_retries = 3
        batch_success = False
        
        while retry_count < max_retries and not batch_success:
            try:
                response = s.patch(
                    "https://api.worldquantbrain.com/alphas",
                    json=batch
                )
                
                if response.status_code == 200:
                    batch_success_count = len(batch)
                    success_count += batch_success_count
                    details.append(f"æ‰¹æ¬¡ {batch_num}: æˆåŠŸè®¾ç½® {batch_success_count} ä¸ªAlpha")
                    batch_success = True
                    
                elif response.status_code == 429:
                    # é€Ÿç‡é™åˆ¶ï¼Œç­‰å¾…åé‡è¯•
                    retry_count += 1
                    if retry_count < max_retries:
                        wait_time = min(2 ** retry_count, 8)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤§8ç§’
                        details.append(f"æ‰¹æ¬¡ {batch_num}: APIé€Ÿç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time}ç§’åé‡è¯• ({retry_count}/{max_retries})")
                        time.sleep(wait_time)
                    else:
                        # é‡è¯•æ¬¡æ•°ç”¨å®Œï¼Œæ ‡è®°ä¸ºå¤±è´¥
                        batch_failed = len(batch)
                        failed_count += batch_failed
                        details.append(f"æ‰¹æ¬¡ {batch_num}: APIé€Ÿç‡é™åˆ¶é‡è¯•å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
                        
                else:
                    # å…¶ä»–HTTPé”™è¯¯ï¼Œä¸é‡è¯•
                    batch_failed = len(batch)
                    failed_count += batch_failed
                    error_msg = f"æ‰¹æ¬¡ {batch_num}: HTTP {response.status_code}"
                    try:
                        error_data = response.json()
                        if 'message' in error_data:
                            error_msg += f" - {error_data['message']}"
                    except:
                        error_msg += f" - {response.text[:100]}"
                    details.append(error_msg)
                    break
                    
            except Exception as e:
                retry_count += 1
                if retry_count < max_retries:
                    wait_time = min(2 ** retry_count, 8)
                    details.append(f"æ‰¹æ¬¡ {batch_num}: ç½‘ç»œå¼‚å¸¸ï¼Œç­‰å¾… {wait_time}ç§’åé‡è¯• ({retry_count}/{max_retries}) - {str(e)}")
                    time.sleep(wait_time)
                else:
                    batch_failed = len(batch)
                    failed_count += batch_failed
                    details.append(f"æ‰¹æ¬¡ {batch_num}: ç½‘ç»œå¼‚å¸¸é‡è¯•å¤±è´¥ - {str(e)}")
                    
        # æ‰¹æ¬¡é—´ä¼‘æ¯ï¼Œé¿å…APIå‹åŠ›ï¼ˆæˆåŠŸçš„æ‰¹æ¬¡ä¹Ÿè¦ä¼‘æ¯ï¼‰
        if i + max_batch_size < len(alpha_data):
            time.sleep(1)
    
    return {
        "success": success_count,
        "failed": failed_count,
        "details": details
    }


def set_alpha_properties(
        s,
        alpha_id,
        name: str = None,
        color: str = None,
        selection_desc: str = None,
        combo_desc: str = None,
        tags: list = None,  # ['tag1', 'tag2']
):
    """
    Function changes alpha's description parameters
    Returns:
    - True: æˆåŠŸ
    - False: ä¸€èˆ¬å¤±è´¥
    - 'RATE_LIMITED': HTTP 429é€Ÿç‡é™åˆ¶ï¼Œå»ºè®®é‡æ–°ç™»å½•
    """
    params = {
        "category": None,
        "regular": {"description": None},
        "name": alpha_id if alpha_id else name  # é»˜è®¤ä½¿ç”¨alpha_idä½œä¸ºname
    }
    if color:
        params["color"] = color
    if tags:
        params["tags"] = tags
    if combo_desc:
        params["combo"] = {"description": combo_desc}
    if selection_desc:
        params["selection"] = {"description": selection_desc}

    try:
        response = s.patch(
            "https://api.worldquantbrain.com/alphas/" + alpha_id, json=params
        )
        
        # æ£€æŸ¥å“åº”çŠ¶æ€
        if response.status_code == 200:
            return True
        else:
            logger.warning(f"âŒ Alpha {alpha_id} å±æ€§è®¾ç½®å¤±è´¥: HTTP {response.status_code}")
            
            # ç‰¹åˆ«å¤„ç†429é”™è¯¯ - è¿”å›ç‰¹æ®Šæ ‡è¯†
            if response.status_code == 429:
                try:
                    error_text = response.text
                    if error_text:
                        logger.warning(f"é”™è¯¯è¯¦æƒ…: {error_text}")
                    try:
                        error_data = response.json()
                        if 'message' in error_data:
                            logger.info(f"APIé€Ÿç‡é™åˆ¶: {error_data['message']}")
                        if 'retry_after' in error_data:
                            logger.info(f"å»ºè®®ç­‰å¾…æ—¶é—´: {error_data['retry_after']}ç§’")
                    except:
                        logger.info("APIé€Ÿç‡é™åˆ¶è¶…å‡ºï¼Œå»ºè®®é‡æ–°ç™»å½•")
                except Exception as parse_error:
                    logger.info(f"è§£æ429é”™è¯¯å“åº”æ—¶å¼‚å¸¸: {parse_error}")
                
                # è¿”å›ç‰¹æ®Šæ ‡è¯†ï¼Œè¡¨ç¤ºéœ€è¦é‡æ–°ç™»å½•
                return 'RATE_LIMITED'
            
            # å…¶ä»–HTTPé”™è¯¯
            try:
                error_text = response.text
                if error_text:
                    logger.warning(f"é”™è¯¯è¯¦æƒ…: {error_text}")
            except Exception as parse_error:
                logger.info(f"è§£æé”™è¯¯å“åº”æ—¶å¼‚å¸¸: {parse_error}")
            return False
            
    except Exception as e:
        logger.warning(f"âŒ Alpha {alpha_id} å±æ€§è®¾ç½®å¼‚å¸¸: {e}")
        return False


def get_vec_fields(fields):
    vec_fields = []

    for field in fields:
        for vec_op in vec_ops:
            if vec_op == "vec_choose":
                vec_fields.append("%s(%s, nth=-1)" % (vec_op, field))
                vec_fields.append("%s(%s, nth=0)" % (vec_op, field))
            else:
                vec_fields.append("%s(%s)" % (vec_op, field))

    return (vec_fields)


def get_datasets(
        s,
        instrument_type: str = 'EQUITY',
        region: str = 'USA',
        delay: int = 1,
        universe: str = 'TOP3000'
):
    url = "https://api.worldquantbrain.com/data-sets?" + \
          f"instrumentType={instrument_type}&region={region}&delay={str(delay)}&universe={universe}"
    result = s.get(url)
    datasets_df = pd.DataFrame(result.json()['results'])
    return datasets_df


def get_datafields(
        s,
        instrument_type: str = 'EQUITY',
        region: str = 'USA',
        delay: int = 1,
        universe: str = 'TOP3000',
        dataset_id: str = '',
        search: str = ''
):
    """å¥å£®è·å–æ•°æ®å­—æ®µåˆ—è¡¨ï¼Œæ”¯æŒåˆ†é¡µã€é‡è¯•å’Œä¼šè¯åˆ·æ–°

    å˜æ›´ç‚¹ï¼š
    - ä¸å†ä¾èµ– response.json()['count']ï¼Œæ”¹ä¸ºåŸºäºåˆ†é¡µç›´åˆ°è¿”å›æ•°é‡<limit
    - å¯¹ 429 é€Ÿç‡é™åˆ¶å°Šé‡ Retry-After å¤´å¹¶é€€é¿é‡è¯•
    - å¯¹ 401/403 æˆ–å¼‚å¸¸å“åº”å°è¯•é‡æ–°ç™»å½•
    - ä»»ä½•å¼‚å¸¸æƒ…å†µä¸‹è¿”å›ç©º DataFrameï¼Œè€Œä¸æ˜¯æŠ›å‡º KeyError
    """
    base_url = "https://api.worldquantbrain.com/data-fields"
    limit = 50
    offset = 0
    aggregated_results = []

    # è¯»å–é‡è¯•é…ç½®
    try:
        cfg = load_digging_config()
        max_retries = cfg.get('api_max_retries', 3)
        retry_delay = cfg.get('api_retry_delay', 5)
        call_interval = cfg.get('api_call_interval', 0.0)
        burst_delay = cfg.get('api_burst_delay', 0.0)
    except Exception:
        max_retries = 3
        retry_delay = 5
        call_interval = 0.0
        burst_delay = 0.0

    # æ„å»ºå›ºå®šæŸ¥è¯¢å‚æ•°
    def make_params(current_offset: int):
        params = {
            'instrumentType': instrument_type,
            'region': region,
            'delay': str(delay),
            'universe': universe,
            'limit': str(limit),
            'offset': str(current_offset),
        }
        if dataset_id and not search:
            params['dataset.id'] = dataset_id
        if search:
            params['search'] = search
        return params

    while True:
        params = make_params(offset)

        # å•æ¬¡è¯·æ±‚çš„é‡è¯•ï¼ˆå¤„ç†429/401/ä¸´æ—¶é”™è¯¯ï¼‰
        attempt = 0
        while True:
            attempt += 1
            try:
                resp = s.get(base_url, params=params, timeout=15)

                # é€Ÿç‡é™åˆ¶å¤„ç†
                if resp.status_code == 429:
                    retry_after = resp.headers.get('Retry-After')
                    if retry_after is not None:
                        time.sleep(float(retry_after))
                    else:
                        time.sleep(retry_delay * attempt)
                    if attempt < max_retries:
                        continue
                    else:
                        logger.info("get_datafields: è¿ç»­é€Ÿç‡é™åˆ¶ï¼Œæå‰ç»“æŸåˆ†é¡µ")
                        return pd.DataFrame([])

                # æœªæˆæƒ/ç¦æ­¢ï¼Œå°è¯•è·å–æœ€æ–°ä¼šè¯
                if resp.status_code in (401, 403):
                    logger.info(f"get_datafields: {resp.status_code}ï¼Œå°è¯•è·å–æœ€æ–°ä¼šè¯åé‡è¯•")
                    try:
                        # ä½¿ç”¨SessionClientè·å–æœ€æ–°ä¼šè¯ï¼ˆSessionKeeperä¼šè‡ªåŠ¨ç»´æŠ¤ï¼‰
                        logger.info(f"get_datafields: è·å–æœ€æ–°ä¼šè¯...")
                        from session_client import get_session
                        new_session = get_session()
                        logger.info(f"get_datafields: SessionClientè·å–æˆåŠŸ")
                        
                        # é‡è¦ï¼šæ›´æ–°ä¼ å…¥çš„sessionå¯¹è±¡çš„å±æ€§
                        s.cookies.update(new_session.cookies)
                        s.headers.update(new_session.headers)
                        if hasattr(new_session, 'auth'):
                            s.auth = new_session.auth
                        logger.info("get_datafields: sessionæ›´æ–°æˆåŠŸ")
                    except Exception as e:
                        logger.error(f"get_datafields: ä¼šè¯æ›´æ–°å¤±è´¥({e})ï¼Œä½¿ç”¨SessionClient")
                        from session_client import get_session
                        new_session = get_session()
                        s.cookies.update(new_session.cookies)
                        s.headers.update(new_session.headers)
                        if hasattr(new_session, 'auth'):
                            s.auth = new_session.auth
                        logger.info(f"get_datafields: SessionClientæ¢å¤æˆåŠŸ")
                    if attempt < max_retries:
                        continue
                    else:
                        return pd.DataFrame([])

                if resp.status_code != 200:
                    logger.info(f"get_datafields: HTTP {resp.status_code}: {resp.text[:200]}")
                    if attempt < max_retries:
                        time.sleep(retry_delay)
                        continue
                    else:
                        return pd.DataFrame([])

                # å°è¯•è§£æJSON
                try:
                    data = resp.json()
                except Exception as e:
                    logger.info(f"get_datafields: JSONè§£æå¤±è´¥: {e}")
                    if attempt < max_retries:
                        time.sleep(retry_delay)
                        continue
                    else:
                        return pd.DataFrame([])

                # å¦‚æœè¿”å›çš„æ˜¯é”™è¯¯æ¶ˆæ¯æ ¼å¼ï¼Œå°è¯•é€€é¿
                if isinstance(data, dict) and 'message' in data and 'results' not in data:
                    logger.info(f"get_datafields: APIé”™è¯¯: {data.get('message')}")
                    if attempt < max_retries:
                        time.sleep(retry_delay)
                        continue
                    else:
                        return pd.DataFrame([])

                # æ­£å¸¸å¤„ç†ç»“æœ
                results = data.get('results', [])
                aggregated_results.extend(results)

                # æ§åˆ¶è¯·æ±‚èŠ‚å¥
                if call_interval > 0:
                    time.sleep(call_interval)

                # ç»“æŸæ¡ä»¶ï¼šè¿”å›æ•°é‡å°äºlimit
                if len(results) < limit:
                    return pd.DataFrame(aggregated_results)

                # å‡†å¤‡ä¸‹ä¸€é¡µ
                offset += limit
                # å¯é€‰çš„æ‰¹æ¬¡é—´å»¶è¿Ÿ
                if burst_delay > 0:
                    time.sleep(burst_delay)
                # è·³å‡ºé‡è¯•å¾ªç¯ï¼Œè¿›è¡Œä¸‹ä¸€é¡µ
                break

            except requests.RequestException as e:
                logger.info(f"get_datafields: è¯·æ±‚å¼‚å¸¸: {e}")
                if attempt < max_retries:
                    time.sleep(retry_delay)
                    continue
                else:
                    return pd.DataFrame([])
            except Exception as e:
                logger.info(f"get_datafields: æœªçŸ¥å¼‚å¸¸: {e}")
                return pd.DataFrame([])


def process_datafields(df, data_type):
    """å¤„ç†æ•°æ®å­—æ®µï¼Œæ”¯æŒç©ºDataFrameçš„å¥å£®å¤„ç†"""
    # æ£€æŸ¥DataFrameæ˜¯å¦ä¸ºç©ºæˆ–ç¼ºå°‘å¿…è¦åˆ—
    if df.empty or 'type' not in df.columns or 'id' not in df.columns:
        logger.info(f"process_datafields: DataFrameä¸ºç©ºæˆ–ç¼ºå°‘å¿…è¦åˆ—(type/id)ï¼Œè¿”å›ç©ºå­—æ®µåˆ—è¡¨")
        return []
    
    try:
        if data_type == "matrix":
            datafields = df[df['type'] == "MATRIX"]["id"].tolist()
        elif data_type == "vector":
            datafields = get_vec_fields(df[df['type'] == "VECTOR"]["id"].tolist())
        else:
            logger.info(f"process_datafields: æœªçŸ¥æ•°æ®ç±»å‹: {data_type}")
            return []

        tb_fields = []
        for field in datafields:
            tb_fields.append("winsorize(ts_backfill(%s, 120), std=4)" % field)
        return tb_fields
        
    except Exception as e:
        logger.info(f"process_datafields: å¤„ç†{data_type}å­—æ®µæ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        return []


def get_alphas(start_date, end_date, sharpe_th, fitness_th, longCount_th, shortCount_th, region, universe, delay,
               instrumentType, alpha_num, usage, tag: str = '', color_exclude='', s=None):
    # color None, RED, YELLOW, GREEN, BLUE, PURPLE
    if s is None:
        from session_client import get_session
        s = get_session()
    alpha_list = []
    next_alphas = []
    decay_alphas = []
    check_alphas = []
    # 3E large 3C less
    # æ­£çš„
    i = 0
    while True:
        # æ„å»ºåŸºç¡€URL
        url_e = (f"https://api.worldquantbrain.com/users/self/alphas?limit=100&offset={i}"
                 f"&tag%3D{tag}&is.longCount%3E={longCount_th}&is.shortCount%3E={shortCount_th}"
                 f"&settings.region={region}&is.sharpe%3E={sharpe_th}")
        
        # åªæœ‰åœ¨fitness_thä¸ä¸ºNoneæ—¶æ‰æ·»åŠ fitnessè¿‡æ»¤
        if fitness_th is not None:
            url_e += f"&is.fitness%3E={fitness_th}"
        
        url_e += (f"&settings.universe={universe}&status=UNSUBMITTED&dateCreated%3E={start_date}"
                 f"T00:00:00-04:00&dateCreated%3C{end_date}T00:00:00-04:00&type=REGULAR&color!={color_exclude}&"
                 f"settings.delay={delay}&settings.instrumentType={instrumentType}&order=-is.sharpe&hidden=false&type!=SUPER")

        response = s.get(url_e)
        # logger.info(response.json())
        try:
            logger.info(i)
            i += 100
            count = response.json()["count"]
            logger.info("count: %d" % count)
            alpha_list.extend(response.json()["results"])
            if i >= count or i == 9900:
                break
            time.sleep(0.01)
        except Exception as e:
            logger.info(f"Failed to get alphas: {e}")
            i -= 100
            logger.info("%d finished re-login" % i)
            from session_client import get_session
            s = get_session()

    # è´Ÿçš„
    if usage != "submit":
        i = 0
        while True:
            url_c = (f"https://api.worldquantbrain.com/users/self/alphas?limit=100&offset={i}"
                     f"&tag%3D{tag}&is.longCount%3E={longCount_th}&is.shortCount%3E={shortCount_th}"
                     f"&settings.region={region}&is.sharpe%3C=-{sharpe_th}&is.fitness%3C=-{fitness_th}"
                     f"&settings.universe={universe}&status=UNSUBMITTED&dateCreated%3E={start_date}"
                     f"T00:00:00-04:00&dateCreated%3C{end_date}T00:00:00-04:00&type=REGULAR&color!={color_exclude}&"
                     f"settings.delay={delay}&settings.instrumentType={instrumentType}&order=-is.sharpe&hidden=false&type!=SUPER")

            response = s.get(url_c)
            # logger.info(response.json())
            try:
                count = response.json()["count"]
                if i >= count or i == 9900:
                    break
                alpha_list.extend(response.json()["results"])
                i += 100
            except Exception as e:
                logger.info(f"Failed to get alphas: {e}")
                logger.info("%d finished re-login" % i)
                from session_client import get_session
                s = get_session()

    # logger.info(alpha_list)
    if len(alpha_list) == 0:
        if usage != "submit":
            return {"next": [], "decay": []}
        else:
            return {"check": []}

    # logger.info(response.json())
    if usage != "submit":
        for j in range(len(alpha_list)):
            alpha_id = alpha_list[j]["id"]
            name = alpha_list[j]["name"]
            dateCreated = alpha_list[j]["dateCreated"]
            sharpe = alpha_list[j]["is"]["sharpe"]
            fitness = alpha_list[j]["is"]["fitness"]
            turnover = alpha_list[j]["is"]["turnover"]
            margin = alpha_list[j]["is"]["margin"]
            longCount = alpha_list[j]["is"]["longCount"]
            shortCount = alpha_list[j]["is"]["shortCount"]
            decay = alpha_list[j]["settings"]["decay"]
            exp = alpha_list[j]['regular']['code']
            region = alpha_list[j]["settings"]["region"]

            concentrated_weight = next(
                (check.get('value', 0) for check in alpha_list[j]["is"]["checks"] if
                 check["name"] == "CONCENTRATED_WEIGHT"), 0)
            sub_universe_sharpe = next(
                (check.get('value', 99) for check in alpha_list[j]["is"]["checks"] if
                 check["name"] == "LOW_SUB_UNIVERSE_SHARPE"), 99)
            two_year_sharpe = next(
                (check.get('value', 99) for check in alpha_list[j]["is"]["checks"] if check["name"] == "LOW_2Y_SHARPE"),
                99)
            ladder_sharpe = next(
                (check.get('value', 99) for check in alpha_list[j]["is"]["checks"] if
                 check["name"] == "IS_LADDER_SHARPE"), 99)

            conditions = ((longCount > 100 or shortCount > 100) and
                          (concentrated_weight < 0.2) and
                        #   (abs(sub_universe_sharpe) > sharpe_th / 1.66) and
                          (abs(sub_universe_sharpe) > math.sqrt(1000/3000) * sharpe) and
                          (abs(two_year_sharpe) > sharpe_th) and
                          (abs(ladder_sharpe) > sharpe_th) and
                          (not (region == "CHN" and sharpe < 0))
                          )
            # if (sharpe > 1.2 and sharpe < 1.6) or (sharpe < -1.2 and sharpe > -1.6):
            if conditions:
                if sharpe < 0:
                    exp = "-%s" % exp
                rec = [alpha_id, exp, sharpe, turnover, fitness, margin, longCount, shortCount, dateCreated, decay]
                # logger.info(rec)
                if turnover > 0.7:
                    rec.append(decay * 4)
                    decay_alphas.append(rec)
                elif turnover > 0.6:
                    rec.append(decay * 3 + 3)
                    decay_alphas.append(rec)
                elif turnover > 0.5:
                    rec.append(decay * 3)
                    decay_alphas.append(rec)
                elif turnover > 0.4:
                    rec.append(decay * 2)
                    decay_alphas.append(rec)
                elif turnover > 0.35:
                    rec.append(decay + 4)
                    decay_alphas.append(rec)
                elif turnover > 0.3:
                    rec.append(decay + 2)
                    decay_alphas.append(rec)
                else:
                    next_alphas.append(rec)
        output_dict = {"next": next_alphas, "decay": decay_alphas}
        logger.info("count: %d" % (len(next_alphas) + len(decay_alphas)))
    else:
        for alpha_detail in alpha_list:
            id = alpha_detail["id"]
            type = alpha_detail["type"]
            author = alpha_detail["author"]
            instrumentType = alpha_detail["settings"]["instrumentType"]
            region = alpha_detail["settings"]["region"]
            universe = alpha_detail["settings"]["universe"]
            delay = alpha_detail["settings"]["delay"]
            decay = alpha_detail["settings"]["decay"]
            neutralization = alpha_detail["settings"]["neutralization"]
            truncation = alpha_detail["settings"]["truncation"]
            pasteurization = alpha_detail["settings"]["pasteurization"]
            unitHandling = alpha_detail["settings"]["unitHandling"]
            nanHandling = alpha_detail["settings"]["nanHandling"]
            language = alpha_detail["settings"]["language"]
            visualization = alpha_detail["settings"]["visualization"]
            code = alpha_detail["regular"]["code"]
            description = alpha_detail["regular"]["description"]
            operatorCount = alpha_detail["regular"]["operatorCount"]
            dateCreated = alpha_detail["dateCreated"]
            dateSubmitted = alpha_detail["dateSubmitted"]
            dateModified = alpha_detail["dateModified"]
            name = alpha_detail["name"]
            favorite = alpha_detail["favorite"]
            hidden = alpha_detail["hidden"]
            color = alpha_detail["color"]
            category = alpha_detail["category"]
            tags = alpha_detail["tags"]
            classifications = alpha_detail["classifications"]
            grade = alpha_detail["grade"]
            stage = alpha_detail["stage"]
            status = alpha_detail["status"]
            pnl = alpha_detail["is"]["pnl"]
            bookSize = alpha_detail["is"]["bookSize"]
            longCount = alpha_detail["is"]["longCount"]
            shortCount = alpha_detail["is"]["shortCount"]
            turnover = alpha_detail["is"]["turnover"]
            returns = alpha_detail["is"]["returns"]
            drawdown = alpha_detail["is"]["drawdown"]
            margin = alpha_detail["is"]["margin"]
            fitness = alpha_detail["is"]["fitness"]
            sharpe = alpha_detail["is"]["sharpe"]
            startDate = alpha_detail["is"]["startDate"]
            checks = alpha_detail["is"]["checks"]
            os = alpha_detail["os"]
            train = alpha_detail["train"]
            test = alpha_detail["test"]
            prod = alpha_detail["prod"]
            competitions = alpha_detail["competitions"]
            themes = alpha_detail["themes"]
            team = alpha_detail["team"]
            checks_df = pd.DataFrame(checks)
            pyramids = next(
                ([y['name'] for y in item['pyramids']] for item in checks if item['name'] == 'MATCHES_PYRAMID'), None)

            if any(checks_df["result"] == "FAIL"):
                # æœ€åŸºç¡€çš„é¡¹ç›®ä¸é€šè¿‡
                set_alpha_properties(s, id, color='RED')
                continue
            else:
                # é€šè¿‡äº†æœ€åŸºç¡€çš„é¡¹ç›®
                # æŠŠå…¨éƒ¨çš„ä¿¡æ¯ä»¥å­—å…¸çš„å½¢å¼è¿”å›
                rec = {"id": id, "type": type, "author": author, "instrumentType": instrumentType, "region": region,
                       "universe": universe, "delay": delay, "decay": decay, "neutralization": neutralization,
                       "truncation": truncation, "pasteurization": pasteurization, "unitHandling": unitHandling,
                       "nanHandling": nanHandling, "language": language, "visualization": visualization, "code": code,
                       "description": description, "operatorCount": operatorCount, "dateCreated": dateCreated,
                       "dateSubmitted": dateSubmitted, "dateModified": dateModified, "name": name, "favorite": favorite,
                       "hidden": hidden, "color": color, "category": category, "tags": tags,
                       "classifications": classifications, "grade": grade, "stage": stage, "status": status, "pnl": pnl,
                       "bookSize": bookSize, "longCount": longCount, "shortCount": shortCount, "turnover": turnover,
                       "returns": returns, "drawdown": drawdown, "margin": margin, "fitness": fitness, "sharpe": sharpe,
                       "startDate": startDate, "checks": checks, "os": os, "train": train, "test": test, "prod": prod,
                       "competitions": competitions, "themes": themes, "team": team, "pyramids": pyramids}
                check_alphas.append(rec)
        output_dict = {"check": check_alphas}

    # è¶…è¿‡äº†é™åˆ¶
    if usage == 'submit' and count >= 9900:
        if len(output_dict['check']) < len(alpha_list):
            # é‚£ä¹ˆå°±å†æ¥ä¸€é
            output_dict = get_alphas(start_date, end_date, sharpe_th, fitness_th, longCount_th, shortCount_th,
                                     region, universe, delay, instrumentType, alpha_num, usage, tag, color_exclude)
        else:
            raise Exception("Too many alphas to check!! over 10000, universe: %s, region: %s" % (universe, region))

    return output_dict


#ä¿ç•™ï¼Œä¼˜åŒ–æŒ–æ˜è„šæœ¬
def prune(next_alpha_recs, prefix, keep_num):
    # prefix is datafield prefix, like fnd6, mdl175 ...
    # keep_num is the num of top sharpe same-field alpha to keep 
    output = []
    num_dict = defaultdict(int)
    for rec in next_alpha_recs:
        exp = rec[1]
        field = exp.split(prefix)[-1].split(",")[0]
        if num_dict[field] < keep_num:
            num_dict[field] += 1
            decay = rec[-1]
            exp = rec[1]
            output.append([exp, decay])
    return output


def transform(next_alpha_recs):
    output = []
    for rec in next_alpha_recs:
        decay = rec[-1]
        exp = rec[1]
        output.append([exp, decay])
    return output


def first_order_factory(fields, ops_set):
    alpha_set = []
    for field in fields:
        # reverse op does the work
        alpha_set.append(field)
        # alpha_set.append("-%s"%field)
        for op in ops_set:

            if op == "ts_percentage":

                # lpha_set += ts_comp_factory(op, field, "percentage", [0.2, 0.5, 0.8])
                alpha_set += ts_comp_factory(op, field, "percentage", [0.5])


            elif op == "ts_decay_exp_window":

                # alpha_set += ts_comp_factory(op, field, "factor", [0.2, 0.5, 0.8])
                alpha_set += ts_comp_factory(op, field, "factor", [0.5])

            elif op == "ts_moment":

                alpha_set += ts_comp_factory(op, field, "k", [2, 3, 4])

            elif op == "ts_entropy":

                # alpha_set += ts_comp_factory(op, field, "buckets", [5, 10, 15, 20])
                alpha_set += ts_comp_factory(op, field, "buckets", [10])

            elif op.startswith("ts_") or op == "inst_tvr":

                alpha_set += ts_factory(op, field)

            elif op.startswith("group_"):

                alpha_set += group_factory(op, field, "usa")

            elif op.startswith("vector"):

                alpha_set += vector_factory(op, field)

            elif op == "signed_power":

                alpha = "%s(%s, 2)" % (op, field)
                alpha_set.append(alpha)

            else:
                alpha = "%s(%s)" % (op, field)
                alpha_set.append(alpha)

    return alpha_set


def get_group_second_order_factory(first_order, group_ops, region):
    second_order = []
    for fo in first_order:
        for group_op in group_ops:
            second_order += group_factory(group_op, fo, region)
    return second_order


def vector_factory(op, field):
    output = []
    vectors = ["cap"]

    for vector in vectors:
        alpha = "%s(%s, %s)" % (op, field, vector)
        output.append(alpha)

    return output


def trade_when_factory(op, field, region, delay=1):
    output = []
    open_events = ["ts_arg_max(volume, 5) == 0", "ts_corr(close, volume, 20) < 0",
                   "ts_corr(close, volume, 5) < 0", "ts_mean(volume,10)>ts_mean(volume,60)",
                   "group_rank(ts_std_dev(returns,60), sector) > 0.7", "ts_zscore(returns,60) > 2",
                   # "ts_skewness(returns,120)> 0.7",
                   "ts_arg_min(volume, 5) > 3",
                   "ts_std_dev(returns, 5) > ts_std_dev(returns, 20)",
                   "ts_arg_max(close, 5) == 0", "ts_arg_max(close, 20) == 0",
                   "ts_corr(close, volume, 5) > 0", "ts_corr(close, volume, 5) > 0.3",
                   "ts_corr(close, volume, 5) > 0.5",
                   "ts_corr(close, volume, 20) > 0", "ts_corr(close, volume, 20) > 0.3",
                   "ts_corr(close, volume, 20) > 0.5",
                   "ts_regression(returns, %s, 5, lag = 0, rettype = 2) > 0" % field,
                   "ts_regression(returns, %s, 20, lag = 0, rettype = 2) > 0" % field,
                   "ts_regression(returns, ts_step(20), 20, lag = 0, rettype = 2) > 0",
                   "ts_regression(returns, ts_step(5), 5, lag = 0, rettype = 2) > 0"]
    if delay==1:
        # exit_events = ["abs(returns) > 0.1", "-1", "days_from_last_change(ern3_pre_reptime) > 20"] # ern3_pre_reptimeå­—æ®µå¤±æ•ˆ
        exit_events = ["abs(returns) > 0.1", "-1"]
    else:
        exit_events = ["abs(returns) > 0.1", "-1"]

    usa_events = ["rank(rp_css_business) > 0.8", "ts_rank(rp_css_business, 22) > 0.8",
                  "rank(vec_avg(mws82_sentiment)) > 0.8",
                  "ts_rank(vec_avg(mws82_sentiment),22) > 0.8", "rank(vec_avg(nws48_ssc)) > 0.8",
                  "ts_rank(vec_avg(nws48_ssc),22) > 0.8", "rank(vec_avg(mws50_ssc)) > 0.8",
                  "ts_rank(vec_avg(mws50_ssc),22) > 0.8",
                  "ts_rank(vec_sum(scl12_alltype_buzzvec),22) > 0.9", "pcr_oi_270 < 1", "pcr_oi_270 > 1", ]

    asi_events = ["rank(vec_avg(mws38_score)) > 0.8", "ts_rank(vec_avg(mws38_score),22) > 0.8"]

    eur_events = ["rank(rp_css_business) > 0.8", "ts_rank(rp_css_business, 22) > 0.8",
                  "rank(vec_avg(oth429_research_reports_fundamental_keywords_4_method_2_pos)) > 0.8",
                  "ts_rank(vec_avg(oth429_research_reports_fundamental_keywords_4_method_2_pos),22) > 0.8",
                  "rank(vec_avg(mws84_sentiment)) > 0.8", "ts_rank(vec_avg(mws84_sentiment),22) > 0.8",
                  "rank(vec_avg(mws85_sentiment)) > 0.8", "ts_rank(vec_avg(mws85_sentiment),22) > 0.8",
                  "rank(mdl110_analyst_sentiment) > 0.8", "ts_rank(mdl110_analyst_sentiment, 22) > 0.8",
                  "rank(vec_avg(nws3_scores_posnormscr)) > 0.8",
                  "ts_rank(vec_avg(nws3_scores_posnormscr),22) > 0.8",
                  "rank(vec_avg(mws36_sentiment_words_positive)) > 0.8",
                  "ts_rank(vec_avg(mws36_sentiment_words_positive),22) > 0.8"]

    glb_events = ["rank(vec_avg(mdl109_news_sent_1m)) > 0.8",
                  "ts_rank(vec_avg(mdl109_news_sent_1m),22) > 0.8",
                  "rank(vec_avg(nws20_ssc)) > 0.8",
                  "ts_rank(vec_avg(nws20_ssc),22) > 0.8",
                  "vec_avg(nws20_ssc) > 0",
                  "rank(vec_avg(nws20_bee)) > 0.8",
                  "ts_rank(vec_avg(nws20_bee),22) > 0.8",
                  "rank(vec_avg(nws20_qmb)) > 0.8",
                  "ts_rank(vec_avg(nws20_qmb),22) > 0.8"]

    chn_events = ["rank(vec_avg(oth111_xueqiunaturaldaybasicdivisionstat_senti_conform)) > 0.8",
                  "ts_rank(vec_avg(oth111_xueqiunaturaldaybasicdivisionstat_senti_conform),22) > 0.8",
                  "rank(vec_avg(oth111_gubanaturaldaydevicedivisionstat_senti_conform)) > 0.8",
                  "ts_rank(vec_avg(oth111_gubanaturaldaydevicedivisionstat_senti_conform),22) > 0.8",
                  "rank(vec_avg(oth111_baragedivisionstat_regi_senti_conform)) > 0.8",
                  "ts_rank(vec_avg(oth111_baragedivisionstat_regi_senti_conform),22) > 0.8"]

    kor_events = ["rank(vec_avg(mdl110_analyst_sentiment)) > 0.8",
                  "ts_rank(vec_avg(mdl110_analyst_sentiment),22) > 0.8",
                  "rank(vec_avg(mws38_score)) > 0.8",
                  "ts_rank(vec_avg(mws38_score),22) > 0.8"]

    twn_events = ["rank(vec_avg(mdl109_news_sent_1m)) > 0.8",
                  "ts_rank(vec_avg(mdl109_news_sent_1m),22) > 0.8",
                  "rank(rp_ess_business) > 0.8",
                  "ts_rank(rp_ess_business,22) > 0.8"]

    for oe in open_events:
        for ee in exit_events:
            alpha = "%s(%s, %s, %s)" % (op, oe, field, ee)
            output.append(alpha)
    return output


def ts_factory(op, field):
    output = []
    # days = [3, 5, 10, 20, 60, 120, 240]
    days = [5, 22, 66, 120, 240]

    for day in days:
        alpha = "%s(%s, %d)" % (op, field, day)
        output.append(alpha)

    return output


def ts_comp_factory(op, field, factor, paras):
    output = []
    # l1, l2 = [3, 5, 10, 20, 60, 120, 240], paras
    l1, l2 = [5, 22, 66, 120, 240], paras
    comb = list(product(l1, l2))

    for day, para in comb:

        if type(para) == float:
            alpha = "%s(%s, %d, %s=%.1f)" % (op, field, day, factor, para)
        elif type(para) == int:
            alpha = "%s(%s, %d, %s=%d)" % (op, field, day, factor, para)

        output.append(alpha)

    return output


def group_factory(op, field, region):
    output = []
    vectors = ["cap"]

    chn_group_13 = ['pv13_h_min2_sector', 'pv13_di_6l', 'pv13_rcsed_6l', 'pv13_di_5l', 'pv13_di_4l',
                    'pv13_di_3l', 'pv13_di_2l', 'pv13_di_1l', 'pv13_parent', 'pv13_level']

    chn_group_1 = ['sta1_top3000c30', 'sta1_top3000c20', 'sta1_top3000c10', 'sta1_top3000c2', 'sta1_top3000c5']

    chn_group_2 = ['sta2_top3000_fact4_c10', 'sta2_top2000_fact4_c50', 'sta2_top3000_fact3_c20']

    chn_group_7 = ['oth171_region_sector_long_d1_sector', 'oth171_region_sector_short_d1_sector',
                   'oth171_sector_long_d1_sector', 'oth171_sector_short_d1_sector']

    hkg_group_13 = ['pv13_10_f3_g2_minvol_1m_sector', 'pv13_10_minvol_1m_sector', 'pv13_20_minvol_1m_sector',
                    'pv13_2_minvol_1m_sector', 'pv13_5_minvol_1m_sector', 'pv13_1l_scibr', 'pv13_3l_scibr',
                    'pv13_2l_scibr', 'pv13_4l_scibr', 'pv13_5l_scibr']

    hkg_group_1 = ['sta1_allc50', 'sta1_allc5', 'sta1_allxjp_513_c20', 'sta1_top2000xjp_513_c5']

    hkg_group_2 = ['sta2_all_xjp_513_all_fact4_c10', 'sta2_top2000_xjp_513_top2000_fact3_c10',
                   'sta2_allfactor_xjp_513_13', 'sta2_top2000_xjp_513_top2000_fact3_c20']

    hkg_group_8 = ['oth455_relation_n2v_p10_q50_w5_kmeans_cluster_5',
                   'oth455_relation_n2v_p10_q50_w4_kmeans_cluster_10',
                   'oth455_relation_n2v_p10_q50_w1_kmeans_cluster_20',
                   'oth455_partner_n2v_p50_q200_w4_kmeans_cluster_5',
                   'oth455_partner_n2v_p10_q50_w4_pca_fact3_cluster_10',
                   'oth455_customer_n2v_p50_q50_w1_kmeans_cluster_5']

    twn_group_13 = ['pv13_2_minvol_1m_sector', 'pv13_20_minvol_1m_sector', 'pv13_10_minvol_1m_sector',
                    'pv13_5_minvol_1m_sector', 'pv13_10_f3_g2_minvol_1m_sector', 'pv13_5_f3_g2_minvol_1m_sector',
                    'pv13_2_f4_g3_minvol_1m_sector']

    twn_group_1 = ['sta1_allc50', 'sta1_allxjp_513_c50', 'sta1_allxjp_513_c20', 'sta1_allxjp_513_c2',
                   'sta1_allc20', 'sta1_allxjp_513_c5', 'sta1_allxjp_513_c10', 'sta1_allc2', 'sta1_allc5']

    twn_group_2 = ['sta2_allfactor_xjp_513_0', 'sta2_all_xjp_513_all_fact3_c20',
                   'sta2_all_xjp_513_all_fact4_c20', 'sta2_all_xjp_513_all_fact4_c50']

    twn_group_8 = ['oth455_relation_n2v_p50_q200_w1_pca_fact1_cluster_20',
                   'oth455_relation_n2v_p10_q50_w3_kmeans_cluster_20',
                   'oth455_relation_roam_w3_pca_fact2_cluster_5',
                   'oth455_relation_n2v_p50_q50_w2_pca_fact2_cluster_10',
                   'oth455_relation_n2v_p10_q200_w5_pca_fact2_cluster_20',
                   'oth455_relation_n2v_p50_q50_w5_kmeans_cluster_5']

    usa_group_13 = ['pv13_h_min2_3000_sector', 'pv13_r2_min20_3000_sector', 'pv13_r2_min2_3000_sector',
                    'pv13_r2_min2_3000_sector', 'pv13_h_min2_focused_pureplay_3000_sector']

    usa_group_1 = ['sta1_top3000c50', 'sta1_allc20', 'sta1_allc10', 'sta1_top3000c20', 'sta1_allc5']

    usa_group_2 = ['sta2_top3000_fact3_c50', 'sta2_top3000_fact4_c20', 'sta2_top3000_fact4_c10']

    usa_group_3 = ['sta3_2_sector', 'sta3_3_sector', 'sta3_news_sector', 'sta3_peer_sector',
                   'sta3_pvgroup1_sector', 'sta3_pvgroup2_sector', 'sta3_pvgroup3_sector', 'sta3_sec_sector']

    usa_group_4 = ['rsk69_01c_1m', 'rsk69_57c_1m', 'rsk69_02c_2m', 'rsk69_5c_2m', 'rsk69_02c_1m',
                   'rsk69_05c_2m', 'rsk69_57c_2m', 'rsk69_5c_1m', 'rsk69_05c_1m', 'rsk69_01c_2m']

    usa_group_5 = ['anl52_2000_backfill_d1_05c', 'anl52_3000_d1_05c', 'anl52_3000_backfill_d1_02c',
                   'anl52_3000_backfill_d1_5c', 'anl52_3000_backfill_d1_05c', 'anl52_3000_d1_5c']

    usa_group_6 = ['mdl10_group_name']

    usa_group_7 = ['oth171_region_sector_long_d1_sector', 'oth171_region_sector_short_d1_sector',
                   'oth171_sector_long_d1_sector', 'oth171_sector_short_d1_sector']

    usa_group_8 = ['oth455_competitor_n2v_p10_q50_w1_kmeans_cluster_10',
                   'oth455_customer_n2v_p10_q50_w5_kmeans_cluster_10',
                   'oth455_relation_n2v_p50_q200_w5_kmeans_cluster_20',
                   'oth455_competitor_n2v_p50_q50_w3_kmeans_cluster_10',
                   'oth455_relation_n2v_p50_q50_w3_pca_fact2_cluster_10',
                   'oth455_partner_n2v_p10_q50_w2_pca_fact2_cluster_5',
                   'oth455_customer_n2v_p50_q50_w3_kmeans_cluster_5',
                   'oth455_competitor_n2v_p50_q200_w5_kmeans_cluster_20']

    asi_group_13 = ['pv13_20_minvol_1m_sector', 'pv13_5_f3_g2_minvol_1m_sector', 'pv13_10_f3_g2_minvol_1m_sector',
                    'pv13_2_f4_g3_minvol_1m_sector', 'pv13_10_minvol_1m_sector', 'pv13_5_minvol_1m_sector']

    asi_group_1 = ['sta1_allc50', 'sta1_allc10', 'sta1_minvol1mc50', 'sta1_minvol1mc20',
                   'sta1_minvol1m_normc20', 'sta1_minvol1m_normc50']
    asi_group_1 = []

    asi_group_8 = ['oth455_partner_roam_w3_pca_fact1_cluster_5',
                   'oth455_relation_roam_w3_pca_fact1_cluster_20',
                   'oth455_relation_roam_w3_kmeans_cluster_20',
                   'oth455_relation_n2v_p10_q200_w5_pca_fact1_cluster_20',
                   'oth455_relation_n2v_p10_q200_w5_pca_fact1_cluster_20',
                   'oth455_competitor_n2v_p10_q200_w1_kmeans_cluster_10']
    asi_group_8 = []

    jpn_group_1 = ['sta1_alljpn_513_c5', 'sta1_alljpn_513_c50', 'sta1_alljpn_513_c2', 'sta1_alljpn_513_c20']

    jpn_group_2 = ['sta2_top2000_jpn_513_top2000_fact3_c20', 'sta2_all_jpn_513_all_fact1_c5',
                   'sta2_allfactor_jpn_513_9', 'sta2_all_jpn_513_all_fact1_c10']

    jpn_group_8 = ['oth455_customer_n2v_p50_q50_w5_kmeans_cluster_10',
                   'oth455_customer_n2v_p50_q50_w4_kmeans_cluster_10',
                   'oth455_customer_n2v_p50_q50_w3_kmeans_cluster_10',
                   'oth455_customer_n2v_p50_q50_w2_kmeans_cluster_10',
                   'oth455_customer_n2v_p50_q200_w5_kmeans_cluster_10',
                   'oth455_customer_n2v_p50_q200_w5_kmeans_cluster_10']

    jpn_group_13 = ['pv13_2_minvol_1m_sector', 'pv13_2_f4_g3_minvol_1m_sector', 'pv13_10_minvol_1m_sector',
                    'pv13_10_f3_g2_minvol_1m_sector', 'pv13_all_delay_1_parent', 'pv13_all_delay_1_level']

    kor_group_13 = ['pv13_10_f3_g2_minvol_1m_sector', 'pv13_5_minvol_1m_sector', 'pv13_5_f3_g2_minvol_1m_sector',
                    'pv13_2_minvol_1m_sector', 'pv13_20_minvol_1m_sector', 'pv13_2_f4_g3_minvol_1m_sector']

    kor_group_1 = ['sta1_allc20', 'sta1_allc50', 'sta1_allc2', 'sta1_allc10', 'sta1_minvol1mc50',
                   'sta1_allxjp_513_c10', 'sta1_top2000xjp_513_c50']

    kor_group_2 = ['sta2_all_xjp_513_all_fact1_c50', 'sta2_top2000_xjp_513_top2000_fact2_c50',
                   'sta2_all_xjp_513_all_fact4_c50', 'sta2_all_xjp_513_all_fact4_c5']

    kor_group_8 = ['oth455_relation_n2v_p50_q200_w3_pca_fact3_cluster_5',
                   'oth455_relation_n2v_p50_q50_w4_pca_fact2_cluster_10',
                   'oth455_relation_n2v_p50_q200_w5_pca_fact2_cluster_5',
                   'oth455_relation_n2v_p50_q200_w4_kmeans_cluster_10',
                   'oth455_relation_n2v_p10_q50_w1_kmeans_cluster_10',
                   'oth455_relation_n2v_p50_q50_w5_pca_fact1_cluster_20']

    eur_group_13 = ['pv13_5_sector', 'pv13_2_sector', 'pv13_v3_3l_scibr', 'pv13_v3_2l_scibr', 'pv13_2l_scibr',
                    'pv13_52_sector', 'pv13_v3_6l_scibr', 'pv13_v3_4l_scibr', 'pv13_v3_1l_scibr']

    eur_group_1 = ['sta1_allc10', 'sta1_allc2', 'sta1_top1200c2', 'sta1_allc20', 'sta1_top1200c10']

    eur_group_2 = ['sta2_top1200_fact3_c50', 'sta2_top1200_fact3_c20', 'sta2_top1200_fact4_c50']

    eur_group_3 = ['sta3_6_sector', 'sta3_pvgroup4_sector', 'sta3_pvgroup5_sector']

    # eur_group_7 = ['oth171_region_sector_long_d1_sector', 'oth171_region_sector_short_d1_sector',
    #                'oth171_sector_long_d1_sector', 'oth171_sector_short_d1_sector']
    eur_group_7 = []

    eur_group_8 = ['oth455_relation_n2v_p50_q200_w3_pca_fact1_cluster_5',
                   'oth455_competitor_n2v_p50_q200_w4_kmeans_cluster_20',
                   'oth455_competitor_n2v_p50_q200_w5_pca_fact1_cluster_10',
                   'oth455_competitor_roam_w4_pca_fact2_cluster_20',
                   'oth455_relation_n2v_p10_q200_w2_pca_fact2_cluster_20',
                   'oth455_competitor_roam_w2_pca_fact3_cluster_20']

    glb_group_13 = ["pv13_10_f2_g3_sector", "pv13_2_f3_g2_sector", "pv13_2_sector", "pv13_52_all_delay_1_sector"]

    glb_group_3 = ['sta3_2_sector', 'sta3_3_sector', 'sta3_news_sector', 'sta3_peer_sector',
                   'sta3_pvgroup1_sector', 'sta3_pvgroup2_sector', 'sta3_pvgroup3_sector', 'sta3_sec_sector']

    glb_group_1 = ['sta1_allc20', 'sta1_allc10', 'sta1_allc50', 'sta1_allc5']

    glb_group_2 = ['sta2_all_fact4_c50', 'sta2_all_fact4_c20', 'sta2_all_fact3_c20', 'sta2_all_fact4_c10']

    glb_group_13 = ['pv13_2_sector', 'pv13_10_sector', 'pv13_3l_scibr', 'pv13_2l_scibr', 'pv13_1l_scibr',
                    'pv13_52_minvol_1m_all_delay_1_sector', 'pv13_52_minvol_1m_sector', 'pv13_52_minvol_1m_sector']

    # glb_group_7 = ['oth171_region_sector_long_d1_sector', 'oth171_region_sector_short_d1_sector',
    #                'oth171_sector_long_d1_sector', 'oth171_sector_short_d1_sector']
    glb_group_7 = []  # å­—æ®µæ¶ˆå¤±äº†

    glb_group_8 = ['oth455_relation_n2v_p10_q200_w5_kmeans_cluster_5',
                   'oth455_relation_n2v_p10_q50_w2_kmeans_cluster_5',
                   'oth455_relation_n2v_p50_q200_w5_kmeans_cluster_5',
                   'oth455_customer_n2v_p10_q50_w4_pca_fact3_cluster_20',
                   'oth455_competitor_roam_w2_pca_fact1_cluster_10',
                   'oth455_relation_n2v_p10_q200_w2_kmeans_cluster_5']

    amr_group_13 = ['pv13_4l_scibr', 'pv13_1l_scibr', 'pv13_hierarchy_min51_f1_sector',
                    'pv13_hierarchy_min2_600_sector', 'pv13_r2_min2_sector', 'pv13_h_min20_600_sector']

    amr_group_3 = ['sta3_news_sector', 'sta3_peer_sector', 'sta3_pvgroup1_sector', 'sta3_pvgroup2_sector',
                   'sta3_pvgroup3_sector']

    amr_group_8 = ['oth455_relation_roam_w1_pca_fact2_cluster_10',
                   'oth455_competitor_n2v_p50_q50_w4_kmeans_cluster_10',
                   'oth455_competitor_n2v_p50_q50_w3_kmeans_cluster_10',
                   'oth455_competitor_n2v_p50_q50_w2_kmeans_cluster_10',
                   'oth455_competitor_n2v_p50_q50_w1_kmeans_cluster_10',
                   'oth455_competitor_n2v_p50_q200_w5_kmeans_cluster_10']

    group_3 = ["oth171_region_sector_long_d1_sector", "oth171_region_sector_short_d1_sector",
               "oth171_sector_long_d1_sector", "oth171_sector_short_d1_sector"]

    # bps_group = "bucket(rank(fnd28_value_05480/close), range='0.2, 1, 0.2')"  # å­—æ®µä¸å¯ç”¨ï¼Œå·²ç¦ç”¨
    cap_group = "bucket(rank(cap), range='0.1, 1, 0.1')"
    sector_cap_group = "bucket(group_rank(cap,sector),range='0,1,0.1')"
    vol_group = "bucket(rank(ts_std_dev(ts_returns(close,1),20)),range = '0.1,1,0.1')"

    # groups = ["market", "sector", "industry", "subindustry", cap_group, sector_cap_group]
    #å‰”é™¤æ··ä¿¡å·åˆ†ç»„
    groups = ["market","sector", "industry", "subindustry", "country"]

    # if region == "chn" or region.lower() == "chn":
    #     groups += chn_group_13 + chn_group_1 + chn_group_2
    # if region == "twn" or region.lower() == "twn":
    #     groups += twn_group_13 + twn_group_1 + twn_group_2 + twn_group_8
    # if region == "asi" or region.lower() == "asi":
    #     groups += asi_group_13 + asi_group_1 + asi_group_8
    # if region == "usa" or region.lower() == "usa":
    #     groups += usa_group_13 + usa_group_2 + usa_group_4 + usa_group_8
    #     groups += usa_group_5 + usa_group_6
    #     # + usa_group_1 + usa_group_3 + usa_group_7
    # if region == "hkg" or region.lower() == "hkg":
    #     groups += hkg_group_13 + hkg_group_1 + hkg_group_2 + hkg_group_8
    # if region == "kor" or region.lower() == "kor":
    #     groups += kor_group_13 + kor_group_1 + kor_group_2 + kor_group_8
    # if region == "eur" or region.lower() == "eur":
    #     groups += eur_group_13 + eur_group_1 + eur_group_2 + eur_group_3 + eur_group_8 + eur_group_7
    # if region == "glb" or region.lower() == "glb":
    #     # groups += glb_group_13 + glb_group_8 + glb_group_3 + glb_group_1 + glb_group_7
    #     groups += []
    # if region == "amr" or region.lower() == "amr":
    #     groups += amr_group_3 + amr_group_13
    # if region == "jpn" or region.lower() == "jpn":
    #     groups += jpn_group_1 + jpn_group_2 + jpn_group_13 + jpn_group_8

    for group in groups:
        if op.startswith("group_vector"):
            for vector in vectors:
                alpha = "%s(%s,%s,densify(%s))" % (op, field, vector, group)
                output.append(alpha)
        elif op.startswith("group_percentage"):
            alpha = "%s(%s,densify(%s),percentage=0.5)" % (op, field, group)
            output.append(alpha)
        else:
            alpha = "%s(%s,densify(%s))" % (op, field, group)
            output.append(alpha)

    return output


async def async_login():
    """
    ä»YAMLæ–‡ä»¶åŠ è½½ç”¨æˆ·ä¿¡æ¯å¹¶å¼‚æ­¥ç™»å½•åˆ°æŒ‡å®šAPI
    """
    def load_decrypted_data(txt_file=None):
        if txt_file is None:
            txt_file = os.path.join(ROOT_PATH, 'config', 'user_info.txt')
        with open(txt_file, 'r') as f:
            data = f.read()
            data = data.strip().split('\n')
            data = {line.split(': ')[0]: line.split(': ')[1] for line in data if ': ' in line}

        return data['username'][1:-1], data['password'][1:-1]

    username, password = load_decrypted_data()

    # åˆ›å»ºä¸€ä¸ªaiohttpçš„Session
    conn = aiohttp.TCPConnector(ssl=False)
    session = aiohttp.ClientSession(connector=conn)

    try:
        # å‘é€ä¸€ä¸ªPOSTè¯·æ±‚åˆ°/authentication API
        async with session.post('https://api.worldquantbrain.com/authentication',
                                auth=aiohttp.BasicAuth(username, password)) as response:
            # æ£€æŸ¥çŠ¶æ€ç æ˜¯å¦ä¸º201ï¼Œç¡®ä¿ç™»å½•æˆåŠŸ
            if response.status == 201:
                logger.info("Login successful!")
            else:
                logger.info(f"Login failed! Status code: {response.status}, Response: {await response.text()}")
                await session.close()
                return None

        return session

    except aiohttp.ClientError as e:
        logger.info(f"Error during login request: {e}")
        await session.close()
    except Exception as e:
        logger.info(f"An unexpected error occurred: {e}")
        await session.close()

    return None


async def simulate_single(session_manager, alpha_expression, region_info, name, neut,
                          decay, delay, stone_bag, tags=['None'],
                          semaphore=None, max_trade='OFF'):
    """
    å•æ¬¡æ¨¡æ‹Ÿä¸€ä¸ªalphaè¡¨è¾¾å¼å¯¹åº”çš„æŸä¸ªåœ°åŒºçš„ä¿¡æ¯
    """
    async with semaphore:
        # æ³¨æ„ï¼šä¼šè¯ç®¡ç†å·²ç”±ç»Ÿä¸€ä¼šè¯ç®¡ç†å™¨åœ¨åå°å¤„ç†ï¼Œæ— éœ€åœ¨æ­¤æ£€æŸ¥è¿‡æœŸ

        region, uni = region_info
        alpha = "%s" % (alpha_expression)

        logger.info("Simulating for alpha: %s, region: %s, universe: %s, decay: %s" % (alpha, region, uni, decay))

        simulation_data = {
            'type': 'REGULAR',
            'settings': {
                'instrumentType': 'EQUITY',
                'region': region,
                'universe': uni,
                'delay': delay,
                'decay': decay,
                'neutralization': neut,
                'maxTrade': max_trade,
                'truncation': 0.08,
                'pasteurization': 'ON',
                'unitHandling': 'VERIFY',
                'nanHandling': 'ON',
                'language': 'FASTEXPR',
                'visualization': False,
            },
            'regular': alpha
        }

        while True:
            try:
                async with session_manager.post('https://api.worldquantbrain.com/simulations',
                                                json=simulation_data) as resp:
                    # é€Ÿç‡ä¸é‰´æƒå¤„ç†
                    if resp.status in (401, 403):
                        logger.info("Unauthorized/Forbidden on POST, session issue detected")
                        # è·å–æœ€æ–°ä¼šè¯ï¼ˆSessionKeeperä¼šè‡ªåŠ¨ç»´æŠ¤ï¼‰
                        try:
                            from session_client import get_session_cookies
                            import yarl
                            
                            # è·å–SessionClientç»´æŠ¤çš„cookies
                            current_cookies = get_session_cookies()
                            if current_cookies:
                                logger.info("ğŸ” æ£€æŸ¥SessionClientæ˜¯å¦å·²æœ‰æ–°çš„cookies...")
                                
                                # æ›´æ–°aiohttpçš„cookies
                                session_manager.cookie_jar.clear()
                                cookie_dict = {}
                                for name, value in current_cookies.items():
                                    cookie_dict[name] = value
                                
                                if cookie_dict:
                                    session_manager.cookie_jar.update_cookies(
                                        cookie_dict, 
                                        response_url=yarl.URL("https://api.worldquantbrain.com")
                                    )
                                    logger.info(f"âœ… aiohttp cookiesæ›´æ–°å®Œæˆï¼ŒåŒ…å«{len(cookie_dict)}ä¸ªcookie")
                                else:
                                    logger.warning("âš ï¸ å½“å‰cookiesä¸ºç©ºï¼Œå°è¯•å¼ºåˆ¶åˆ·æ–°...")
                                    raise Exception("å½“å‰cookiesä¸ºç©º")
                            else:
                                logger.warning("âš ï¸ æ— æ³•è·å–å½“å‰cookiesï¼Œå°è¯•å¼ºåˆ¶åˆ·æ–°...")
                                raise Exception("æ— æ³•è·å–å½“å‰cookies")
                                
                        except Exception as e:
                            # å¦‚æœè·å–ç°æœ‰cookieså¤±è´¥ï¼Œæ‰è¿›è¡Œå¼ºåˆ¶åˆ·æ–°
                            logger.info(f"ğŸ”„ è·å–ç°æœ‰cookieså¤±è´¥({e})ï¼Œå¼ºåˆ¶åˆ·æ–°ä¼šè¯...")
                    
                        await asyncio.sleep(2)  # ç»™ä¼šè¯æ›´æ–°ä¸€ç‚¹æ—¶é—´
                        continue
                    if resp.status == 429:
                        retry_after_hdr = resp.headers.get('Retry-After')
                        try:
                            wait_s = float(retry_after_hdr) if retry_after_hdr is not None else 5.0
                        except Exception:
                            wait_s = 5.0
                        logger.info(f"Rate limited on POST, sleep {wait_s}s then retry")
                        await asyncio.sleep(wait_s)
                        continue
                    simulation_progress_url = resp.headers.get('Location', 0)
                    if simulation_progress_url == 0:
                        # æ—  Locationï¼Œè§£æé”™è¯¯ä¸»ä½“ï¼Œå…¼å®¹å¤šç§ç»“æ„
                        try:
                            json_data = await resp.json()
                        except Exception:
                            json_data = await resp.text()

                        def extract_detail(payload):
                            if isinstance(payload, dict):
                                return payload.get('detail') or payload.get('message') or payload.get('error') or ''
                            return str(payload)

                        detail = extract_detail(json_data)
                        detail_str = str(detail)

                        # å¹¶å‘ä¸Šé™ â†’ ç­‰å¾…é‡è¯•
                        if 'CONCURRENT_SIMULATION_LIMIT_EXCEEDED' in detail_str:
                            logger.info("Limited by the number of simulations allowed per time")
                            await asyncio.sleep(5)
                            continue

                        # é‡å¤è¡¨è¾¾å¼ â†’ è®°å½•å¹¶è·³è¿‡
                        if 'duplicate' in detail_str.lower():
                            logger.info("Alpha expression is duplicated")
                            await asyncio.sleep(1)
                            return 0

                        # å…¶ä»–é”™è¯¯ â†’ æ‰“å°å¹¶è·³è¿‡
                        logger.info(f"detail: {detail_str}")
                        logger.info(f"json_data: {json_data}")
                        await asyncio.sleep(1)
                        return 0
                    else:
                        logger.info(f'simulation_progress_url: {simulation_progress_url}')
                        break
            except KeyError:
                logger.info("Location key error during simulation request")
                await asyncio.sleep(60)
                return
            except Exception as e:
                logger.info(f"An error occurred: {str(e)}")
                await asyncio.sleep(60)
                return

        while True:
            try:
                async with session_manager.get(simulation_progress_url) as resp:
                    # é€Ÿç‡ä¸é‰´æƒå¤„ç†
                    if resp.status in (401, 403):
                        logger.info("Unauthorized/Forbidden on GET, session issue detected")
                        # è·å–æœ€æ–°ä¼šè¯ï¼ˆSessionKeeperä¼šè‡ªåŠ¨ç»´æŠ¤ï¼‰
                        try:
                            from session_client import get_session_cookies
                            import yarl
                            
                            # è·å–SessionClientç»´æŠ¤çš„cookies
                            current_cookies = get_session_cookies()
                            if current_cookies:
                                logger.info("ğŸ” æ£€æŸ¥SessionClientæ˜¯å¦å·²æœ‰æ–°çš„cookies...")
                                
                                # æ›´æ–°aiohttpçš„cookies
                                session_manager.cookie_jar.clear()
                                cookie_dict = {}
                                for name, value in current_cookies.items():
                                    cookie_dict[name] = value
                                
                                if cookie_dict:
                                    session_manager.cookie_jar.update_cookies(
                                        cookie_dict, 
                                        response_url=yarl.URL("https://api.worldquantbrain.com")
                                    )
                                    logger.info(f"âœ… aiohttp cookiesæ›´æ–°å®Œæˆï¼ŒåŒ…å«{len(cookie_dict)}ä¸ªcookie")
                                else:
                                    logger.warning("âš ï¸ å½“å‰cookiesä¸ºç©ºï¼Œå°è¯•å¼ºåˆ¶åˆ·æ–°...")
                                    raise Exception("å½“å‰cookiesä¸ºç©º")
                            else:
                                logger.warning("âš ï¸ æ— æ³•è·å–å½“å‰cookiesï¼Œå°è¯•å¼ºåˆ¶åˆ·æ–°...")
                                raise Exception("æ— æ³•è·å–å½“å‰cookies")
                                
                        except Exception as e:
                            # å¦‚æœè·å–ç°æœ‰cookieså¤±è´¥ï¼Œæ‰è¿›è¡Œå¼ºåˆ¶åˆ·æ–°
                            logger.info(f"ğŸ”„ è·å–ç°æœ‰cookieså¤±è´¥({e})ï¼Œå¼ºåˆ¶åˆ·æ–°ä¼šè¯...")
                        
                        await asyncio.sleep(2)  # ç»™ä¼šè¯æ›´æ–°ä¸€ç‚¹æ—¶é—´
                        continue
                    if resp.status == 429:
                        retry_after_hdr = resp.headers.get('Retry-After')
                        try:
                            wait_s = float(retry_after_hdr) if retry_after_hdr is not None else 5.0
                        except Exception:
                            wait_s = 5.0
                        logger.info(f"Rate limited on GET, sleep {wait_s}s then retry")
                        await asyncio.sleep(wait_s)
                        continue
                    
                    # é JSON å“åº”ï¼ˆå¦‚ 504 HTML ç½‘å…³é¡µï¼‰å¥å£®å¤„ç†
                    content_type = (resp.headers.get('Content-Type') or '').lower()
                    if 'application/json' not in content_type:
                        try:
                            body_preview = (await resp.text())[:200]
                        except Exception:
                            body_preview = '<non-json>'
                        logger.info(f"Non-JSON progress response: status={resp.status}, content-type={content_type}, body[:200]={body_preview}")
                        if resp.status in (500, 502, 503, 504, 408):
                            await asyncio.sleep(5)
                            continue
                        else:
                            await asyncio.sleep(2)
                            continue

                    json_data = await resp.json()
                    # è·å–å“åº”å¤´å¹¶å¤„ç† Retry-After
                    retry_after_hdr = resp.headers.get('Retry-After')
                    try:
                        retry_after_val = float(retry_after_hdr) if retry_after_hdr is not None else 0.0
                    except Exception:
                        retry_after_val = 0.0
                    if retry_after_val <= 0:
                        break
                    await asyncio.sleep(retry_after_val)
            except Exception as e:
                logger.info(f"Error while checking progress: {str(e)}")
                await asyncio.sleep(60)

        logger.info("%s done simulating, getting alpha details" % (simulation_progress_url))
        try:
            # é¦–å…ˆæ£€æŸ¥æ¨¡æ‹ŸçŠ¶æ€
            status = json_data.get("status")
            if status == "ERROR":
                # æ¨¡æ‹Ÿå¤±è´¥ï¼Œè®°å½•å¤±è´¥è¡¨è¾¾å¼
                message = json_data.get("message", "Unknown error")
                logger.info(f"Simulation failed: {message}")
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºçœŸæ­£çš„è¡¨è¾¾å¼é”™è¯¯ï¼ˆä¸æ˜¯ä¸´æ—¶é—®é¢˜ï¼‰
                message_str = str(message).lower() if message else ""
                is_expression_error = (
                    "end of input" in message_str or
                    "syntax" in message_str or
                    "parse" in message_str or
                    "invalid" in message_str or
                    "undefined" in message_str or
                    "unknown" in message_str or
                    ("error" in message_str and "duplicate" not in message_str)
                )
                
                if is_expression_error:
                    failure_reason = message if message else "Expression error"
                    if "end of input" in message_str:
                        failure_reason = "Unexpected end of input"
                    elif "syntax" in message_str:
                        failure_reason = "Syntax error"
                    elif "parse" in message_str:
                        failure_reason = "Parse error"
                    elif "invalid" in message_str:
                        failure_reason = "Invalid expression"
                    elif "undefined" in message_str or "unknown" in message_str:
                        failure_reason = "Undefined function or field"
                    
                    await _record_failed_expression(
                        alpha_expression=alpha_expression,
                        tag_name=name,
                        failure_reason=failure_reason,
                        error_details=str(json_data)
                    )
                else:
                    logger.info("Skipping record - temporary failure (duplication, rate limit, etc.)")
                
                return 0
            
            alpha_id = json_data.get("alpha")
            if not alpha_id:
                logger.info(f"No alpha_id returned for simulation: {simulation_progress_url}")
                return 0

            # è°ƒç”¨å±æ€§è®¾ç½®å‡½æ•°ï¼Œå¹¶ä¼ é€’å¿…è¦çš„æ•°æ®åº“å†™å…¥å‚æ•°
            success = await async_set_alpha_properties(session_manager,
                                             alpha_id,
                                             name="%s" % name,
                                             color=None,
                                             tags=tags,
                                             # ä¼ é€’æ•°æ®åº“å†™å…¥æ‰€éœ€å‚æ•°
                                             alpha_expression=alpha,
                                             tag_name=name)

            # stone_bag.append(alpha_id)

        except KeyError:
            logger.info("Failed to retrieve alpha ID for: %s" % simulation_progress_url)
        except Exception as e:
            logger.info(f"An error occurred while setting alpha properties: {str(e)}")

        # return stone_bag
        return 0


async def async_set_alpha_properties(
        session,  # aiohttp çš„ session
        alpha_id,
        name: str = None,
        color: str = None,
        selection_desc: str = None,
        combo_desc: str = None,
        tags: list = None,
        # æ–°å¢å‚æ•°ç”¨äºæ•°æ®åº“å†™å…¥
        alpha_expression: str = None,
        tag_name: str = None,
):
    """
    å¼‚æ­¥å‡½æ•°ï¼Œä¿®æ”¹ alpha çš„æè¿°å‚æ•°
    æˆåŠŸåå†™å…¥æ•°æ®åº“
    
    Returns:
        bool: True if successful, False otherwise
    """

    params = {
        "category": None,
        "regular": {"description": None},
        "name": alpha_id  # ä½¿ç”¨alpha_idä½œä¸ºnameï¼Œä¸ä½¿ç”¨tagåç§°
    }
    if color:
        params["color"] = color
    if tags:
        params["tags"] = tags
    if combo_desc:
        params["combo"] = {"description": combo_desc}
    if selection_desc:
        params["selection"] = {"description": selection_desc}

    url = f"https://api.worldquantbrain.com/alphas/{alpha_id}"

    try:
        async with session.patch(url, json=params) as response:
            # æ£€æŸ¥çŠ¶æ€ç ï¼Œç¡®ä¿è¯·æ±‚æˆåŠŸ
            if response.status == 200:
                logger.info(f"Alpha {alpha_id} properties updated successfully! Tag: {tags}")
                
                # âœ… å±æ€§è®¾ç½®æˆåŠŸåï¼Œå†™å…¥æ•°æ®åº“
                if alpha_expression and tag_name:
                    await _write_to_database(alpha_expression, tag_name, alpha_id)
                
                return True
            else:
                logger.info(
                    f"Failed to update alpha {alpha_id}. Status code: {response.status}, Response: {await response.text()}")
                return False

    except aiohttp.ClientError as e:
        logger.info(f"Error during patch request for alpha {alpha_id}: {e}")
        return False
    except Exception as e:
        logger.info(f"An unexpected error occurred for alpha {alpha_id}: {e}")
        return False


async def _write_to_database(alpha_expression: str, tag_name: str, alpha_id: str = None):
    """
    å¼‚æ­¥å†™å…¥å› å­è¡¨è¾¾å¼åˆ°æ•°æ®åº“çš„è¾…åŠ©å‡½æ•°
    
    Args:
        alpha_expression: å› å­è¡¨è¾¾å¼
        tag_name: æ ‡ç­¾åç§°ï¼Œå¦‚ "USA_1_EQUITY_TOP3000_fundamental6_step1"
        alpha_id: Alpha IDï¼ˆç”¨äºæ—¥å¿—ï¼‰
    """
    logger.debug(f"ğŸ” _write_to_databaseè°ƒç”¨å‚æ•°: alpha_expression='{alpha_expression}', tag_name='{tag_name}', alpha_id='{alpha_id}'")
    try:
        # å¯¼å…¥æ•°æ®åº“ç®¡ç†å™¨ï¼ˆéœ€è¦åœ¨å‡½æ•°å†…å¯¼å…¥é¿å…å¾ªç¯å¯¼å…¥ï¼‰
        import sys
        import os
        project_root = os.path.dirname(RECORDS_PATH)
        if project_root not in sys.path:
            sys.path.append(project_root)
        from database.db_manager import FactorDatabaseManager
        from database.partitioned_db_manager import PartitionedFactorManager
        
        # è§£ææ ‡ç­¾ä¿¡æ¯ - tag_nameæ˜¯æ–°æ ¼å¼tagå¦‚"USA_1_EQUITY_TOP3000_fundamental6_step1"
        # æ–°æ ¼å¼ï¼šregion_delay_instrumentType_universe_dataset_stepN
        parts = tag_name.split('_')
        
        logger.debug(f"ğŸ” Tagè§£æè°ƒè¯•: tag_name='{tag_name}', parts={parts}, len={len(parts)}")
        
        if len(parts) >= 6:
            # æ–°æ ¼å¼tagï¼šUSA_1_EQUITY_TOP3000_fundamental6_step1
            region = parts[0].upper()
            delay = parts[1]
            instrument_type = parts[2]
            universe = parts[3]
            dataset_id = parts[4]  # æ•°æ®é›†IDåœ¨ç¬¬5ä¸ªä½ç½®
            step_part = parts[5]   # step1
            
            logger.debug(f"ğŸ” æ–°æ ¼å¼è§£æ: region={region}, dataset_id={dataset_id}, step_part={step_part}")
            
            try:
                step = int(step_part.replace('step', ''))
            except:
                step = 1
        else:
            # å…¼å®¹æ—§æ ¼å¼tagå¦‚"fundamental2_usa_1step"
            region = 'USA'  # é»˜è®¤
            step = 1      # é»˜è®¤
            
            # æå–regionå’Œstep
            for part in parts:
                if part.lower() in ['usa', 'chn', 'eur', 'asi', 'hkg', 'twn', 'kor', 'jpn', 'glb', 'amr']:
                    region = part.upper()
                elif 'step' in part:
                    try:
                        step = int(part.replace('step', ''))
                    except:
                        pass
            
            # æ„é€ åŸºç¡€dataset_idï¼ˆå»é™¤regionå’Œstepéƒ¨åˆ†ï¼‰
            base_dataset_parts = []
            for part in parts:
                if (part.lower() not in ['usa', 'chn', 'eur', 'asi', 'hkg', 'twn', 'kor', 'jpn', 'glb', 'amr'] 
                    and 'step' not in part):
                    base_dataset_parts.append(part)
            
            dataset_id = '_'.join(base_dataset_parts)
        
        # è·å–æ•°æ®åº“ç®¡ç†å™¨ - æ”¯æŒåˆ†åº“åŠŸèƒ½
        db_path_full = os.path.join(os.path.dirname(RECORDS_PATH), 'database', 'factors.db')
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨åˆ†åº“åŠŸèƒ½ï¼ˆè¯»å–é…ç½®ï¼‰
        try:
            from config import load_digging_config
            config = load_digging_config()
            use_partitioned_db = config.get('use_partitioned_db', True)
            
            if use_partitioned_db:
                db = PartitionedFactorManager(db_path_full)
            else:
                db = FactorDatabaseManager(db_path_full)
        except:
            # é…ç½®è¯»å–å¤±è´¥æ—¶ï¼Œé»˜è®¤ä½¿ç”¨åˆ†åº“åŠŸèƒ½
            db = PartitionedFactorManager(db_path_full)
        
        # å†™å…¥å› å­è¡¨è¾¾å¼åˆ°æ•°æ®åº“ï¼ˆä½¿ç”¨æ­£ç¡®è§£æçš„dataset_idï¼‰
        db.add_factor_expression(
            expression=alpha_expression,
            dataset_id=dataset_id,
            region=region,
            step=step
        )
        
        alpha_preview = alpha_expression[:50] + "..." if len(alpha_expression) > 50 else alpha_expression
        logger.info(f"âœ… æ•°æ®åº“å†™å…¥æˆåŠŸ [{alpha_id}]: tag_name='{tag_name}' -> [dataset_id={dataset_id}, region={region}, step={step}] - {alpha_preview}")
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“å†™å…¥å¤±è´¥ [{alpha_id}]: {e}")
        # å¦‚æœæ•°æ®åº“å†™å…¥å¤±è´¥ï¼Œå›é€€åˆ°æ–‡ä»¶å†™å…¥ï¼ˆå…¼å®¹æ€§ä¿éšœï¼‰
        try:
            import aiofiles
            async with aiofiles.open(os.path.join(RECORDS_PATH, f'{tag_name}_simulated_alpha_expression.txt'), mode='a') as f:
                await f.write(alpha_expression + '\n')
            logger.warning(f"âš ï¸  å·²å›é€€åˆ°æ–‡ä»¶å†™å…¥: {tag_name}_simulated_alpha_expression.txt")
        except Exception as file_error:
            logger.error(f"âŒ æ–‡ä»¶å†™å…¥ä¹Ÿå¤±è´¥: {file_error}")


async def _record_failed_expression(alpha_expression: str, tag_name: str, 
                                   failure_reason: str = None, error_details: str = None):
    """
    è®°å½•å¤±è´¥çš„å› å­è¡¨è¾¾å¼åˆ°æ•°æ®åº“ï¼ˆä»…è®°å½•çœŸæ­£æ— æ³•æ¨¡æ‹Ÿçš„è¡¨è¾¾å¼ï¼‰
    
    Args:
        alpha_expression: å¤±è´¥çš„å› å­è¡¨è¾¾å¼
        tag_name: æ ‡ç­¾åç§°ï¼Œå¦‚ "fundamental2_usa_1step"
        failure_reason: å¤±è´¥åŸå› 
        error_details: è¯¦ç»†é”™è¯¯ä¿¡æ¯
    """
    try:
        # å¯¼å…¥æ•°æ®åº“ç®¡ç†å™¨
        import sys
        import os
        project_root = os.path.dirname(RECORDS_PATH)
        if project_root not in sys.path:
            sys.path.append(project_root)
        from database.db_manager import FactorDatabaseManager
        from database.partitioned_db_manager import PartitionedFactorManager
        
        # è§£ææ ‡ç­¾ä¿¡æ¯ - ä¸_write_to_databaseä¿æŒä¸€è‡´
        parts = tag_name.split('_')
        
        if len(parts) >= 6:
            # æ–°æ ¼å¼tagï¼šUSA_1_EQUITY_TOP3000_fundamental6_step1
            region = parts[0].upper()
            dataset_id = parts[4]  # æ•°æ®é›†IDåœ¨ç¬¬5ä¸ªä½ç½®
            step_part = parts[5]   # step1
            
            try:
                step = int(step_part.replace('step', ''))
            except:
                step = 1
        else:
            # å…¼å®¹æ—§æ ¼å¼tagå¦‚"fundamental2_usa_1step"
            parts_lower = [p.lower() for p in parts]
            region = 'USA'  # é»˜è®¤
            step = 1      # é»˜è®¤
            
            # æå–regionå’Œstep
            for part in parts_lower:
                if part in ['usa', 'chn', 'eur', 'asi', 'hkg', 'twn', 'kor', 'jpn', 'glb', 'amr']:
                    region = part.upper()
                elif 'step' in part:
                    try:
                        step = int(part.replace('step', ''))
                    except:
                        pass
            
            # æ„é€ åŸºç¡€dataset_id
            base_dataset_parts = []
            for part in parts_lower:
                if (part not in ['usa', 'chn', 'eur', 'asi', 'hkg', 'twn', 'kor', 'jpn', 'glb', 'amr'] 
                    and 'step' not in part):
                    base_dataset_parts.append(part)
            
            dataset_id = '_'.join(base_dataset_parts)
        
        # è·å–æ•°æ®åº“ç®¡ç†å™¨ - æ”¯æŒåˆ†åº“åŠŸèƒ½
        db_path_full = os.path.join(os.path.dirname(RECORDS_PATH), 'database', 'factors.db')
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨åˆ†åº“åŠŸèƒ½ï¼ˆè¯»å–é…ç½®ï¼‰
        try:
            from config import load_digging_config
            config = load_digging_config()
            use_partitioned_db = config.get('use_partitioned_db', True)
            
            if use_partitioned_db:
                db = PartitionedFactorManager(db_path_full)
            else:
                db = FactorDatabaseManager(db_path_full)
        except:
            # é…ç½®è¯»å–å¤±è´¥æ—¶ï¼Œé»˜è®¤ä½¿ç”¨åˆ†åº“åŠŸèƒ½
            db = PartitionedFactorManager(db_path_full)
        
        # å†™å…¥å¤±è´¥è®°å½•ï¼ˆæ³¨æ„ï¼šfailed_expressionsè¡¨ä»åœ¨ä¸»æ•°æ®åº“ä¸­ï¼‰
        success = db.add_failed_expression(
            expression=alpha_expression,
            dataset_id=dataset_id,
            region=region,
            step=step,
            failure_reason=failure_reason,
            error_details=error_details
        )
        
        if success:
            alpha_preview = alpha_expression[:80] + "..." if len(alpha_expression) > 80 else alpha_expression
            logger.warning(f"ğŸ“ ğŸ¯ å¤±è´¥è®°å½•å·²ä¿å­˜: {failure_reason} - {alpha_preview}")
        else:
            logger.error(f"âŒ å¤±è´¥è®°å½•ä¿å­˜å¤±è´¥: {failure_reason}")
            
    except Exception as e:
        logger.error(f"âŒ è®°å½•å¤±è´¥è¡¨è¾¾å¼æ—¶å‡ºé”™: {e}")
        # æœ€åå…œåº•ï¼šå†™å…¥ä¸´æ—¶æ–‡ä»¶
        try:
            import aiofiles
            from datetime import datetime
            failure_log_path = os.path.join(RECORDS_PATH, 'failed_expressions.log')
            async with aiofiles.open(failure_log_path, mode='a') as f:
                timestamp = datetime.now().isoformat()
                await f.write(f"{timestamp} | {tag_name} | {failure_reason} | {alpha_expression}\n")
        except:
            pass  # å½»åº•å¤±è´¥ä¹Ÿä¸æŠ›å¼‚å¸¸

