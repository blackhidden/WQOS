#!/usr/bin/env python3
"""
æ•°æ®é›†å­—æ®µå¯¼å‡ºè„šæœ¬
ä½œè€…ï¼še.e.
æ—¥æœŸï¼š2025.08.24

åŠŸèƒ½ï¼š
- è·å–æŒ‡å®šæ•°æ®é›†çš„å­—æ®µä¿¡æ¯
- å¤„ç†å­—æ®µï¼ˆmatrixå’Œvectorç±»å‹ï¼‰
- å°†å­—æ®µä¿¡æ¯å¯¼å‡ºåˆ°JSONæ–‡ä»¶
- æ”¯æŒæ‰¹é‡å¯¼å‡ºå¤šä¸ªæ•°æ®é›†
"""

import os
import sys
import json
import time
import argparse
from datetime import datetime
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
src_dir = current_dir.parent / 'src'
sys.path.insert(0, str(src_dir))

try:
    from machine_lib_ee import init_session, login, get_datafields, process_datafields
    from config import ROOT_PATH, RECORDS_PATH
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)


class DatasetFieldsExporter:
    """æ•°æ®é›†å­—æ®µå¯¼å‡ºå™¨"""
    
    def __init__(self, records_dir: str = None):
        """åˆå§‹åŒ–å¯¼å‡ºå™¨"""
        self.records_dir = Path(records_dir) if records_dir else Path(RECORDS_PATH)
        self.records_dir.mkdir(parents=True, exist_ok=True)
        
        # ç¡®ä¿sessionå·²åˆå§‹åŒ–
        print("ğŸ” æ­£åœ¨åˆå§‹åŒ–session...")
        try:
            self.session = init_session()
            print("âœ… Sessionåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Sessionåˆå§‹åŒ–å¤±è´¥: {e}")
            sys.exit(1)
    
    def get_dataset_fields(self, dataset_id: str, region: str = 'USA', 
                          universe: str = 'TOP3000', delay: int = 1) -> dict:
        """è·å–æ•°æ®é›†çš„å­—æ®µä¿¡æ¯"""
        print(f"ğŸ” æ­£åœ¨è·å–æ•°æ®é›† {dataset_id} çš„å­—æ®µä¿¡æ¯...")
        print(f"   ğŸŒ åœ°åŒº: {region}")
        print(f"   ğŸ›ï¸ å®‡å®™: {universe}")
        print(f"   â° å»¶è¿Ÿ: {delay}")
        
        try:
            # è·å–åŸå§‹å­—æ®µä¿¡æ¯
            start_time = time.time()
            df = get_datafields(
                self.session, 
                dataset_id=dataset_id,
                region=region,
                universe=universe,
                delay=delay
            )
            fetch_time = time.time() - start_time
            
            if df.empty:
                print("âš ï¸  æœªè·å–åˆ°ä»»ä½•å­—æ®µä¿¡æ¯")
                return {
                    'dataset_id': dataset_id,
                    'region': region,
                    'universe': universe,
                    'delay': delay,
                    'fetch_time': fetch_time,
                    'total_fields': 0,
                    'raw_fields': [],
                    'processed_fields': {
                        'matrix': [],
                        'vector': []
                    },
                    'error': 'No fields returned from API'
                }
            
            # ç»Ÿè®¡å­—æ®µä¿¡æ¯
            total_fields = len(df)
            matrix_fields = df[df['type'] == 'MATRIX']['id'].tolist() if 'type' in df.columns else []
            vector_fields = df[df['type'] == 'VECTOR']['id'].tolist() if 'type' in df.columns else []
            other_fields = df[~df['type'].isin(['MATRIX', 'VECTOR'])]['id'].tolist() if 'type' in df.columns else []
            
            print(f"ğŸ“Š å­—æ®µç»Ÿè®¡:")
            print(f"   æ€»è®¡: {total_fields:,} ä¸ªå­—æ®µ")
            print(f"   Matrix: {len(matrix_fields):,} ä¸ª")
            print(f"   Vector: {len(vector_fields):,} ä¸ª")
            print(f"   å…¶ä»–: {len(other_fields):,} ä¸ª")
            print(f"   è·å–è€—æ—¶: {fetch_time:.2f}ç§’")
            
            # ç»Ÿè®¡å­—æ®µä¿¡æ¯
            print("ğŸ“Š å­—æ®µç»Ÿè®¡:")
            print(f"   æ€»è®¡: {total_fields:,} ä¸ªå­—æ®µ")
            print(f"   Matrix: {len(matrix_fields):,} ä¸ª")
            print(f"   Vector: {len(vector_fields):,} ä¸ª")
            print(f"   å…¶ä»–: {len(other_fields):,} ä¸ª")
            print(f"   è·å–è€—æ—¶: {fetch_time:.2f}ç§’")
            
            # å¤„ç†å­—æ®µï¼ˆä¸ºç®€åŒ–ç‰ˆæœ¬å‡†å¤‡ï¼‰
            print("ğŸ”§ æ­£åœ¨å¤„ç†å­—æ®µ...")
            start_time = time.time()
            
            # å¤„ç†matrixå­—æ®µ
            processed_matrix = process_datafields(df, "matrix")
            
            # å¤„ç†vectorå­—æ®µ
            processed_vector = process_datafields(df, "vector")
            
            process_time = time.time() - start_time
            print(f"   å¤„ç†è€—æ—¶: {process_time:.2f}ç§’")
            print(f"   å¤„ç†åMatrixå­—æ®µ: {len(processed_matrix):,} ä¸ª")
            print(f"   å¤„ç†åVectorå­—æ®µ: {len(processed_vector):,} ä¸ª")
            
            # æ„å»ºç»“æœ
            result = {
                'dataset_id': dataset_id,
                'region': region,
                'universe': universe,
                'delay': delay,
                'fetch_time': fetch_time,
                'total_fields': total_fields,
                'raw_fields': {
                    'all': df.to_dict('records'),
                    'matrix': matrix_fields,
                    'vector': vector_fields,
                    'other': other_fields
                },
                'metadata': {
                    'export_timestamp': datetime.now().isoformat(),
                    'export_script': 'export_dataset_fields.py',
                    'api_version': 'v1'
                }
            }
            
            # ä¸ºç®€åŒ–ç‰ˆæœ¬å‡†å¤‡æ•°æ®ï¼ˆåŒ…å«å¤„ç†åçš„å­—æ®µï¼‰
            result['_simplified_fields'] = []
            for field in df.to_dict('records'):
                if 'id' in field:
                    field_info = {
                        'id': field['id'],
                        'description': field.get('description', ''),
                        'type': field.get('type', ''),
                        'processed_fields': []
                    }
                    
                    # æ ¹æ®å­—æ®µç±»å‹æ·»åŠ å¤„ç†åçš„å­—æ®µ
                    if field.get('type') == 'MATRIX':
                        # ä¸ºmatrixå­—æ®µç”Ÿæˆå¤„ç†åçš„è¡¨è¾¾å¼
                        field_expr = f"winsorize(ts_backfill({field['id']}, 120), std=4)"
                        field_info['processed_fields'].append(field_expr)
                    elif field.get('type') == 'VECTOR':
                        # ä¸ºvectorå­—æ®µç”Ÿæˆå¤„ç†åçš„è¡¨è¾¾å¼
                        vec_ops = ["vec_avg", "vec_sum", "vec_ir", "vec_max", "vec_count", "vec_skewness", "vec_stddev", "vec_choose"]
                        for vec_op in vec_ops:
                            if vec_op == "vec_choose":
                                field_info['processed_fields'].extend([
                                    f"{vec_op}({field['id']}, nth=-1)",
                                    f"{vec_op}({field['id']}, nth=0)"
                                ])
                            else:
                                field_info['processed_fields'].append(f"{vec_op}({field['id']})")
                    
                    result['_simplified_fields'].append(field_info)
            
            return result
            
        except Exception as e:
            print(f"âŒ è·å–å­—æ®µä¿¡æ¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                'dataset_id': dataset_id,
                'region': region,
                'universe': universe,
                'delay': delay,
                'error': str(e),
                'metadata': {
                    'export_timestamp': datetime.now().isoformat(),
                    'export_script': 'export_dataset_fields.py',
                    'api_version': 'v1'
                }
            }
    
    def export_to_json(self, dataset_id: str, fields_data: dict, 
                      output_dir: str = None, simplified_only: bool = False) -> str:
        """å°†å­—æ®µä¿¡æ¯å¯¼å‡ºåˆ°JSONæ–‡ä»¶"""
        if output_dir is None:
            output_dir = self.records_dir
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if simplified_only:
            filename = f"{dataset_id}_fields_simplified_{timestamp}.json"
        else:
            filename = f"{dataset_id}_fields_{timestamp}.json"
        filepath = output_path / filename
        
        try:
            # å¦‚æœåªéœ€è¦ç®€åŒ–ç‰ˆæœ¬ï¼Œåˆ›å»ºç®€åŒ–æ•°æ®ç»“æ„
            if simplified_only:
                simplified_data = {
                    'dataset_id': fields_data['dataset_id'],
                    'region': fields_data['region'],
                    'universe': fields_data['universe'],
                    'delay': fields_data['delay'],
                    'total_fields': fields_data['total_fields'],
                    'fields': fields_data.get('_simplified_fields', []),
                    'metadata': fields_data['metadata']
                }
                export_data = simplified_data
            else:
                export_data = fields_data
            
            # å†™å…¥JSONæ–‡ä»¶
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            file_type = "ç®€åŒ–ç‰ˆ" if simplified_only else "å®Œæ•´ç‰ˆ"
            print(f"ğŸ’¾ {file_type}å­—æ®µä¿¡æ¯å·²å¯¼å‡ºåˆ°: {filepath}")
            
            # è®¡ç®—æ–‡ä»¶å¤§å°
            file_size = filepath.stat().st_size
            if file_size > 1024 * 1024:  # å¤§äº1MB
                size_str = f"{file_size / (1024 * 1024):.2f} MB"
            else:
                size_str = f"{file_size / 1024:.2f} KB"
            print(f"ğŸ“ æ–‡ä»¶å¤§å°: {size_str}")
            
            return str(filepath)
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºJSONæ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def export_dataset(self, dataset_id: str, region: str = 'USA', 
                      universe: str = 'TOP3000', delay: int = 1,
                      output_dir: str = None, export_simplified: bool = True) -> dict:
        """å¯¼å‡ºå•ä¸ªæ•°æ®é›†çš„å­—æ®µä¿¡æ¯"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹å¯¼å‡ºæ•°æ®é›†: {dataset_id}")
        print(f"{'='*60}")
        
        # è·å–å­—æ®µä¿¡æ¯
        fields_data = self.get_dataset_fields(dataset_id, region, universe, delay)
        
        # å¯¼å‡ºåˆ°JSON
        if 'error' not in fields_data:
            results = {}
            
            # å¯¼å‡ºå®Œæ•´ç‰ˆæœ¬
            full_output_file = self.export_to_json(dataset_id, fields_data, output_dir, simplified_only=False)
            if full_output_file:
                results['full'] = full_output_file
                print(f"âœ… å®Œæ•´ç‰ˆå¯¼å‡ºå®Œæˆ")
            else:
                print(f"âŒ å®Œæ•´ç‰ˆå¯¼å‡ºå¤±è´¥")
            
            # å¯¼å‡ºç®€åŒ–ç‰ˆæœ¬
            if export_simplified:
                simplified_output_file = self.export_to_json(dataset_id, fields_data, output_dir, simplified_only=True)
                if simplified_output_file:
                    results['simplified'] = simplified_output_file
                    print(f"âœ… ç®€åŒ–ç‰ˆå¯¼å‡ºå®Œæˆ")
                else:
                    print(f"âŒ ç®€åŒ–ç‰ˆå¯¼å‡ºå¤±è´¥")
            
            if results:
                print(f"âœ… æ•°æ®é›† {dataset_id} å¯¼å‡ºå®Œæˆ")
                return results
            else:
                print(f"âŒ æ•°æ®é›† {dataset_id} å¯¼å‡ºå¤±è´¥")
                return {}
        else:
            print(f"âŒ æ•°æ®é›† {dataset_id} è·å–å­—æ®µå¤±è´¥: {fields_data['error']}")
            return {}
    
    def batch_export(self, dataset_ids: list, region: str = 'USA', 
                    universe: str = 'TOP3000', delay: int = 1,
                    output_dir: str = None) -> dict:
        """æ‰¹é‡å¯¼å‡ºå¤šä¸ªæ•°æ®é›†çš„å­—æ®µä¿¡æ¯"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¯¼å‡º {len(dataset_ids)} ä¸ªæ•°æ®é›†")
        print(f"{'='*60}")
        
        results = {}
        success_count = 0
        failed_count = 0
        
        for i, dataset_id in enumerate(dataset_ids, 1):
            print(f"\nğŸ“Š è¿›åº¦: {i}/{len(dataset_ids)}")
            
            try:
                export_results = self.export_dataset(
                    dataset_id, region, universe, delay, output_dir
                )
                
                if export_results:
                    results[dataset_id] = {
                        'status': 'success',
                        'output_files': export_results
                    }
                    success_count += 1
                else:
                    results[dataset_id] = {
                        'status': 'failed',
                        'error': 'Export failed'
                    }
                    failed_count += 1
                    
            except Exception as e:
                print(f"âŒ å¯¼å‡ºæ•°æ®é›† {dataset_id} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                results[dataset_id] = {
                    'status': 'error',
                    'error': str(e)
                }
                failed_count += 1
            
            # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
            if i < len(dataset_ids):
                print("â³ ç­‰å¾…2ç§’åç»§ç»­ä¸‹ä¸€ä¸ªæ•°æ®é›†...")
                time.sleep(2)
        
        # æ‰“å°æ€»ç»“
        print(f"\n{'='*60}")
        print(f"ğŸ“Š æ‰¹é‡å¯¼å‡ºå®Œæˆ")
        print(f"   âœ… æˆåŠŸ: {success_count} ä¸ª")
        print(f"   âŒ å¤±è´¥: {failed_count} ä¸ª")
        print(f"   ğŸ“ è¾“å‡ºç›®å½•: {output_dir or self.records_dir}")
        print(f"{'='*60}")
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='å¯¼å‡ºWorldQuantæ•°æ®é›†çš„å­—æ®µä¿¡æ¯åˆ°JSONæ–‡ä»¶',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¯¼å‡ºå•ä¸ªæ•°æ®é›†ï¼ˆé»˜è®¤å¯¼å‡ºå®Œæ•´ç‰ˆå’Œç®€åŒ–ç‰ˆï¼‰
  python export_dataset_fields.py fundamental6
  
  # å¯¼å‡ºå¤šä¸ªæ•°æ®é›†
  python export_dataset_fields.py fundamental6 fundamental7 fundamental8
  
  # æŒ‡å®šåœ°åŒºå’Œå®‡å®™
  python export_dataset_fields.py fundamental6 --region USA --universe TOP3000
  
  # æŒ‡å®šè¾“å‡ºç›®å½•
  python export_dataset_fields.py fundamental6 --output-dir ./custom_output
  
  # åªå¯¼å‡ºå®Œæ•´ç‰ˆæœ¬ï¼ˆä¸å¯¼å‡ºç®€åŒ–ç‰ˆï¼‰
  python export_dataset_fields.py fundamental6 --no-simplified
  
  # æ‰¹é‡å¯¼å‡ºé…ç½®æ–‡ä»¶ä¸­çš„æ•°æ®é›†
  python export_dataset_fields.py --config-file ./config/dataset.json
        """
    )
    
    parser.add_argument('dataset_ids', nargs='*', 
                       help='è¦å¯¼å‡ºçš„æ•°æ®é›†IDåˆ—è¡¨')
    
    parser.add_argument('--region', default='USA',
                       help='åœ°åŒº (é»˜è®¤: USA)')
    
    parser.add_argument('--universe', default='TOP3000',
                       help='å®‡å®™ (é»˜è®¤: TOP3000)')
    
    parser.add_argument('--delay', type=int, default=1,
                       help='å»¶è¿Ÿ (é»˜è®¤: 1)')
    
    parser.add_argument('--output-dir', 
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: recordsç›®å½•)')
    
    parser.add_argument('--config-file',
                       help='åŒ…å«æ•°æ®é›†åˆ—è¡¨çš„é…ç½®æ–‡ä»¶è·¯å¾„')
    
    parser.add_argument('--batch', action='store_true',
                       help='å¯ç”¨æ‰¹é‡å¯¼å‡ºæ¨¡å¼')
    
    parser.add_argument('--no-simplified', action='store_true',
                       help='ä¸å¯¼å‡ºç®€åŒ–ç‰ˆæœ¬ï¼ˆåªå¯¼å‡ºå®Œæ•´ç‰ˆæœ¬ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥å‚æ•°
    if not args.dataset_ids and not args.config_file:
        parser.error("å¿…é¡»æŒ‡å®šæ•°æ®é›†IDæˆ–é…ç½®æ–‡ä»¶")
    
    # åˆå§‹åŒ–å¯¼å‡ºå™¨
    try:
        exporter = DatasetFieldsExporter(args.output_dir)
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¯¼å‡ºå™¨å¤±è´¥: {e}")
        sys.exit(1)
    
    # è·å–æ•°æ®é›†åˆ—è¡¨
    dataset_ids = []
    
    if args.config_file:
        # ä»é…ç½®æ–‡ä»¶è¯»å–æ•°æ®é›†åˆ—è¡¨
        try:
            with open(args.config_file, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            
            # å°è¯•ä»ä¸åŒè·¯å¾„è·å–æ•°æ®é›†ID
            if 'results' in config_data:
                dataset_ids = [item['id'] for item in config_data['results']]
            elif 'datasets' in config_data:
                dataset_ids = config_data['datasets']
            else:
                print(f"âš ï¸  é…ç½®æ–‡ä»¶ {args.config_file} ä¸­æœªæ‰¾åˆ°æ•°æ®é›†åˆ—è¡¨")
                print("è¯·æ£€æŸ¥é…ç½®æ–‡ä»¶æ ¼å¼")
                sys.exit(1)
                
            print(f"ğŸ“‹ ä»é…ç½®æ–‡ä»¶è¯»å–åˆ° {len(dataset_ids)} ä¸ªæ•°æ®é›†")
            
        except Exception as e:
            print(f"âŒ è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            sys.exit(1)
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°ä¸­çš„æ•°æ®é›†ID
    dataset_ids.extend(args.dataset_ids)
    
    # å»é‡
    dataset_ids = list(dict.fromkeys(dataset_ids))
    
    if not dataset_ids:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°è¦å¯¼å‡ºçš„æ•°æ®é›†")
        sys.exit(1)
    
    print(f"ğŸ¯ å‡†å¤‡å¯¼å‡º {len(dataset_ids)} ä¸ªæ•°æ®é›†:")
    for i, dataset_id in enumerate(dataset_ids, 1):
        print(f"   {i:2d}. {dataset_id}")
    
    # æ‰§è¡Œå¯¼å‡º
    export_simplified = not args.no_simplified
    
    if len(dataset_ids) == 1:
        # å•ä¸ªæ•°æ®é›†
        exporter.export_dataset(
            dataset_ids[0], 
            args.region, 
            args.universe, 
            args.delay,
            args.output_dir,
            export_simplified
        )
    else:
        # å¤šä¸ªæ•°æ®é›†
        exporter.batch_export(
            dataset_ids,
            args.region,
            args.universe,
            args.delay,
            args.output_dir
        )
    
    print("\nğŸ‰ å¯¼å‡ºå®Œæˆï¼")


if __name__ == '__main__':
    main()
