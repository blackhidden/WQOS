#!/usr/bin/env python3
"""
æ¸…ç†å­¤å„¿æ—¥å¿—æ–‡ä»¶è„šæœ¬

åŠŸèƒ½ï¼š
1. æ‰«ælogsç›®å½•ä¸‹çš„æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
2. æ£€æŸ¥å“ªäº›æ—¥å¿—æ–‡ä»¶åœ¨æ•°æ®åº“ä¸­æ²¡æœ‰å¯¹åº”è®°å½•
3. æ¸…ç†è¿™äº›å­¤å„¿æ—¥å¿—æ–‡ä»¶
4. æ˜¾ç¤ºæ¸…ç†æŠ¥å‘Š
"""

import os
import sys
import re
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Set

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
backend_path = project_root / "digging-dashboard" / "backend"
sys.path.insert(0, str(backend_path))

# å¯¼å…¥æ•°æ®åº“å·¥å…·
from sqlalchemy import create_engine, text

def get_database_engine():
    """èŽ·å–æ•°æ®åº“å¼•æ“Ž"""
    try:
        # ä½¿ç”¨Dashboardçš„æ•°æ®åº“é…ç½®
        dashboard_db_path = project_root / "digging-dashboard" / "backend" / "dashboard.db"
        if not dashboard_db_path.exists():
            # å¦‚æžœdashboard.dbä¸å­˜åœ¨ï¼Œå°è¯•å½“å‰ç›®å½•
            dashboard_db_path = project_root / "digging-dashboard" / "dashboard.db"
        
        if not dashboard_db_path.exists():
            print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {dashboard_db_path}")
            print("ðŸ’¡ è¯·ç¡®ä¿Dashboardå·²ç»åˆå§‹åŒ–å¹¶åˆ›å»ºäº†æ•°æ®åº“")
            return None
        
        engine = create_engine(f"sqlite:///{dashboard_db_path}")
        return engine
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿žæŽ¥å¤±è´¥: {e}")
        return None

def scan_log_directory() -> Dict[str, List[str]]:
    """æ‰«ææ—¥å¿—ç›®å½•ï¼Œè¿”å›žæ‰€æœ‰æ—¥å¿—æ–‡ä»¶"""
    logs_dir = project_root / "logs"
    if not logs_dir.exists():
        print(f"âŒ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {logs_dir}")
        return {}
    
    log_files = {}
    total_files = 0
    total_size = 0
    
    print(f"ðŸ“‚ æ‰«ææ—¥å¿—ç›®å½•: {logs_dir}")
    
    for file_path in logs_dir.glob("*.log"):
        if file_path.is_file():
            # è§£æžæ–‡ä»¶åæ ¼å¼: script_type_YYYYMMDD_HHMMSS_pid.log
            file_name = file_path.name
            size = file_path.stat().st_size
            total_size += size
            total_files += 1
            
            # æå–è„šæœ¬ç±»åž‹
            parts = file_name.split('_')
            if len(parts) >= 4:
                if parts[0] == "check" and parts[1] == "optimized":
                    script_type = "check_optimized"
                    date_part = parts[2]
                    time_part = parts[3]
                    pid_part = parts[4].replace('.log', '')
                elif parts[0] == "correlation" and parts[1] == "checker":
                    script_type = "correlation_checker"
                    date_part = parts[2]
                    time_part = parts[3]
                    pid_part = parts[4].replace('.log', '')
                elif parts[0] == "unified" and parts[1] == "digging":
                    script_type = "unified_digging"
                    date_part = parts[2]
                    time_part = parts[3]
                    pid_part = parts[4].replace('.log', '')
                else:
                    script_type = "unknown"
                    date_part = ""
                    time_part = ""
                    pid_part = ""
            else:
                script_type = "unknown"
                date_part = ""
                time_part = ""
                pid_part = ""
            
            if script_type not in log_files:
                log_files[script_type] = []
            
            log_files[script_type].append({
                'path': str(file_path),
                'name': file_name,
                'size': size,
                'date': date_part,
                'time': time_part,
                'pid': pid_part,
                'modified': datetime.fromtimestamp(file_path.stat().st_mtime)
            })
    
    print(f"ðŸ“Š å‘çŽ° {total_files} ä¸ªæ—¥å¿—æ–‡ä»¶ï¼Œæ€»å¤§å°: {total_size / 1024 / 1024:.2f} MB")
    
    # æŒ‰è„šæœ¬ç±»åž‹æ˜¾ç¤ºç»Ÿè®¡
    for script_type, files in log_files.items():
        type_size = sum(f['size'] for f in files)
        print(f"  ðŸ“„ {script_type}: {len(files)} ä¸ªæ–‡ä»¶ï¼Œ{type_size / 1024 / 1024:.2f} MB")
    
    return log_files

def get_database_log_paths(engine) -> Set[str]:
    """èŽ·å–æ•°æ®åº“ä¸­è®°å½•çš„æ‰€æœ‰æ—¥å¿—æ–‡ä»¶è·¯å¾„"""
    try:
        # ä½¿ç”¨åŽŸç”ŸSQLæŸ¥è¯¢ï¼Œé¿å…ORMæ¨¡åž‹å¯¼å…¥é—®é¢˜
        with engine.connect() as conn:
            result = conn.execute(text("""
                SELECT log_file_path 
                FROM digging_processes 
                WHERE log_file_path IS NOT NULL 
                AND log_file_path != ''
            """))
            
            log_paths = set()
            for row in result:
                log_file_path = row[0]
                if log_file_path:
                    # èŽ·å–ç»å¯¹è·¯å¾„
                    log_path = Path(log_file_path)
                    if not log_path.is_absolute():
                        log_path = project_root / log_file_path
                    log_paths.add(str(log_path.resolve()))
            
            print(f"ðŸ“‹ æ•°æ®åº“ä¸­æœ‰ {len(log_paths)} ä¸ªæ—¥å¿—æ–‡ä»¶è®°å½•")
            return log_paths
        
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢æ•°æ®åº“å¤±è´¥: {e}")
        return set()

def identify_orphan_logs(log_files: Dict[str, List[str]], db_log_paths: Set[str]) -> List[Dict]:
    """è¯†åˆ«å­¤å„¿æ—¥å¿—æ–‡ä»¶"""
    orphan_logs = []
    
    for script_type, files in log_files.items():
        for file_info in files:
            file_path = Path(file_info['path']).resolve()
            
            # æ£€æŸ¥æ˜¯å¦åœ¨æ•°æ®åº“ä¸­æœ‰è®°å½•
            if str(file_path) not in db_log_paths:
                orphan_logs.append({
                    'script_type': script_type,
                    'path': str(file_path),
                    'name': file_info['name'],
                    'size': file_info['size'],
                    'modified': file_info['modified']
                })
    
    return orphan_logs

def cleanup_orphan_logs(orphan_logs: List[Dict], dry_run: bool = True) -> Dict:
    """æ¸…ç†å­¤å„¿æ—¥å¿—æ–‡ä»¶"""
    cleaned_count = 0
    cleaned_size = 0
    errors = []
    
    print(f"\n{'ðŸ” é¢„è§ˆæ¸…ç†æ“ä½œ' if dry_run else 'ðŸ—‘ï¸ å¼€å§‹æ¸…ç†å­¤å„¿æ—¥å¿—æ–‡ä»¶'}...")
    
    for log_info in orphan_logs:
        file_path = Path(log_info['path'])
        size_mb = log_info['size'] / 1024 / 1024
        
        if dry_run:
            print(f"  {'ðŸ“„' if log_info['script_type'] != 'unknown' else 'â“'} "
                  f"{log_info['name']} ({size_mb:.2f} MB) - "
                  f"ä¿®æ”¹æ—¶é—´: {log_info['modified'].strftime('%Y-%m-%d %H:%M:%S')}")
        else:
            try:
                if file_path.exists():
                    file_path.unlink()
                    cleaned_count += 1
                    cleaned_size += log_info['size']
                    print(f"  âœ… å·²åˆ é™¤: {log_info['name']} ({size_mb:.2f} MB)")
                else:
                    print(f"  âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {log_info['name']}")
            except Exception as e:
                error_msg = f"åˆ é™¤å¤±è´¥: {log_info['name']} - {e}"
                errors.append(error_msg)
                print(f"  âŒ {error_msg}")
    
    return {
        'cleaned_count': cleaned_count,
        'cleaned_size': cleaned_size,
        'errors': errors
    }

def generate_cleanup_report(orphan_logs: List[Dict], cleanup_result: Dict = None):
    """ç”Ÿæˆæ¸…ç†æŠ¥å‘Š"""
    print(f"\nðŸ“Š æ¸…ç†æŠ¥å‘Š")
    print("=" * 50)
    
    # æŒ‰è„šæœ¬ç±»åž‹ç»Ÿè®¡å­¤å„¿æ—¥å¿—
    type_stats = {}
    total_orphan_size = 0
    
    for log_info in orphan_logs:
        script_type = log_info['script_type']
        size = log_info['size']
        
        if script_type not in type_stats:
            type_stats[script_type] = {'count': 0, 'size': 0}
        
        type_stats[script_type]['count'] += 1
        type_stats[script_type]['size'] += size
        total_orphan_size += size
    
    print(f"ðŸ” å‘çŽ° {len(orphan_logs)} ä¸ªå­¤å„¿æ—¥å¿—æ–‡ä»¶ï¼Œæ€»å¤§å°: {total_orphan_size / 1024 / 1024:.2f} MB")
    
    for script_type, stats in type_stats.items():
        print(f"  ðŸ“„ {script_type}: {stats['count']} ä¸ªæ–‡ä»¶ï¼Œ{stats['size'] / 1024 / 1024:.2f} MB")
    
    if cleanup_result:
        print(f"\nâœ… æ¸…ç†å®Œæˆ:")
        print(f"  ðŸ—‘ï¸ å·²åˆ é™¤: {cleanup_result['cleaned_count']} ä¸ªæ–‡ä»¶")
        print(f"  ðŸ’¾ é‡Šæ”¾ç©ºé—´: {cleanup_result['cleaned_size'] / 1024 / 1024:.2f} MB")
        
        if cleanup_result['errors']:
            print(f"  âŒ é”™è¯¯: {len(cleanup_result['errors'])} ä¸ª")
            for error in cleanup_result['errors']:
                print(f"    - {error}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ¸…ç†å­¤å„¿æ—¥å¿—æ–‡ä»¶")
    parser.add_argument('--dry-run', action='store_true', default=True,
                       help='é¢„è§ˆæ¨¡å¼ï¼Œä¸å®žé™…åˆ é™¤æ–‡ä»¶ (é»˜è®¤)')
    parser.add_argument('--execute', action='store_true',
                       help='æ‰§è¡Œå®žé™…åˆ ç†æ“ä½œ')
    parser.add_argument('--days', type=int, default=0,
                       help='åªæ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æ–‡ä»¶ (0=å…¨éƒ¨)')
    
    args = parser.parse_args()
    
    # å¦‚æžœæŒ‡å®šäº† --executeï¼Œåˆ™å…³é—­é¢„è§ˆæ¨¡å¼
    dry_run = not args.execute
    
    print("ðŸ§¹ å­¤å„¿æ—¥å¿—æ–‡ä»¶æ¸…ç†å·¥å…·")
    print("=" * 50)
    
    # 1. èŽ·å–æ•°æ®åº“å¼•æ“Ž
    engine = get_database_engine()
    if not engine:
        return 1
    
    try:
        # 2. æ‰«ææ—¥å¿—ç›®å½•
        log_files = scan_log_directory()
        if not log_files:
            print("âœ… æ²¡æœ‰å‘çŽ°æ—¥å¿—æ–‡ä»¶")
            return 0
        
        # 3. èŽ·å–æ•°æ®åº“ä¸­çš„æ—¥å¿—è·¯å¾„
        db_log_paths = get_database_log_paths(engine)
        
        # 4. è¯†åˆ«å­¤å„¿æ—¥å¿—æ–‡ä»¶
        orphan_logs = identify_orphan_logs(log_files, db_log_paths)
        
        if not orphan_logs:
            print("âœ… æ²¡æœ‰å‘çŽ°å­¤å„¿æ—¥å¿—æ–‡ä»¶")
            return 0
        
        # 5. æŒ‰æ—¥æœŸè¿‡æ»¤
        if args.days > 0:
            from datetime import timedelta
            cutoff_date = datetime.now() - timedelta(days=args.days)
            orphan_logs = [log for log in orphan_logs if log['modified'] < cutoff_date]
            print(f"ðŸ—“ï¸ è¿‡æ»¤æ¡ä»¶: è¶…è¿‡ {args.days} å¤©çš„æ–‡ä»¶")
        
        if not orphan_logs:
            print(f"âœ… æ²¡æœ‰å‘çŽ°ç¬¦åˆæ¡ä»¶çš„å­¤å„¿æ—¥å¿—æ–‡ä»¶")
            return 0
        
        # 6. æ¸…ç†å­¤å„¿æ—¥å¿—
        cleanup_result = cleanup_orphan_logs(orphan_logs, dry_run)
        
        # 7. ç”ŸæˆæŠ¥å‘Š
        generate_cleanup_report(orphan_logs, cleanup_result if not dry_run else None)
        
        if dry_run:
            print(f"\nðŸ’¡ é¢„è§ˆå®Œæˆã€‚ä½¿ç”¨ --execute å‚æ•°æ‰§è¡Œå®žé™…æ¸…ç†æ“ä½œ")
        
        return 0
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
        
    finally:
        if engine:
            engine.dispose()

if __name__ == "__main__":
    exit(main())
