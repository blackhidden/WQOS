"""
æŒ–æ˜è¿›ç¨‹ç®¡ç†æœåŠ¡
"""

import os
import sys
import signal
import subprocess
import psutil
import json
import time
import platform
from typing import Optional, Dict, List, Any
from datetime import datetime
from sqlalchemy.orm import Session

from app.config import get_settings
from app.db.database import get_db
from app.db.models import DiggingProcess, AuditLog
from app.core.exceptions import ProcessError, ValidationError
from app.models.config import DiggingConfig
from app.utils.tag_generator import TagGenerator
from app.utils.path_utils import detect_project_root, get_script_path, get_config_path

settings = get_settings()


class ProcessService:
    """æŒ–æ˜è¿›ç¨‹ç®¡ç†æœåŠ¡"""
    
    def __init__(self):
        # å¹³å°æ£€æµ‹
        self.is_windows = platform.system().lower() == 'windows'
        
        # è‡ªåŠ¨æ£€æµ‹é¡¹ç›®æ ¹ç›®å½•
        self.project_root = detect_project_root()
        self.script_path = get_script_path("unified_digging_scheduler")
        
        # è„šæœ¬è·¯å¾„æ˜ å°„
        self.script_paths = {
            "unified_digging": get_script_path("unified_digging_scheduler"),
            "check_optimized": get_script_path("check_optimized"),
            "correlation_checker": get_script_path("correlation_checker_independent"),
            "session_keeper": get_script_path("session_keeper")
        }
        
        # è„šæœ¬æ˜¾ç¤ºåç§°
        self.script_names = {
            "unified_digging": "å› å­æŒ–æ˜",
            "check_optimized": "Alphaæ£€æŸ¥å™¨", 
            "correlation_checker": "ç›¸å…³æ€§æ£€æŸ¥å™¨",
            "session_keeper": "ä¼šè¯ä¿æŒå™¨"
        }
        
        # éœ€è¦é…ç½®æ¨¡æ¿çš„è„šæœ¬
        self.scripts_need_config = {"unified_digging"}
        
        # ç‹¬ç«‹è„šæœ¬ï¼ˆä¸éœ€è¦é…ç½®æ¨¡æ¿ï¼‰
        self.independent_scripts = {"check_optimized", "correlation_checker", "session_keeper"}
        self.config_path = get_config_path("digging_config.txt")
        self.process_info: Optional[Dict[str, Any]] = None
    
    def _get_process_kwargs(self) -> Dict[str, Any]:
        """è·å–è·¨å¹³å°çš„è¿›ç¨‹åˆ›å»ºå‚æ•°"""
        kwargs = {}
        
        if not self.is_windows:
            # Unix/Linux ç³»ç»Ÿä½¿ç”¨ setsid åˆ›å»ºæ–°è¿›ç¨‹ç»„
            kwargs['preexec_fn'] = os.setsid
        else:
            # Windows ç³»ç»Ÿä½¿ç”¨ CREATE_NEW_PROCESS_GROUP
            kwargs['creationflags'] = subprocess.CREATE_NEW_PROCESS_GROUP
            
        return kwargs
    
    def _terminate_process_group(self, pid: int, force: bool = False) -> str:
        """è·¨å¹³å°ç»ˆæ­¢è¿›ç¨‹ç»„"""
        try:
            if not self.is_windows:
                # Unix/Linux ç³»ç»Ÿä½¿ç”¨ killpg
                if force:
                    os.killpg(os.getpgid(pid), signal.SIGKILL)
                    return "SIGKILL"
                else:
                    os.killpg(os.getpgid(pid), signal.SIGTERM)
                    return "SIGTERM"
            else:
                # Windows ç³»ç»Ÿä½¿ç”¨ psutil
                try:
                    process = psutil.Process(pid)
                    # ç»ˆæ­¢è¿›ç¨‹åŠå…¶æ‰€æœ‰å­è¿›ç¨‹
                    for child in process.children(recursive=True):
                        try:
                            if force:
                                child.kill()
                            else:
                                child.terminate()
                        except psutil.NoSuchProcess:
                            pass
                    
                    # ç»ˆæ­¢ä¸»è¿›ç¨‹
                    if force:
                        process.kill()
                        return "KILL"
                    else:
                        process.terminate()
                        return "TERMINATE"
                        
                except psutil.NoSuchProcess:
                    return "NOT_FOUND"
                    
        except Exception as e:
            raise ProcessError(f"ç»ˆæ­¢è¿›ç¨‹å¤±è´¥: {e}")
        
    def get_current_process_status(self, db: Session) -> Dict[str, Any]:
        """è·å–å½“å‰è¿›ç¨‹çŠ¶æ€"""
        try:
            # ä»æ•°æ®åº“è·å–æœ€æ–°çš„è¿›ç¨‹è®°å½•
            latest_process = db.query(DiggingProcess).order_by(
                DiggingProcess.started_at.desc()
            ).first()
            
            if not latest_process:
                return {
                    "status": "stopped",
                    "pid": None,
                    "config_id": None,
                    "start_time": None,
                    "uptime": None,
                    "memory_usage": None,
                    "cpu_usage": None
                }
            
            # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦ä»åœ¨è¿è¡Œ
            is_running = False
            process_info = None
            
            if latest_process.process_id:
                try:
                    process = psutil.Process(latest_process.process_id)
                    if process.is_running() and process.status() != psutil.STATUS_ZOMBIE:
                        is_running = True
                        # ä½¿ç”¨è¿›ç¨‹çš„å®é™…åˆ›å»ºæ—¶é—´
                        try:
                            create_time = process.create_time()
                            uptime = int(time.time() - create_time)
                        except (psutil.AccessDenied, OSError):
                            # å¦‚æœæ— æ³•è·å–åˆ›å»ºæ—¶é—´ï¼Œä½¿ç”¨æ•°æ®åº“æ—¶é—´
                            start_time = latest_process.started_at
                            uptime = int((datetime.now() - start_time).total_seconds()) if start_time else 0
                        
                        process_info = {
                            "memory_usage": process.memory_info().rss / 1024 / 1024,  # MB
                            "cpu_usage": process.cpu_percent(),
                            "uptime": max(0, uptime)
                        }
                except psutil.NoSuchProcess:
                    is_running = False
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            if not is_running and latest_process.status == "running":
                latest_process.status = "stopped"
                latest_process.stopped_at = datetime.now()
                db.commit()
            
            status = "running" if is_running else latest_process.status
            start_time = latest_process.started_at
            
            return {
                "status": status,
                "pid": latest_process.process_id if is_running else None,
                "config_id": latest_process.config_template_id,
                "start_time": start_time.isoformat() if start_time else None,
                "uptime": process_info["uptime"] if process_info and "uptime" in process_info else None,
                "memory_usage": process_info["memory_usage"] if process_info else None,
                "cpu_usage": process_info["cpu_usage"] if process_info else None,
                "tag": latest_process.tag_name,
                "error_message": latest_process.error_message
            }
            
        except Exception as e:
            raise ProcessError(f"è·å–è¿›ç¨‹çŠ¶æ€å¤±è´¥: {str(e)}")

    def get_all_processes_status(self, db: Session) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰è¿›ç¨‹çŠ¶æ€ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # ä»æ•°æ®åº“è·å–æ‰€æœ‰è¿è¡Œä¸­çš„è¿›ç¨‹
            running_processes = db.query(DiggingProcess).filter(
                DiggingProcess.status == "running"
            ).all()
            
            active_processes = []
            total_memory = 0
            max_uptime = 0
            
            for db_process in running_processes:
                if db_process.process_id:
                    try:
                        process = psutil.Process(db_process.process_id)
                        if process.is_running() and process.status() != psutil.STATUS_ZOMBIE:
                            memory_mb = process.memory_info().rss / 1024 / 1024
                            
                            # ä½¿ç”¨è¿›ç¨‹çš„å®é™…åˆ›å»ºæ—¶é—´è®¡ç®—uptime
                            try:
                                process_create_time = process.create_time()
                                uptime = int(time.time() - process_create_time)
                                actual_start_time = datetime.fromtimestamp(process_create_time)
                            except (psutil.AccessDenied, OSError):
                                # å¦‚æœæ— æ³•è·å–è¿›ç¨‹åˆ›å»ºæ—¶é—´ï¼Œä½¿ç”¨æ•°æ®åº“æ—¶é—´
                                start_time = db_process.started_at
                                uptime = int((datetime.now() - start_time).total_seconds()) if start_time else 0
                                actual_start_time = start_time
                            
                            active_processes.append({
                                "pid": db_process.process_id,
                                "tag": db_process.tag_name,
                                "script_type": getattr(db_process, 'script_type', 'unknown'),
                                "start_time": actual_start_time.isoformat() if actual_start_time else None,
                                "uptime": max(0, uptime),  # ç¡®ä¿uptimeä¸ä¸ºè´Ÿæ•°
                                "memory_usage": memory_mb,
                                "cpu_usage": process.cpu_percent()
                            })
                            
                            total_memory += memory_mb
                            max_uptime = max(max_uptime, max(0, uptime))
                        else:
                            # è¿›ç¨‹å·²åœæ­¢ï¼Œæ›´æ–°æ•°æ®åº“çŠ¶æ€
                            db_process.status = "stopped"
                            db_process.stopped_at = datetime.now()
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        # è¿›ç¨‹ä¸å­˜åœ¨ï¼Œæ›´æ–°æ•°æ®åº“çŠ¶æ€
                        db_process.status = "stopped"
                        db_process.stopped_at = datetime.now()
            
            # æäº¤æ•°æ®åº“æ›´æ”¹
            db.commit()
            
            return {
                "total_processes": len(active_processes),
                "status": "running" if active_processes else "stopped",
                "total_memory_usage": total_memory,
                "max_uptime": max_uptime,
                "processes": active_processes
            }
            
        except Exception as e:
            print(f"è·å–æ‰€æœ‰è¿›ç¨‹çŠ¶æ€æ—¶å‡ºé”™: {e}")
            return {
                "total_processes": 0,
                "status": "error",
                "total_memory_usage": 0,
                "max_uptime": 0,
                "processes": [],
                "error_message": str(e)
            }
    
    def start_process(self, config: DiggingConfig, user_id: int, db: Session, stage: int = 1, n_jobs: int = 5, enable_multi_simulation: bool = False) -> Dict[str, Any]:
        """å¯åŠ¨æŒ–æ˜è¿›ç¨‹"""
        try:
            # ç§»é™¤å•å®ä¾‹é™åˆ¶ï¼Œå…è®¸å¤šä¸ªunified_diggingå®ä¾‹è¿è¡Œ
            # ï¼ˆæ¯ä¸ªå®ä¾‹å¯ä»¥å¤„ç†ä¸åŒçš„é˜¶æ®µæˆ–é…ç½®ï¼‰
            
            # ç”Ÿæˆé…ç½®æ–‡ä»¶
            config_file_path = self._create_config_file(config)
            
            # ç”Ÿæˆtag (åŸºäºé˜¶æ®µ)
            tag = TagGenerator.generate_tag(
                region=config.region,
                delay=config.delay,
                instrument_type=config.instrument_type,
                universe=config.universe,
                dataset_id=config.dataset_id if not config.use_recommended_fields else None,
                recommended_name=config.recommended_name if config.use_recommended_fields else None,
                step=f"step{stage}"
            )
            
            # æ„å»ºå¯åŠ¨å‘½ä»¤ï¼ŒåŒ…å«æ–°çš„å‚æ•°
            cmd = [
                sys.executable,
                self.script_path,
                "--config", config_file_path,
                "--stage", str(stage),
                "--n_jobs", str(n_jobs)
            ]
            
            # å¦‚æœå¯ç”¨å¤šæ¨¡æ‹Ÿï¼Œæ·»åŠ å‚æ•°
            if enable_multi_simulation:
                cmd.extend(["--enable_multi_simulation", "true"])
            
            # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
            log_dir = os.path.join(self.project_root, "logs")
            os.makedirs(log_dir, exist_ok=True)
            
            # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºå”¯ä¸€çš„æ—¥å¿—æ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file_name = f"unified_digging_{timestamp}_{os.getpid()}.log"
            log_file = os.path.join(log_dir, log_file_name)
            
            # æ£€æŸ¥å¹¶å¤„ç†æ—¥å¿—è½®è½¬ï¼ˆçˆ¶è¿›ç¨‹çº§åˆ«çš„è½®è½¬ç®¡ç†ï¼‰
            self._ensure_log_rotation(log_file)
            
            # å¯åŠ¨è¿›ç¨‹ï¼Œé‡å®šå‘è¾“å‡ºåˆ°ç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶
            with open(log_file, 'w', encoding='utf-8') as f:
                # å†™å…¥å¯åŠ¨æ—¶é—´
                f.write(f"\n\n=== è¿›ç¨‹å¯åŠ¨ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
                f.write(f"å‘½ä»¤: {' '.join(cmd)}\n")
                f.write(f"é…ç½®: {tag}\n")
                f.write("=" * 50 + "\n\n")
                f.flush()
                
                # è·å–è·¨å¹³å°è¿›ç¨‹åˆ›å»ºå‚æ•°
                process_kwargs = self._get_process_kwargs()
                
                process = subprocess.Popen(
                    cmd,
                    cwd=self.project_root,
                    stdout=f,
                    stderr=subprocess.STDOUT,  # å°†stderré‡å®šå‘åˆ°stdout
                    **process_kwargs  # è·¨å¹³å°è¿›ç¨‹ç»„åˆ›å»º
                )
            
            # è®°å½•åˆ°æ•°æ®åº“
            db_process = DiggingProcess(
                config_template_id=config.template_id,
                tag_name=tag,
                process_id=process.pid,
                status="running",
                script_type="unified_digging",
                started_at=datetime.now(),
                log_file_path=log_file
            )
            db.add(db_process)
            
            # è®°å½•å®¡è®¡æ—¥å¿—
            audit_log = AuditLog(
                user_id=user_id,
                action="START_PROCESS",
                resource_type="DIGGING_PROCESS",
                resource_id=str(process.pid),
                details={
                    "config_id": config.template_id,
                    "tag": tag,
                    "command": " ".join(cmd)
                }
            )
            db.add(audit_log)
            db.commit()
            
            return {
                "status": "started",
                "pid": process.pid,
                "tag": tag,
                "config_id": config.template_id,
                "start_time": db_process.started_at.isoformat()
            }
            
        except Exception as e:
            db.rollback()
            raise ProcessError(f"å¯åŠ¨è¿›ç¨‹å¤±è´¥: {str(e)}")
    

    def start_independent_script(self, script_type: str, user_id: int, db: Session, script_params: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¯åŠ¨ç‹¬ç«‹è„šæœ¬ï¼ˆæ— éœ€é…ç½®æ–‡ä»¶ï¼‰"""
        try:
            # éªŒè¯è„šæœ¬ç±»å‹
            if script_type not in self.script_paths:
                raise ValidationError(f"ä¸æ”¯æŒçš„è„šæœ¬ç±»å‹: {script_type}")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºéœ€è¦é…ç½®æ¨¡æ¿çš„è„šæœ¬
            if script_type in self.scripts_need_config:
                raise ValidationError(f"{self.script_names[script_type]}éœ€è¦é…ç½®æ¨¡æ¿ï¼Œè¯·ä½¿ç”¨è¿›ç¨‹æ§åˆ¶é¡µé¢å¯åŠ¨")
            
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒç±»å‹çš„è„šæœ¬åœ¨è¿è¡Œ
            existing_process = db.query(DiggingProcess).filter(
                DiggingProcess.script_type == script_type,
                DiggingProcess.status == "running"
            ).first()
            
            if existing_process:
                raise ProcessError(f"{self.script_names[script_type]}å·²åœ¨è¿è¡Œä¸­ï¼ˆPID: {existing_process.process_id}ï¼‰ï¼Œè¯·å…ˆåœæ­¢åå†å¯åŠ¨æ–°å®ä¾‹")
            
            script_path = self.script_paths[script_type]
            script_name = self.script_names[script_type]
            
            # ç”Ÿæˆç®€å•çš„tag
            tag = f"{script_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # æ„å»ºå¯åŠ¨å‘½ä»¤
            cmd = [sys.executable, script_path]
            
            # ä¸ºcheck_optimizedè„šæœ¬æ·»åŠ å‚æ•°æ”¯æŒ
            if script_type == "check_optimized" and script_params:
                if script_params.get("mode"):
                    cmd.extend(["--mode", script_params["mode"]])
                if script_params.get("sharpe_threshold") is not None:
                    cmd.extend(["--sharpe-threshold", str(script_params["sharpe_threshold"])])
                if script_params.get("fitness_threshold") is not None:
                    cmd.extend(["--fitness-threshold", str(script_params["fitness_threshold"])])
                if script_params.get("start_date"):
                    cmd.extend(["--start-date", script_params["start_date"]])
            
            # ä¸ºsession_keeperè„šæœ¬æ·»åŠ å¯åŠ¨å‚æ•°
            elif script_type == "session_keeper":
                cmd.extend(["--action", "start"])
            
            # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
            log_dir = os.path.join(self.project_root, "logs")
            os.makedirs(log_dir, exist_ok=True)
            
            # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºå”¯ä¸€çš„æ—¥å¿—æ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file_name = f"{script_type}_{timestamp}_{os.getpid()}.log"
            log_file_path = os.path.join(log_dir, log_file_name)
            
            # æ£€æŸ¥å¹¶å¤„ç†æ—¥å¿—è½®è½¬ï¼ˆçˆ¶è¿›ç¨‹çº§åˆ«çš„è½®è½¬ç®¡ç†ï¼‰
            self._ensure_log_rotation(log_file_path)
            
            # è·å–è·¨å¹³å°è¿›ç¨‹åˆ›å»ºå‚æ•°
            process_kwargs = self._get_process_kwargs()
            process_kwargs.update({
                'text': True,
                'bufsize': 1  # è¡Œç¼“å†²
            })
            
            # å¯åŠ¨è¿›ç¨‹ï¼Œé‡å®šå‘è¾“å‡ºåˆ°ç‹¬ç«‹çš„æ—¥å¿—æ–‡ä»¶
            with open(log_file_path, 'w', encoding='utf-8') as log_file:
                process = subprocess.Popen(
                    cmd,
                    cwd=self.project_root,
                    stdout=log_file,
                    stderr=subprocess.STDOUT,
                    **process_kwargs  # è·¨å¹³å°è¿›ç¨‹ç»„åˆ›å»º
                )
            
            # è®°å½•åˆ°æ•°æ®åº“
            db_process = DiggingProcess(
                process_id=process.pid,
                status="running",
                script_type=script_type,
                tag_name=tag,
                started_at=datetime.now(),
                log_file_path=log_file_path
            )
            db.add(db_process)
            db.commit()
            
            # è®°å½•å®¡è®¡æ—¥å¿—
            audit_details = {
                "script_type": script_type,
                "script_name": script_name,
                "tag": tag,
                "command": " ".join(cmd)
            }
            # æ·»åŠ è„šæœ¬å‚æ•°åˆ°å®¡è®¡æ—¥å¿—
            if script_params:
                audit_details["script_params"] = script_params
                
            audit_log = AuditLog(
                user_id=user_id,
                action="start_script",
                resource_type="INDEPENDENT_SCRIPT",
                resource_id=str(process.pid),
                details=audit_details
            )
            db.add(audit_log)
            db.commit()
            
            return {
                "status": "started",
                "pid": process.pid,
                "script_type": script_type,
                "script_name": script_name,
                "tag": tag,
                "log_file": log_file_path,
                "start_time": db_process.started_at.isoformat()
            }
            
        except Exception as e:
            db.rollback()
            raise ProcessError(f"å¯åŠ¨{self.script_names.get(script_type, script_type)}å¤±è´¥: {str(e)}")
    
    def stop_independent_script(self, script_type: str, user_id: int, db: Session, force: bool = False) -> Dict[str, Any]:
        """åœæ­¢ç‹¬ç«‹è„šæœ¬"""
        try:
            # éªŒè¯è„šæœ¬ç±»å‹
            if script_type not in self.script_paths:
                raise ValidationError(f"ä¸æ”¯æŒçš„è„šæœ¬ç±»å‹: {script_type}")
            
            # è·å–å½“å‰è¿è¡Œçš„è„šæœ¬è¿›ç¨‹
            db_process = db.query(DiggingProcess).filter(
                DiggingProcess.script_type == script_type,
                DiggingProcess.status == "running"
            ).first()
            
            if not db_process:
                raise ProcessError(f"{self.script_names[script_type]}æœªåœ¨è¿è¡Œ")
            
            pid = db_process.process_id
            script_name = self.script_names[script_type]
            
            # ç»ˆæ­¢è¿›ç¨‹
            try:
                # ä½¿ç”¨è·¨å¹³å°æ–¹æ³•ç»ˆæ­¢è¿›ç¨‹
                terminate_method = self._terminate_process_group(pid, force)
                
                if not force:
                    # ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œå¦‚æœè¿˜æ²¡ç»ˆæ­¢åˆ™å¼ºåˆ¶ç»ˆæ­¢
                    time.sleep(5)
                    try:
                        process = psutil.Process(pid)
                        if process.is_running():
                            terminate_method = self._terminate_process_group(pid, force=True)
                            terminate_method += " (after timeout)"
                    except psutil.NoSuchProcess:
                        pass  # è¿›ç¨‹å·²ç»ç»ˆæ­¢
                        
            except ProcessLookupError:
                # è¿›ç¨‹å·²ç»ä¸å­˜åœ¨
                pass
            except Exception as e:
                if not force:
                    raise ProcessError(f"ç»ˆæ­¢è¿›ç¨‹å¤±è´¥: {str(e)}")
                # å¦‚æœæ˜¯å¼ºåˆ¶ç»ˆæ­¢ï¼Œå¿½ç•¥é”™è¯¯
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            db_process.status = "stopped"
            db_process.stopped_at = datetime.now()
            db.commit()
            
            # è®°å½•å®¡è®¡æ—¥å¿—
            audit_log = AuditLog(
                user_id=user_id,
                action="stop_script",
                resource_type="INDEPENDENT_SCRIPT",
                resource_id=str(pid),
                details={
                    "script_type": script_type,
                    "script_name": script_name,
                    "tag": db_process.tag_name,
                    "terminate_method": terminate_method
                }
            )
            db.add(audit_log)
            db.commit()
            
            return {
                "status": "stopped",
                "pid": pid,
                "script_type": script_type,
                "script_name": script_name,
                "tag": db_process.tag_name,
                "terminate_method": terminate_method,
                "stop_time": db_process.stopped_at.isoformat()
            }
            
        except Exception as e:
            db.rollback()
            raise ProcessError(f"åœæ­¢{self.script_names.get(script_type, script_type)}å¤±è´¥: {str(e)}")
    
    def stop_task_by_id(self, task_id: int, user_id: int, db: Session, force: bool = False) -> Dict[str, Any]:
        """åœæ­¢ç‰¹å®šä»»åŠ¡IDçš„è„šæœ¬"""
        try:
            # è·å–ä»»åŠ¡è®°å½•
            task = db.query(DiggingProcess).filter(DiggingProcess.id == task_id).first()
            if not task:
                raise ProcessError(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")
            
            # åªèƒ½åœæ­¢æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
            if task.status != "running":
                raise ProcessError(f"ä»»åŠ¡æœªåœ¨è¿è¡Œï¼Œå½“å‰çŠ¶æ€: {task.status}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¿›ç¨‹ID
            if not task.process_id:
                raise ProcessError(f"ä»»åŠ¡æ²¡æœ‰å…³è”çš„è¿›ç¨‹ID")
            
            # åœæ­¢è¿›ç¨‹
            try:
                import psutil
                process = psutil.Process(task.process_id)
                
                if force:
                    # å¼ºåˆ¶ç»ˆæ­¢
                    process.kill()
                else:
                    # ä¼˜é›…åœæ­¢
                    process.terminate()
                
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                task.status = "stopped"
                task.stopped_at = datetime.now()
                db.commit()
                
                script_name = self.script_names.get(task.script_type, task.script_type)
                display_info = task.tag_name if task.tag_name else f"{script_name} (ID: {task.id})"
                
                return {
                    "message": f"ä»»åŠ¡åœæ­¢æˆåŠŸ: {display_info}",
                    "task_id": task.id,
                    "script_type": task.script_type,
                    "force": force
                }
                
            except psutil.NoSuchProcess:
                # è¿›ç¨‹å·²ç»ä¸å­˜åœ¨ï¼Œæ›´æ–°çŠ¶æ€
                task.status = "stopped"
                task.stopped_at = datetime.now()
                db.commit()
                
                return {
                    "message": f"è¿›ç¨‹å·²åœæ­¢ (PID {task.process_id} ä¸å­˜åœ¨)",
                    "task_id": task.id,
                    "script_type": task.script_type,
                    "force": force
                }
            except psutil.AccessDenied:
                raise ProcessError(f"æ²¡æœ‰æƒé™åœæ­¢è¿›ç¨‹ {task.process_id}")
            
        except Exception as e:
            db.rollback()
            if isinstance(e, ProcessError):
                raise
            raise ProcessError(f"åœæ­¢ä»»åŠ¡å¤±è´¥: {str(e)}")
    
    def delete_task(self, task_id: int, user_id: int, db: Session) -> Dict[str, Any]:
        """åˆ é™¤å·²åœæ­¢çš„ä»»åŠ¡åŠå…¶æ—¥å¿—"""
        try:
            # è·å–ä»»åŠ¡è®°å½•
            task = db.query(DiggingProcess).filter(DiggingProcess.id == task_id).first()
            if not task:
                raise ProcessError(f"ä»»åŠ¡ä¸å­˜åœ¨")
            
            # åªèƒ½åˆ é™¤å·²åœæ­¢çš„ä»»åŠ¡
            if task.status == "running":
                raise ProcessError(f"æ— æ³•åˆ é™¤æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡")
            
            # åˆ é™¤æ—¥å¿—æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            log_deleted = False
            log_deletion_details = []
            
            print(f"\n=== å¼€å§‹åˆ é™¤ä»»åŠ¡ {task.id} çš„æ—¥å¿—æ–‡ä»¶ ===")
            print(f"ä»»åŠ¡ä¿¡æ¯: ID={task.id}, è„šæœ¬ç±»å‹={task.script_type}, çŠ¶æ€={task.status}")
            
            if task.log_file_path:
                print(f"ä»»åŠ¡æ—¥å¿—æ–‡ä»¶è·¯å¾„: {task.log_file_path}")
                
                # æ£€æŸ¥æ—¥å¿—ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼ˆè°ƒè¯•ç”¨ï¼‰
                log_dir = os.path.dirname(task.log_file_path)
                if os.path.exists(log_dir):
                    all_log_files = [f for f in os.listdir(log_dir) if f.endswith('.log')]
                    print(f"æ—¥å¿—ç›®å½• {log_dir} ä¸­çš„æ‰€æœ‰æ—¥å¿—æ–‡ä»¶ ({len(all_log_files)}ä¸ª):")
                    for i, f in enumerate(all_log_files):
                        print(f"  {i+1}. {f}")
                else:
                    print(f"æ—¥å¿—ç›®å½•ä¸å­˜åœ¨: {log_dir}")
                
                if os.path.exists(task.log_file_path):
                    try:
                        # æ£€æŸ¥æ–‡ä»¶æƒé™
                        if os.access(task.log_file_path, os.W_OK):
                            os.remove(task.log_file_path)
                            log_deleted = True
                            log_deletion_details.append(f"å·²åˆ é™¤æ—¥å¿—æ–‡ä»¶: {task.log_file_path}")
                            print(f"æˆåŠŸåˆ é™¤æ—¥å¿—æ–‡ä»¶: {task.log_file_path}")
                        else:
                            log_deletion_details.append(f"æƒé™ä¸è¶³ï¼Œæ— æ³•åˆ é™¤: {task.log_file_path}")
                            print(f"è­¦å‘Š: æƒé™ä¸è¶³ï¼Œæ— æ³•åˆ é™¤æ—¥å¿—æ–‡ä»¶: {task.log_file_path}")
                    except Exception as e:
                        # æ—¥å¿—åˆ é™¤å¤±è´¥ä¸å½±å“ä»»åŠ¡åˆ é™¤
                        error_msg = f"åˆ é™¤æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}"
                        log_deletion_details.append(error_msg)
                        print(f"è­¦å‘Š: {error_msg}")
                else:
                    # æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆå¯èƒ½æ˜¯æ—©æœŸä»»åŠ¡ä½¿ç”¨ç»Ÿä¸€æ—¥å¿—æ–‡ä»¶ï¼‰
                    info_msg = f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {task.log_file_path}"
                    log_deletion_details.append(info_msg)
                    print(f"ä¿¡æ¯: {info_msg}")
            else:
                # æ—©æœŸä»»åŠ¡å¯èƒ½æ²¡æœ‰è®°å½•æ—¥å¿—æ–‡ä»¶è·¯å¾„
                info_msg = f"ä»»åŠ¡ {task.id} æ²¡æœ‰å…³è”çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„"
                log_deletion_details.append(info_msg)
                print(f"ä¿¡æ¯: {info_msg}")
            
            # å°è¯•åˆ é™¤ç›¸å…³çš„è½®è½¬æ—¥å¿—æ–‡ä»¶å’ŒåŒPIDçš„å…³è”æ—¥å¿—
            # æ·»åŠ å®‰å…¨æ£€æŸ¥ï¼šåªæœ‰å½“ä»»åŠ¡ç¡®å®å·²åœæ­¢æ—¶æ‰æ¸…ç†ç›¸å…³æ—¥å¿—
            if task.log_file_path and task.status != "running":
                # è¿›ä¸€æ­¥æ£€æŸ¥ï¼šéªŒè¯æ²¡æœ‰å…¶ä»–æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ä½¿ç”¨ç›¸åŒçš„æ—¥å¿—æ¨¡å¼
                if self._is_safe_to_cleanup_logs(task, db):
                    self._cleanup_related_log_files(task, log_deletion_details, db)
                else:
                    warning_msg = f"æ£€æµ‹åˆ°ç›¸å…³çš„æ­£åœ¨è¿è¡Œä»»åŠ¡ï¼Œè·³è¿‡ç›¸å…³æ—¥å¿—æ¸…ç†ä»¥ç¡®ä¿å®‰å…¨"
                    log_deletion_details.append(warning_msg)
                    print(f"è­¦å‘Š: {warning_msg}")
            
            # è®°å½•å®¡è®¡æ—¥å¿—
            audit_log = AuditLog(
                user_id=user_id,
                action="delete_task",
                resource_type="task",
                resource_id=str(task.id),
                details=f"åˆ é™¤ä»»åŠ¡: {task.script_type} (ID: {task.id})"
                # created_at ä¼šè‡ªåŠ¨è®¾ç½®ä¸ºå½“å‰æ—¶é—´
            )
            db.add(audit_log)
            
            # åˆ é™¤ä»»åŠ¡è®°å½•
            db.delete(task)
            db.commit()
            
            # è¾“å‡ºæ—¥å¿—æ¸…ç†æ±‡æ€»
            print(f"\n=== ä»»åŠ¡ {task.id} åˆ é™¤å®Œæˆ ===")
            print(f"æ—¥å¿—æ¸…ç†è¯¦æƒ… ({len(log_deletion_details)}æ¡):")
            for detail in log_deletion_details:
                print(f"  - {detail}")
            
            # å†æ¬¡æ£€æŸ¥æ—¥å¿—ç›®å½•ï¼ˆç¡®è®¤æ¸…ç†ç»“æœï¼‰
            if task.log_file_path:
                log_dir = os.path.dirname(task.log_file_path)
                if os.path.exists(log_dir):
                    remaining_log_files = [f for f in os.listdir(log_dir) if f.endswith('.log')]
                    print(f"æ¸…ç†åå‰©ä½™æ—¥å¿—æ–‡ä»¶ ({len(remaining_log_files)}ä¸ª):")
                    for i, f in enumerate(remaining_log_files):
                        print(f"  {i+1}. {f}")
            
            # æ„å»ºè¿”å›æ¶ˆæ¯
            message = f"ä»»åŠ¡å·²åˆ é™¤"
            if log_deleted:
                message += "ï¼Œæ—¥å¿—æ–‡ä»¶å·²æ¸…ç†"
            elif task.log_file_path:
                message += "ï¼Œæ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼ˆå¯èƒ½æ˜¯æ—©æœŸä»»åŠ¡ï¼‰"
            else:
                message += "ï¼Œæ— å…³è”æ—¥å¿—æ–‡ä»¶"

            return {
                "status": "success",
                "message": message,
                "task_id": task_id,
                "script_type": task.script_type,
                "log_deleted": log_deleted,
                "log_deletion_details": log_deletion_details  # æ·»åŠ è¯¦ç»†çš„åˆ é™¤ä¿¡æ¯
            }
            
        except Exception as e:
            db.rollback()
            raise ProcessError(f"åˆ é™¤ä»»åŠ¡å¤±è´¥: {str(e)}")
    
    def get_all_scripts_status(self, db: Session) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰è„šæœ¬çš„çŠ¶æ€ - æ˜¾ç¤ºæ‰€æœ‰å†å²ä»»åŠ¡ï¼Œæ´»è·ƒä»»åŠ¡åœ¨é¡¶ç«¯"""
        try:
            # è¿”å›æ ¼å¼ï¼š{'scripts': [è„šæœ¬å®ä¾‹åˆ—è¡¨], 'script_types': {ç±»å‹æ˜ å°„}}
            result = {
                'scripts': [],
                'script_types': self.script_names
            }
            
            # è·å–æ‰€æœ‰è¿›ç¨‹è®°å½•ï¼ŒæŒ‰çŠ¶æ€å’Œæ—¶é—´æ’åºï¼ˆè¿è¡Œä¸­çš„åœ¨å‰ï¼Œç„¶åæŒ‰æ—¶é—´å€’åºï¼‰
            # ä½¿ç”¨CASEè¯­å¥ç¡®ä¿runningçŠ¶æ€æ’åœ¨stoppedçŠ¶æ€å‰é¢
            from sqlalchemy import case
            all_processes = db.query(DiggingProcess).order_by(
                case(
                    (DiggingProcess.status == "running", 0),
                    (DiggingProcess.status == "stopped", 1),
                    else_=2
                ).asc(),  # running(0) < stopped(1)
                DiggingProcess.started_at.desc()  # æœ€æ–°çš„åœ¨å‰
            ).all()
            
            # å¤„ç†æ‰€æœ‰è¿›ç¨‹è®°å½•
            for process_record in all_processes:
                script_info = {
                    "id": process_record.id,
                    "script_type": process_record.script_type,
                    "status": process_record.status,
                    "script_name": self.script_names.get(process_record.script_type, process_record.script_type),
                    "tag": process_record.tag_name,
                    "started_at": process_record.started_at.isoformat(),
                    "log_file": process_record.log_file_path
                }
                
                if process_record.status == "running":
                    # éªŒè¯è¿›ç¨‹æ˜¯å¦çœŸçš„åœ¨è¿è¡Œ
                    try:
                        process = psutil.Process(process_record.process_id)
                        if process.is_running():
                            script_info["pid"] = process_record.process_id
                        else:
                            # è¿›ç¨‹å·²æ­»ï¼Œæ›´æ–°æ•°æ®åº“çŠ¶æ€
                            process_record.status = "stopped"
                            process_record.stopped_at = datetime.now()
                            db.commit()
                            script_info["status"] = "stopped"
                    except psutil.NoSuchProcess:
                        # è¿›ç¨‹ä¸å­˜åœ¨ï¼Œæ›´æ–°æ•°æ®åº“çŠ¶æ€
                        process_record.status = "stopped"
                        process_record.stopped_at = datetime.now()
                        db.commit()
                        script_info["status"] = "stopped"
                else:
                    # å·²åœæ­¢çš„ä»»åŠ¡ï¼Œæ·»åŠ åœæ­¢æ—¶é—´
                    if process_record.stopped_at:
                        script_info["stopped_at"] = process_record.stopped_at.isoformat()
                
                result['scripts'].append(script_info)
            
            
            return result
            
        except Exception as e:
            raise ProcessError(f"è·å–è„šæœ¬çŠ¶æ€å¤±è´¥: {str(e)}")
    
    def stop_process(self, user_id: int, db: Session, force: bool = False) -> Dict[str, Any]:
        """åœæ­¢æŒ–æ˜è¿›ç¨‹"""
        try:
            # è·å–å½“å‰è¿è¡Œçš„è¿›ç¨‹
            current_status = self.get_current_process_status(db)
            
            if current_status["status"] != "running":
                raise ProcessError("æ²¡æœ‰æ­£åœ¨è¿è¡Œçš„æŒ–æ˜è¿›ç¨‹")
            
            pid = current_status["pid"]
            if not pid:
                raise ProcessError("æ— æ³•è·å–è¿›ç¨‹PID")
            
            # è·å–æ•°æ®åº“ä¸­çš„è¿›ç¨‹è®°å½•
            db_process = db.query(DiggingProcess).filter(
                DiggingProcess.process_id == pid,
                DiggingProcess.status == "running"
            ).first()
            
            if not db_process:
                raise ProcessError("æ•°æ®åº“ä¸­æœªæ‰¾åˆ°å¯¹åº”çš„è¿›ç¨‹è®°å½•")
            
            # ç»ˆæ­¢è¿›ç¨‹
            try:
                # ä½¿ç”¨è·¨å¹³å°æ–¹æ³•ç»ˆæ­¢è¿›ç¨‹
                terminate_method = self._terminate_process_group(pid, force)
                
                if not force:
                    # ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œå¦‚æœè¿˜æ²¡ç»ˆæ­¢åˆ™å¼ºåˆ¶ç»ˆæ­¢
                    time.sleep(5)
                    try:
                        process = psutil.Process(pid)
                        if process.is_running():
                            terminate_method = self._terminate_process_group(pid, force=True)
                            terminate_method += " (after timeout)"
                    except psutil.NoSuchProcess:
                        pass  # è¿›ç¨‹å·²ç»ç»ˆæ­¢
                        
            except ProcessLookupError:
                # è¿›ç¨‹å·²ç»ä¸å­˜åœ¨
                pass
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€
            db_process.status = "stopped"
            db_process.stopped_at = datetime.now()
            
            # è®°å½•å®¡è®¡æ—¥å¿—
            audit_log = AuditLog(
                user_id=user_id,
                action="STOP_PROCESS",
                resource_type="DIGGING_PROCESS", 
                resource_id=str(pid),
                details={
                    "terminate_method": terminate_method,
                    "force": force
                }
            )
            db.add(audit_log)
            db.commit()
            
            return {
                "status": "stopped",
                "pid": pid,
                "terminate_method": terminate_method,
                "stop_time": db_process.stopped_at.isoformat()
            }
            
        except Exception as e:
            db.rollback()
            raise ProcessError(f"åœæ­¢è¿›ç¨‹å¤±è´¥: {str(e)}")
    
    def get_process_logs(self, limit: int = 100, offset: int = 0) -> List[Dict[str, Any]]:
        """è·å–è¿›ç¨‹æ—¥å¿—"""
        try:
            log_file = os.path.join(self.project_root, "logs", "unified_digging.log")
            
            if not os.path.exists(log_file):
                return []
            
            with open(log_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # å€’åºè·å–æœ€æ–°çš„æ—¥å¿—
            lines = lines[::-1]
            
            # åˆ†é¡µ
            start = offset
            end = offset + limit
            selected_lines = lines[start:end]
            
            logs = []
            for i, line in enumerate(selected_lines):
                try:
                    # å°è¯•è§£æJSONæ ¼å¼çš„æ—¥å¿—
                    log_data = json.loads(line.strip())
                    logs.append({
                        "id": offset + i,
                        "timestamp": log_data.get("timestamp"),
                        "level": log_data.get("level", "INFO"),
                        "message": log_data.get("message", line.strip()),
                        "logger": log_data.get("logger"),
                        "details": log_data
                    })
                except json.JSONDecodeError:
                    # æ™®é€šæ–‡æœ¬æ ¼å¼çš„æ—¥å¿—
                    logs.append({
                        "id": offset + i,
                        "timestamp": None,
                        "level": "INFO",
                        "message": line.strip(),
                        "logger": None,
                        "details": {}
                    })
            
            return logs
            
        except Exception as e:
            raise ProcessError(f"è·å–è¿›ç¨‹æ—¥å¿—å¤±è´¥: {str(e)}")
    
    def _create_config_file(self, config: DiggingConfig) -> str:
        """åˆ›å»ºé…ç½®æ–‡ä»¶"""
        try:
            # ç”Ÿæˆä¸´æ—¶é…ç½®æ–‡ä»¶å
            timestamp = int(time.time())
            temp_config_path = os.path.join(
                self.project_root, 
                "config", 
                f"digging_config_temp_{timestamp}.txt"
            )
            
            # è¯»å–åŸå§‹é…ç½®æ–‡ä»¶
            with open(self.config_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            # æ›´æ–°é…ç½®å€¼
            config_dict = config.dict()
            
            # å­—æ®µåæ˜ å°„ï¼šæ•°æ®åº“å­—æ®µå -> é…ç½®æ–‡ä»¶å­—æ®µå
            field_name_mapping = {
                'dataset_id': 'priority_dataset'  # æ•°æ®åº“ä¸­çš„dataset_idå¯¹åº”é…ç½®æ–‡ä»¶ä¸­çš„priority_dataset
            }
            
            updated_lines = []
            
            for line in lines:
                if ':' in line and not line.strip().startswith('#'):
                    key = line.split(':')[0].strip()
                    
                    # æŸ¥æ‰¾åŒ¹é…çš„é…ç½®å€¼ï¼ˆè€ƒè™‘å­—æ®µåæ˜ å°„ï¼‰
                    config_value = None
                    if key in config_dict:
                        config_value = config_dict[key]
                    else:
                        # æ£€æŸ¥æ˜¯å¦æœ‰åå‘æ˜ å°„
                        for db_field, config_field in field_name_mapping.items():
                            if config_field == key and db_field in config_dict:
                                config_value = config_dict[db_field]
                                break
                    
                    if config_value is not None:
                        # ç‰¹æ®Šå¤„ç†recommended_fieldsï¼ˆéœ€è¦è½¬ä¸ºJSONå­—ç¬¦ä¸²ï¼‰
                        if key == 'recommended_fields':
                            value = json.dumps(config_value)
                        else:
                            value = str(config_value).lower() if isinstance(config_value, bool) else str(config_value)
                        updated_lines.append(f"{key}: {value}\n")
                    else:
                        updated_lines.append(line)
                else:
                    updated_lines.append(line)
            
            # å†™å…¥ä¸´æ—¶é…ç½®æ–‡ä»¶
            with open(temp_config_path, 'w', encoding='utf-8') as f:
                f.writelines(updated_lines)
            
            return temp_config_path
            
        except Exception as e:
            raise ProcessError(f"åˆ›å»ºé…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def _is_safe_to_cleanup_logs(self, task, db) -> bool:
        """æ£€æŸ¥æ˜¯å¦å®‰å…¨æ¸…ç†æ—¥å¿—æ–‡ä»¶ï¼ˆç¡®ä¿ä¸ä¼šå½±å“å…¶ä»–æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ï¼‰"""
        try:
            if not task.log_file_path:
                return True
            
            # è§£æå½“å‰ä»»åŠ¡çš„æ—¥å¿—æ–‡ä»¶å
            log_filename = os.path.basename(task.log_file_path)
            
            # æå–PID
            import re
            match = re.match(r'^(.+)_(\d{8})_(\d{6})_(\d+)\.log$', log_filename)
            if not match:
                # æ— æ³•è§£ææ–‡ä»¶åæ ¼å¼ï¼Œä¿å®ˆèµ·è§ä¸æ¸…ç†
                return False
            
            script_type = match.group(1)
            date_part = match.group(2)
            time_part = match.group(3)
            pid = match.group(4)
            
            # æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦æœ‰å…¶ä»–æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ä½¿ç”¨ç›¸åŒçš„è„šæœ¬ç±»å‹
            from sqlalchemy.orm import Session
            from ..db.models import DiggingProcess
            
            running_tasks = db.query(DiggingProcess).filter(
                DiggingProcess.status == "running",
                DiggingProcess.script_type == script_type,
                DiggingProcess.id != task.id
            ).all()
            
            if running_tasks:
                print(f"å®‰å…¨æ£€æŸ¥: å‘ç° {len(running_tasks)} ä¸ªæ­£åœ¨è¿è¡Œçš„ {script_type} ä»»åŠ¡ï¼Œè·³è¿‡æ—¥å¿—æ¸…ç†")
                return False
            
            return True
            
        except Exception as e:
            print(f"å®‰å…¨æ£€æŸ¥å¼‚å¸¸: {e}ï¼Œä¿å®ˆèµ·è§è·³è¿‡æ—¥å¿—æ¸…ç†")
            return False

    def _is_pid_in_use_by_running_tasks(self, pid: str, script_type: str, exclude_task_id: int, db) -> bool:
        """æ£€æŸ¥æŒ‡å®šPIDæ˜¯å¦è¢«å…¶ä»–æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡ä½¿ç”¨"""
        try:
            from ..db.models import DiggingProcess
            
            # æŸ¥æ‰¾ä½¿ç”¨ç›¸åŒPIDä¸”æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
            running_tasks_with_pid = db.query(DiggingProcess).filter(
                DiggingProcess.status == "running",
                DiggingProcess.script_type == script_type,
                DiggingProcess.id != exclude_task_id,
                DiggingProcess.process_id == int(pid)
            ).all()
            
            return len(running_tasks_with_pid) > 0
            
        except (ValueError, Exception) as e:
            print(f"PIDä½¿ç”¨æ£€æŸ¥å¼‚å¸¸: {e}ï¼Œä¿å®ˆèµ·è§è®¤ä¸ºæ­£åœ¨ä½¿ç”¨")
            return True  # ä¿å®ˆç­–ç•¥ï¼šæœ‰ç–‘è™‘å°±ä¸åˆ é™¤

    def _cleanup_related_log_files(self, task, log_deletion_details, db):
        """æ¸…ç†ä¸ä»»åŠ¡ç›¸å…³çš„æ—¥å¿—æ–‡ä»¶"""
        import re
        
        try:
            log_dir = os.path.dirname(task.log_file_path)
            if not os.path.exists(log_dir):
                return
            
            # è§£æå½“å‰ä»»åŠ¡çš„æ—¥å¿—æ–‡ä»¶å
            log_filename = os.path.basename(task.log_file_path)
            
            # æå–æ–‡ä»¶åç»„ä»¶: script_type_YYYYMMDD_HHMMSS_pid.log
            match = re.match(r'^(.+)_(\d{8})_(\d{6})_(\d+)\.log$', log_filename)
            if not match:
                # å¦‚æœæ— æ³•è§£æï¼Œä½¿ç”¨åŸæ¥çš„è½®è½¬æ—¥å¿—æ¸…ç†é€»è¾‘
                self._cleanup_rotated_logs(task.log_file_path, log_deletion_details)
                return
            
            script_type = match.group(1)
            date_part = match.group(2)
            time_part = match.group(3)
            pid = match.group(4)
            
            print(f"æ¸…ç†ä»»åŠ¡æ—¥å¿—: è„šæœ¬={script_type}, æ—¥æœŸ={date_part}, æ—¶é—´={time_part}, PID={pid}")
            
            # æŸ¥æ‰¾ç›¸å…³æ—¥å¿—æ–‡ä»¶ï¼ˆä¸¥æ ¼åŒ¹é…ï¼Œé¿å…è¯¯åˆ ï¼‰
            related_files = []
            
            for file in os.listdir(log_dir):
                if not file.endswith('.log'):
                    continue
                
                file_path = os.path.join(log_dir, file)
                
                # 1. è½®è½¬æ—¥å¿—æ–‡ä»¶ (ç¡®åˆ‡åŒ¹é…: åŸæ–‡ä»¶å.log.æ•°å­—)
                if file.startswith(log_filename + ".") and file != log_filename:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æ•°å­—åç¼€çš„è½®è½¬æ–‡ä»¶
                    suffix = file[len(log_filename)+1:]
                    if suffix.isdigit():
                        related_files.append((file_path, "è½®è½¬æ—¥å¿—"))
                        print(f"æ‰¾åˆ°è½®è½¬æ—¥å¿—: {file}")
                
                # 2. ã€å·²ç§»é™¤ã€‘åŒPIDå…³è”æ—¥å¿—æ¸…ç†é€»è¾‘
                # 
                # åŸå› ï¼šä¸åŒçš„ä»»åŠ¡å®ä¾‹å¯èƒ½ä½¿ç”¨ç›¸åŒçš„PIDï¼Œåˆ é™¤ä¸€ä¸ªä»»åŠ¡æ—¶ä¸åº”è¯¥
                # ä»…ä»…å› ä¸ºPIDç›¸åŒå°±åˆ é™¤å…¶ä»–ä»»åŠ¡çš„æ—¥å¿—æ–‡ä»¶ã€‚
                # ä¾‹å¦‚ï¼š
                # - unified_digging_20250904_213749_1.log (ä»»åŠ¡A, PID=1)  
                # - unified_digging_20250904_213838_1.log (ä»»åŠ¡B, PID=1)
                # åˆ é™¤ä»»åŠ¡Bæ—¶ï¼Œä¸åº”è¯¥åˆ é™¤ä»»åŠ¡Açš„æ—¥å¿—ã€‚
                # 
                # æ³¨é‡ŠåŸå§‹é€»è¾‘ï¼š
                # elif file != log_filename and file.endswith(f"_{pid}.log"):
                #     if file.startswith(f"{script_type}_"):
                #         related_files.append((file_path, "åŒè„šæœ¬åŒPIDå…³è”æ—¥å¿—"))
                
                # ã€ä¿ç•™æ³¨é‡Šä»¥ä¾¿è°ƒè¯•ã€‘è·³è¿‡åŒPIDä½†ä¸åŒä»»åŠ¡çš„æ—¥å¿—æ–‡ä»¶
                elif file != log_filename and file.endswith(f"_{pid}.log"):
                    if re.match(rf'^.+_\d{{8}}_\d{{6}}_{re.escape(pid)}\.log$', file):
                        if file.startswith(f"{script_type}_"):
                            print(f"ğŸš« è·³è¿‡åŒè„šæœ¬åŒPIDæ—¥å¿—: {file} (å¯èƒ½æ˜¯ä¸åŒä»»åŠ¡å®ä¾‹)")
                        else:
                            print(f"ğŸš« è·³è¿‡ä¸åŒè„šæœ¬ç±»å‹çš„åŒPIDæ—¥å¿—: {file} (ä¸åŒè„šæœ¬ç±»å‹)")
                
                # 3. æŸ¥æ‰¾åŒä¸€æ—¶é—´æˆ³ä½†ä¸åŒPIDçš„æ—¥å¿—æ–‡ä»¶ï¼ˆå¤„ç†åŒé‡æ—¥å¿—é—®é¢˜ï¼‰
                elif file != log_filename and file.startswith(f"{script_type}_{date_part}_{time_part}_"):
                    # éªŒè¯æ–‡ä»¶åæ ¼å¼ï¼šscript_type_YYYYMMDD_HHMMSS_differentPID.log
                    import re
                    match_pattern = rf'^{re.escape(script_type)}_{re.escape(date_part)}_{re.escape(time_part)}_(\d+)\.log$'
                    if re.match(match_pattern, file):
                        # è¿›ä¸€æ­¥å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿è¿™ä¸ªPIDä¸å±äºå…¶ä»–æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
                        other_pid = re.match(match_pattern, file).group(1)
                        if not self._is_pid_in_use_by_running_tasks(other_pid, script_type, task.id, db):
                            related_files.append((file_path, "åŒæ—¶é—´æˆ³å…³è”æ—¥å¿—"))
                            print(f"æ‰¾åˆ°åŒæ—¶é—´æˆ³æ—¥å¿—: {file} (PID: {other_pid})")
                        else:
                            print(f"è·³è¿‡åŒæ—¶é—´æˆ³æ—¥å¿—: {file} (PID {other_pid} æ­£åœ¨è¢«å…¶ä»–ä»»åŠ¡ä½¿ç”¨)")
                
                # 4. ä¸åˆ é™¤å…¶ä»–ä»»ä½•æ–‡ä»¶ï¼Œé¿å…è¯¯åˆ æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡æ—¥å¿—
            
            # åˆ é™¤æ‰¾åˆ°çš„ç›¸å…³æ–‡ä»¶
            for file_path, file_type in related_files:
                try:
                    if os.path.exists(file_path):
                        file_size = os.path.getsize(file_path) / 1024 / 1024  # MB
                        os.remove(file_path)
                        log_deletion_details.append(f"å·²åˆ é™¤{file_type}: {os.path.basename(file_path)} ({file_size:.2f}MB)")
                        print(f"å·²åˆ é™¤{file_type}: {os.path.basename(file_path)} ({file_size:.2f}MB)")
                except Exception as e:
                    log_deletion_details.append(f"åˆ é™¤{file_type}å¤±è´¥: {os.path.basename(file_path)} - {e}")
                    print(f"è­¦å‘Š: åˆ é™¤{file_type}å¤±è´¥: {os.path.basename(file_path)} - {e}")
            
            if related_files:
                print(f"æ¸…ç†å®Œæˆ: åˆ é™¤äº† {len(related_files)} ä¸ªç›¸å…³æ—¥å¿—æ–‡ä»¶")
            else:
                print(f"æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„ç›¸å…³æ—¥å¿—æ–‡ä»¶")
                
        except Exception as e:
            log_deletion_details.append(f"æ¸…ç†ç›¸å…³æ—¥å¿—æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            print(f"è­¦å‘Š: æ¸…ç†ç›¸å…³æ—¥å¿—æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    def _cleanup_rotated_logs(self, log_file_path, log_deletion_details):
        """æ¸…ç†è½®è½¬æ—¥å¿—æ–‡ä»¶ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰- ä¿®å¤è¿‡åº¦åˆ é™¤é—®é¢˜"""
        try:
            log_dir = os.path.dirname(log_file_path)
            log_filename = os.path.basename(log_file_path)
            
            if not os.path.exists(log_dir):
                return
            
            # æŸ¥æ‰¾ç›¸å…³çš„è½®è½¬æ—¥å¿—æ–‡ä»¶ (åªæŸ¥æ‰¾ç¡®åˆ‡çš„è½®è½¬æ–‡ä»¶: original.log.1, original.log.2, ç­‰)
            rotated_files = []
            for file in os.listdir(log_dir):
                file_path = os.path.join(log_dir, file)
                
                # åªåŒ¹é…ç¡®åˆ‡çš„è½®è½¬æ–‡ä»¶æ ¼å¼: åŸæ–‡ä»¶å.log.æ•°å­—
                # ä¾‹å¦‚: correlation_checker_20250904_192615_123.log.1
                if file.startswith(log_filename + ".") and file != log_filename:
                    # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å­—åç¼€ï¼ˆè½®è½¬æ–‡ä»¶çš„ç‰¹å¾ï¼‰
                    suffix = file[len(log_filename)+1:]  # å»æ‰ "åŸæ–‡ä»¶å." éƒ¨åˆ†
                    if suffix.isdigit():  # åªæœ‰çº¯æ•°å­—åç¼€æ‰æ˜¯è½®è½¬æ–‡ä»¶
                        rotated_files.append(file_path)
                        print(f"æ‰¾åˆ°è½®è½¬æ—¥å¿—æ–‡ä»¶: {file}")
            
            # åˆ é™¤è½®è½¬æ—¥å¿—æ–‡ä»¶
            for rotated_file in rotated_files:
                try:
                    if os.path.exists(rotated_file):
                        file_size = os.path.getsize(rotated_file) / 1024 / 1024  # MB
                        os.remove(rotated_file)
                        log_deletion_details.append(f"å·²åˆ é™¤è½®è½¬æ—¥å¿—: {os.path.basename(rotated_file)} ({file_size:.2f}MB)")
                        print(f"å·²åˆ é™¤è½®è½¬æ—¥å¿—: {os.path.basename(rotated_file)} ({file_size:.2f}MB)")
                except Exception as e:
                    log_deletion_details.append(f"åˆ é™¤è½®è½¬æ—¥å¿—å¤±è´¥: {os.path.basename(rotated_file)} - {e}")
                    print(f"è­¦å‘Š: åˆ é™¤è½®è½¬æ—¥å¿—å¤±è´¥: {os.path.basename(rotated_file)} - {e}")
            
            if rotated_files:
                print(f"è½®è½¬æ—¥å¿—æ¸…ç†å®Œæˆ: åˆ é™¤äº† {len(rotated_files)} ä¸ªè½®è½¬æ–‡ä»¶")
            else:
                print(f"æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„è½®è½¬æ—¥å¿—æ–‡ä»¶")
                    
        except Exception as e:
            log_deletion_details.append(f"æŸ¥æ‰¾è½®è½¬æ—¥å¿—æ—¶å‡ºé”™: {e}")
            print(f"è­¦å‘Š: æŸ¥æ‰¾è½®è½¬æ—¥å¿—æ—¶å‡ºé”™: {e}")

    def cleanup_temp_configs(self):
        """æ¸…ç†ä¸´æ—¶é…ç½®æ–‡ä»¶"""
        try:
            config_dir = os.path.join(self.project_root, "config")
            current_time = time.time()
            
            for filename in os.listdir(config_dir):
                if filename.startswith("digging_config_temp_"):
                    file_path = os.path.join(config_dir, filename)
                    file_time = os.path.getctime(file_path)
                    
                    # åˆ é™¤è¶…è¿‡1å°æ—¶çš„ä¸´æ—¶é…ç½®æ–‡ä»¶
                    if current_time - file_time > 3600:
                        os.remove(file_path)
                        
        except Exception as e:
            # æ¸…ç†å¤±è´¥ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œåªè®°å½•æ—¥å¿—
            print(f"æ¸…ç†ä¸´æ—¶é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")

    def _ensure_log_rotation(self, log_file_path: str, max_size_mb: int = 10, max_backups: int = 3):
        """
        çˆ¶è¿›ç¨‹çº§åˆ«çš„æ—¥å¿—è½®è½¬ç®¡ç†
        
        Args:
            log_file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            max_size_mb: æœ€å¤§æ–‡ä»¶å¤§å°(MB)ï¼Œé»˜è®¤10MB
            max_backups: æœ€å¤§å¤‡ä»½æ–‡ä»¶æ•°é‡ï¼Œé»˜è®¤3ä¸ª
        """
        try:
            # å¦‚æœæ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— éœ€è½®è½¬
            if not os.path.exists(log_file_path):
                return
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size_mb = os.path.getsize(log_file_path) / 1024 / 1024
            
            if file_size_mb <= max_size_mb:
                return  # æ–‡ä»¶æœªè¶…è¿‡å¤§å°é™åˆ¶
            
            print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶è¶…è¿‡å¤§å°é™åˆ¶: {file_size_mb:.2f}MB > {max_size_mb}MBï¼Œå¼€å§‹è½®è½¬...")
            
            # æ‰§è¡Œè½®è½¬ï¼šå‘åç§»åŠ¨ç°æœ‰å¤‡ä»½æ–‡ä»¶
            for i in range(max_backups, 0, -1):
                old_backup = f"{log_file_path}.{i}"
                new_backup = f"{log_file_path}.{i+1}"
                
                if i == max_backups:
                    # åˆ é™¤æœ€è€çš„å¤‡ä»½
                    if os.path.exists(old_backup):
                        os.remove(old_backup)
                        print(f"ğŸ—‘ï¸ åˆ é™¤æœ€è€å¤‡ä»½: {os.path.basename(old_backup)}")
                else:
                    # ç§»åŠ¨å¤‡ä»½æ–‡ä»¶
                    if os.path.exists(old_backup):
                        os.rename(old_backup, new_backup)
                        print(f"ğŸ“¦ ç§»åŠ¨å¤‡ä»½: {os.path.basename(old_backup)} â†’ {os.path.basename(new_backup)}")
            
            # å°†å½“å‰æ–‡ä»¶ç§»åŠ¨ä¸ºç¬¬ä¸€ä¸ªå¤‡ä»½
            first_backup = f"{log_file_path}.1"
            os.rename(log_file_path, first_backup)
            print(f"ğŸ”„ è½®è½¬å®Œæˆ: {os.path.basename(log_file_path)} â†’ {os.path.basename(first_backup)}")
            
            # åˆ›å»ºæ–°çš„æ—¥å¿—æ–‡ä»¶å¤´éƒ¨ä¿¡æ¯
            with open(log_file_path, 'w', encoding='utf-8') as f:
                f.write(f"ğŸ”„ æ—¥å¿—æ–‡ä»¶è½®è½¬å®Œæˆ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"ğŸ“ è½®è½¬åŸå› : æ–‡ä»¶å¤§å° {file_size_mb:.2f}MB è¶…è¿‡é™åˆ¶ {max_size_mb}MB\n")
                f.write(f"ğŸ“¦ å¤‡ä»½æ–‡ä»¶: {os.path.basename(first_backup)}\n")
                f.write("=" * 60 + "\n\n")
            
            print(f"âœ… æ—¥å¿—è½®è½¬å®Œæˆï¼Œåˆ›å»ºæ–°æ–‡ä»¶: {os.path.basename(log_file_path)}")
            
        except Exception as e:
            print(f"âŒ æ—¥å¿—è½®è½¬å¤±è´¥: {e}")
            # è½®è½¬å¤±è´¥ä¸å½±å“è¿›ç¨‹å¯åŠ¨ï¼Œç»§ç»­æ‰§è¡Œ


# å…¨å±€å®ä¾‹
process_service = ProcessService()
