"""
æ•°æ®é›†å­—æ®µæŸ¥è¯¢API
"""

import time
import asyncio
import uuid
from typing import Optional
from fastapi import APIRouter, Query, HTTPException, BackgroundTasks
from pydantic import BaseModel

# å¯¼å…¥WorldQuantç›¸å…³æ¨¡å—
import sys
import os
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
src_dir = current_dir.parent.parent.parent.parent / 'src'
sys.path.insert(0, str(src_dir))

try:
    from lib.data_client import get_datafields
    from sessions.session_client import get_session
except ImportError as e:
    print(f"Warning: Could not import required modules: {e}")
    get_datafields = None
    get_session = None

router = APIRouter(prefix="/dataset", tags=["dataset"])

# è¿›åº¦è·Ÿè¸ªå­˜å‚¨
progress_store = {}

# ç§»é™¤å…¨å±€sessionå˜é‡ï¼Œç›´æ¥ä½¿ç”¨session_client

async def get_datafields_with_progress(
    session,
    dataset_id: str,
    region: str = 'USA',
    universe: str = 'TOP3000',
    delay: int = 1,
    instrument_type: str = 'EQUITY',
    task_id: str = None
):
    """
    è·å–æ•°æ®é›†å­—æ®µä¿¡æ¯ï¼Œå¹¶å®æ—¶æ›´æ–°è¿›åº¦
    è¿™æ˜¯å¯¹machine_lib_ee.get_datafieldsçš„åŒ…è£…ï¼Œæ·»åŠ äº†åˆ†é¡µè¿›åº¦è·Ÿè¸ª
    """
    import requests
    import pandas as pd
    
    base_url = "https://api.worldquantbrain.com/data-fields"
    limit = 50
    offset = 0
    aggregated_results = []
    
    # è¿›åº¦è·Ÿè¸ªå˜é‡
    page_count = 0
    start_time = time.time()
    estimated_total = None

    # æ„å»ºå›ºå®šæŸ¥è¯¢å‚æ•°
    def make_params(current_offset: int):
        params = {
            'instrumentType': instrument_type,
            'region': region,
            'delay': str(delay),
            'universe': universe,
            'limit': str(limit),
            'offset': str(current_offset),
        }
        if dataset_id:
            params['dataset.id'] = dataset_id
        return params

    # è¿›åº¦æ›´æ–°å‡½æ•°
    def update_progress(progress_percent, message, details):
        if task_id and task_id in progress_store:
            progress_store[task_id].update({
                'progress': progress_percent,
                'message': message,
                'details': details
            })
            print(f"ğŸ“Š è¿›åº¦æ›´æ–°: {progress_percent}% - {message}")

    try:
        while True:
            params = make_params(offset)
            
            # å‘é€è¯·æ±‚
            resp = session.get(base_url, params=params, timeout=15)
            
            if resp.status_code != 200:
                print(f"get_datafields_with_progress: HTTP {resp.status_code}: {resp.text[:200]}")
                break
            
            try:
                data = resp.json()
            except Exception as e:
                print(f"get_datafields_with_progress: JSONè§£æå¤±è´¥: {e}")
                break
            
            # å¤„ç†ç»“æœ
            results = data.get('results', [])
            if not results:
                break
                
            aggregated_results.extend(results)
            page_count += 1
            
            # é¦–æ¬¡è·å–ï¼Œå°è¯•ä¼°ç®—æ€»è®°å½•æ•°
            if estimated_total is None and 'count' in data:
                estimated_total = data['count']
            elif estimated_total is None and len(results) == limit:
                # ç²—ç•¥ä¼°ç®—ä¸ºè‡³å°‘2å€å½“å‰å·²è·å–æ•°
                estimated_total = len(aggregated_results) * 2
            
            # è®¡ç®—å¹¶æ›´æ–°è¿›åº¦
            current_count = len(aggregated_results)
            elapsed_time = time.time() - start_time
            
            # è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”
            if estimated_total and estimated_total > 0:
                progress_percent = min(95, int((current_count / estimated_total) * 100))
            else:
                # æ²¡æœ‰æ€»æ•°æ—¶ï¼ŒåŸºäºé¡µæ•°ç»™å‡ºæ¸è¿›è¿›åº¦
                progress_percent = min(80, 30 + page_count * 5)
            
            # ä¼°ç®—å‰©ä½™æ—¶é—´
            if page_count > 1 and current_count > 0:
                avg_time_per_record = elapsed_time / current_count
                if estimated_total:
                    remaining_records = estimated_total - current_count
                    estimated_remaining_time = remaining_records * avg_time_per_record
                else:
                    estimated_remaining_time = None
            else:
                estimated_remaining_time = None
            
            # æ„é€ è¿›åº¦æ¶ˆæ¯
            if estimated_total:
                message = f"æ­£åœ¨è·å–æ•°æ®é›†å­—æ®µ... ç¬¬{page_count}é¡µï¼Œå·²è·å– {current_count}/{estimated_total} ä¸ªå­—æ®µ"
            else:
                message = f"æ­£åœ¨è·å–æ•°æ®é›†å­—æ®µ... ç¬¬{page_count}é¡µï¼Œå·²è·å– {current_count} ä¸ªå­—æ®µ"
            
            if estimated_remaining_time and estimated_remaining_time > 0:
                message += f"ï¼Œé¢„è®¡å‰©ä½™ {estimated_remaining_time:.1f}ç§’"
            
            # æ›´æ–°è¿›åº¦
            update_progress(progress_percent, message, {
                'page_count': page_count,
                'current_count': current_count,
                'estimated_total': estimated_total,
                'elapsed_time': elapsed_time,
                'estimated_remaining_time': estimated_remaining_time
            })
            
            # ç»“æŸæ¡ä»¶ï¼šè¿”å›æ•°é‡å°äºlimit
            if len(results) < limit:
                # æœ€åä¸€æ¬¡è¿›åº¦æ›´æ–°ï¼Œè®¾ä¸º100%
                final_count = len(aggregated_results)
                final_elapsed = time.time() - start_time
                update_progress(100, f"å®Œæˆï¼å…±è·å– {final_count} ä¸ªå­—æ®µï¼Œè€—æ—¶ {final_elapsed:.2f}ç§’", {
                    'page_count': page_count,
                    'current_count': final_count,
                    'estimated_total': final_count,
                    'elapsed_time': final_elapsed,
                    'estimated_remaining_time': 0
                })
                break
            
            # å‡†å¤‡ä¸‹ä¸€é¡µ
            offset += limit
            
            # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            await asyncio.sleep(0.1)
        
        return pd.DataFrame(aggregated_results)
        
    except Exception as e:
        print(f"get_datafields_with_progress: å‘ç”Ÿå¼‚å¸¸: {e}")
        # å¦‚æœè‡ªå®šä¹‰æ–¹æ³•å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ–¹æ³•
        return get_datafields(
            session,
            dataset_id=dataset_id,
            region=region,
            universe=universe,
            delay=delay,
            instrument_type=instrument_type
        )

class DatasetField(BaseModel):
    """æ•°æ®é›†å­—æ®µæ¨¡å‹"""
    id: str
    description: str = ""
    type: str = ""

class DatasetFieldsResponse(BaseModel):
    """æ•°æ®é›†å­—æ®µå“åº”æ¨¡å‹"""
    dataset_id: str
    region: str
    universe: str
    delay: int
    total_fields: int
    raw_fields: list[DatasetField]
    fetch_time: Optional[float] = None
    error: Optional[str] = None

class DatasetFieldsProgressDetails(BaseModel):
    """æ•°æ®é›†å­—æ®µè·å–è¿›åº¦è¯¦ç»†ä¿¡æ¯"""
    page_count: int = 0
    current_count: int = 0
    estimated_total: Optional[int] = None
    elapsed_time: float = 0.0
    estimated_remaining_time: Optional[float] = None

class DatasetFieldsProgressResponse(BaseModel):
    """æ•°æ®é›†å­—æ®µè¿›åº¦å“åº”æ¨¡å‹"""
    task_id: str
    status: str  # 'pending', 'running', 'completed', 'failed'
    progress: int  # 0-100
    message: str
    data: Optional[DatasetFieldsResponse] = None
    details: Optional[DatasetFieldsProgressDetails] = None

async def fetch_dataset_fields_async(
    task_id: str,
    dataset_id: str,
    region: str,
    universe: str,
    delay: int,
    instrument_type: str
):
    """å¼‚æ­¥è·å–æ•°æ®é›†å­—æ®µï¼Œæ”¯æŒè¯¦ç»†è¿›åº¦è·Ÿè¸ª"""
    try:
        # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹è·å–
        progress_store[task_id] = {
            'status': 'running',
            'progress': 5,
            'message': f'æ­£åœ¨è¿æ¥åˆ°WorldQuantå¹³å°...',
            'data': None,
            'details': {
                'page_count': 0,
                'current_count': 0,
                'estimated_total': None,
                'elapsed_time': 0,
                'estimated_remaining_time': None
            }
        }
        
        # ä½¿ç”¨session_clientè·å–è½»é‡çº§session
        print(f"ğŸ“¡ ä½¿ç”¨SessionClientè·å–æ•°æ®é›† {dataset_id} çš„å­—æ®µä¿¡æ¯")
        session = get_session()
        if session is None:
            raise Exception("æ— æ³•è·å–æœ‰æ•ˆçš„session")
        
        # æ›´æ–°è¿›åº¦ï¼šå·²è¿æ¥
        progress_store[task_id]['progress'] = 10
        progress_store[task_id]['message'] = f'å·²è¿æ¥ï¼Œå¼€å§‹è¯·æ±‚æ•°æ®é›† {dataset_id} çš„å­—æ®µä¿¡æ¯...'
        
        # è·å–å­—æ®µä¿¡æ¯ï¼ˆä½¿ç”¨è‡ªå®šä¹‰åˆ†é¡µè¿›åº¦è·Ÿè¸ªï¼‰
        start_time = time.time()
        df = await get_datafields_with_progress(
            session,
            dataset_id=dataset_id,
            region=region,
            universe=universe,
            delay=delay,
            instrument_type=instrument_type,
            task_id=task_id
        )
        fetch_time = time.time() - start_time
        
        print(f"âœ… æˆåŠŸè·å–æ•°æ®é›† {dataset_id} çš„ {len(df)} ä¸ªå­—æ®µ (è€—æ—¶: {fetch_time:.2f}s)")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if df.empty:
            result = DatasetFieldsResponse(
                dataset_id=dataset_id,
                region=region,
                universe=universe,
                delay=delay,
                total_fields=0,
                raw_fields=[],
                fetch_time=fetch_time,
                error="æœªè·å–åˆ°ä»»ä½•å­—æ®µä¿¡æ¯"
            )
        else:
            # è½¬æ¢ä¸ºå­—æ®µåˆ—è¡¨
            raw_fields = []
            for _, row in df.iterrows():
                field = DatasetField(
                    id=str(row.get('id', '')),
                    description=str(row.get('description', '')),
                    type=str(row.get('type', ''))
                )
                raw_fields.append(field)
            
            result = DatasetFieldsResponse(
                dataset_id=dataset_id,
                region=region,
                universe=universe,
                delay=delay,
                total_fields=len(raw_fields),
                raw_fields=raw_fields,
                fetch_time=fetch_time
            )
        
        # æ›´æ–°è¿›åº¦ï¼šå®Œæˆ
        if task_id in progress_store:
            progress_store[task_id].update({
                'status': 'completed',
                'progress': 100,
                'message': f'æˆåŠŸè·å– {result.total_fields} ä¸ªå­—æ®µ',
                'data': result
            })
        else:
            progress_store[task_id] = {
                'status': 'completed',
                'progress': 100,
                'message': f'æˆåŠŸè·å– {result.total_fields} ä¸ªå­—æ®µ',
                'data': result,
                'details': {
                    'page_count': 0,
                    'current_count': result.total_fields,
                    'estimated_total': result.total_fields,
                    'elapsed_time': fetch_time,
                    'estimated_remaining_time': 0
                }
            }
        
    except Exception as e:
        # æ›´æ–°è¿›åº¦ï¼šå¤±è´¥
        error_msg = str(e)
        print(f"è·å–æ•°æ®é›†å­—æ®µå¤±è´¥: {error_msg}")
        
        if task_id in progress_store:
            progress_store[task_id].update({
                'status': 'failed',
                'progress': 0,
                'message': f'è·å–å¤±è´¥: {error_msg}',
                'data': DatasetFieldsResponse(
                    dataset_id=dataset_id,
                    region=region,
                    universe=universe,
                    delay=delay,
                    total_fields=0,
                    raw_fields=[],
                    error=error_msg
                )
            })
        else:
            progress_store[task_id] = {
                'status': 'failed',
                'progress': 0,
                'message': f'è·å–å¤±è´¥: {error_msg}',
                'data': DatasetFieldsResponse(
                    dataset_id=dataset_id,
                    region=region,
                    universe=universe,
                    delay=delay,
                    total_fields=0,
                    raw_fields=[],
                    error=error_msg
                ),
                'details': {
                    'page_count': 0,
                    'current_count': 0,
                    'estimated_total': 0,
                    'elapsed_time': 0,
                    'estimated_remaining_time': 0
                }
            }

@router.post("/fields/async", response_model=DatasetFieldsProgressResponse)
async def start_dataset_fields_fetch(
    background_tasks: BackgroundTasks,
    dataset_id: str = Query(..., description="æ•°æ®é›†ID"),
    region: str = Query("USA", description="åœ°åŒº"),
    universe: str = Query("TOP3000", description="universe"),
    delay: int = Query(1, description="å»¶è¿Ÿ", ge=0, le=30),
    instrument_type: str = Query("EQUITY", description="å·¥å…·ç±»å‹")
):
    """
    å¯åŠ¨å¼‚æ­¥è·å–æ•°æ®é›†å­—æ®µä»»åŠ¡
    """
    if get_datafields is None or get_session is None:
        raise HTTPException(
            status_code=500, 
            detail="å¿…éœ€æ¨¡å—æœªæ­£ç¡®åŠ è½½ï¼Œæ— æ³•è·å–æ•°æ®é›†å­—æ®µ"
        )
    
    # ç”Ÿæˆä»»åŠ¡ID
    task_id = str(uuid.uuid4())
    
    # åˆå§‹åŒ–è¿›åº¦
    progress_store[task_id] = {
        'status': 'pending',
        'progress': 0,
        'message': 'ä»»åŠ¡å·²åˆ›å»ºï¼Œç­‰å¾…å¼€å§‹...',
        'data': None
    }
    
    # å¯åŠ¨åå°ä»»åŠ¡
    background_tasks.add_task(
        fetch_dataset_fields_async,
        task_id,
        dataset_id,
        region,
        universe,
        delay,
        instrument_type
    )
    
    return DatasetFieldsProgressResponse(
        task_id=task_id,
        status='pending',
        progress=0,
        message='ä»»åŠ¡å·²åˆ›å»ºï¼Œæ­£åœ¨å¯åŠ¨...',
        data=None
    )

@router.get("/fields/progress/{task_id}", response_model=DatasetFieldsProgressResponse)
async def get_dataset_fields_progress(task_id: str):
    """
    è·å–æ•°æ®é›†å­—æ®µè·å–ä»»åŠ¡çš„è¿›åº¦
    """
    if task_id not in progress_store:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    progress_info = progress_store[task_id]
    
    # æ„é€ è¯¦ç»†ä¿¡æ¯
    details = None
    if 'details' in progress_info:
        details = DatasetFieldsProgressDetails(**progress_info['details'])
    
    return DatasetFieldsProgressResponse(
        task_id=task_id,
        status=progress_info['status'],
        progress=progress_info['progress'],
        message=progress_info['message'],
        data=progress_info['data'],
        details=details
    )

@router.get("/fields", response_model=DatasetFieldsResponse)
async def get_dataset_fields(
    dataset_id: str = Query(..., description="æ•°æ®é›†ID"),
    region: str = Query("USA", description="åœ°åŒº"),
    universe: str = Query("TOP3000", description="universe"),
    delay: int = Query(1, description="å»¶è¿Ÿ", ge=0, le=30),
    instrument_type: str = Query("EQUITY", description="å·¥å…·ç±»å‹")
):
    """
    è·å–æŒ‡å®šæ•°æ®é›†çš„å­—æ®µä¿¡æ¯
    """
    if get_datafields is None or get_session is None:
        raise HTTPException(
            status_code=500, 
            detail="å¿…éœ€æ¨¡å—æœªæ­£ç¡®åŠ è½½ï¼Œæ— æ³•è·å–æ•°æ®é›†å­—æ®µ"
        )
    
    try:
        # ä½¿ç”¨session_clientè·å–è½»é‡çº§session
        print(f"ğŸ“¡ ä½¿ç”¨SessionClientè·å–æ•°æ®é›† {dataset_id} çš„å­—æ®µä¿¡æ¯")
        session = get_session()
        if session is None:
            raise HTTPException(status_code=500, detail="æ— æ³•è·å–æœ‰æ•ˆçš„session")
        
        # è·å–å­—æ®µä¿¡æ¯
        start_time = time.time()
        df = get_datafields(
            session,
            dataset_id=dataset_id,
            region=region,
            universe=universe,
            delay=delay,
            instrument_type=instrument_type
        )
        fetch_time = time.time() - start_time
        
        print(f"âœ… æˆåŠŸè·å–æ•°æ®é›† {dataset_id} çš„ {len(df)} ä¸ªå­—æ®µ (è€—æ—¶: {fetch_time:.2f}s)")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if df.empty:
            return DatasetFieldsResponse(
                dataset_id=dataset_id,
                region=region,
                universe=universe,
                delay=delay,
                total_fields=0,
                raw_fields=[],
                fetch_time=fetch_time,
                error="æœªè·å–åˆ°ä»»ä½•å­—æ®µä¿¡æ¯"
            )
        
        # è½¬æ¢ä¸ºå­—æ®µåˆ—è¡¨
        raw_fields = []
        for _, row in df.iterrows():
            field = DatasetField(
                id=str(row.get('id', '')),
                description=str(row.get('description', '')),
                type=str(row.get('type', ''))
            )
            raw_fields.append(field)
        
        return DatasetFieldsResponse(
            dataset_id=dataset_id,
            region=region,
            universe=universe,
            delay=delay,
            total_fields=len(raw_fields),
            raw_fields=raw_fields,
            fetch_time=fetch_time
        )
        
    except Exception as e:
        # è®°å½•é”™è¯¯ä¿¡æ¯
        error_msg = str(e)
        print(f"è·å–æ•°æ®é›†å­—æ®µå¤±è´¥: {error_msg}")
        
        return DatasetFieldsResponse(
            dataset_id=dataset_id,
            region=region,
            universe=universe,
            delay=delay,
            total_fields=0,
            raw_fields=[],
            error=error_msg
        )
