#!/usr/bin/env python3
"""
æ•°æ®åº“åˆ†åº“è¿ç§»è„šæœ¬
ä½œè€…ï¼šAssistant  
æ—¥æœŸï¼š2025.09.05

åŠŸèƒ½ï¼š
- å°†ä¸»æ•°æ®åº“ä¸­çš„factor_expressionsè¡¨æ•°æ®è¿ç§»åˆ°æ•°æ®é›†åˆ†åº“
- ä¿ç•™ä¸»æ•°æ®åº“ä¸­çš„å…¶ä»–è¡¨
- æä¾›è¿ç§»å‰åçš„æ€§èƒ½å¯¹æ¯”
"""

import os
import sys
import time
import argparse
from typing import List, Dict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

from database.partitioned_db_manager import PartitionedFactorManager
from database.db_manager import FactorDatabaseManager


def performance_test(db_manager, dataset_id: str, region: str, step: int, test_name: str):
    """æ€§èƒ½æµ‹è¯•"""
    print(f"ğŸ”¬ {test_name} æ€§èƒ½æµ‹è¯•...")
    
    # æµ‹è¯•æŸ¥è¯¢æ€§èƒ½
    start_time = time.time()
    expressions = db_manager.get_factor_expressions(dataset_id, region, step)
    query_time = time.time() - start_time
    
    print(f"  ğŸ“Š æŸ¥è¯¢ç»“æœ: {len(expressions)} æ¡è®°å½•")
    print(f"  â±ï¸  æŸ¥è¯¢æ—¶é—´: {query_time:.4f} ç§’")
    
    if len(expressions) > 0:
        # æµ‹è¯•å­˜åœ¨æ€§æ£€æŸ¥
        test_expr = expressions[len(expressions)//2]
        start_time = time.time()
        exists = db_manager.is_expression_exists(test_expr, dataset_id, region, step)
        check_time = time.time() - start_time
        print(f"  ğŸ” å­˜åœ¨æ€§æ£€æŸ¥: {check_time:.4f} ç§’ (ç»“æœ: {exists})")
    
    return query_time


def main():
    parser = argparse.ArgumentParser(description='æ•°æ®åº“åˆ†åº“è¿ç§»å·¥å…·')
    parser.add_argument('--db-path', default='database/factors.db', 
                       help='ä¸»æ•°æ®åº“è·¯å¾„ (é»˜è®¤: database/factors.db)')
    parser.add_argument('--datasets', nargs='*', 
                       help='æŒ‡å®šè¦è¿ç§»çš„æ•°æ®é›†IDåˆ—è¡¨ (é»˜è®¤: å…¨éƒ¨)')
    parser.add_argument('--test-performance', action='store_true',
                       help='è¿ç§»å‰åè¿›è¡Œæ€§èƒ½æµ‹è¯•')
    parser.add_argument('--cleanup-main', action='store_true',
                       help='è¿ç§»åæ¸…ç†ä¸»æ•°æ®åº“ä¸­çš„factor_expressionsæ•°æ®')
    parser.add_argument('--dry-run', action='store_true',
                       help='åªæ˜¾ç¤ºè¿ç§»è®¡åˆ’ï¼Œä¸æ‰§è¡Œå®é™…è¿ç§»')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶
    if not os.path.exists(args.db_path):
        print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {args.db_path}")
        return 1
    
    print("ğŸš€ æ•°æ®åº“åˆ†åº“è¿ç§»å·¥å…·")
    print("=" * 50)
    
    # åˆå§‹åŒ–ç®¡ç†å™¨
    print(f"ğŸ“‚ ä¸»æ•°æ®åº“: {args.db_path}")
    main_db = FactorDatabaseManager(args.db_path)
    partitioned_db = PartitionedFactorManager(args.db_path)
    
    try:
        # 1. åˆ†æå½“å‰æ•°æ®
        print("\nğŸ“Š åˆ†æå½“å‰æ•°æ®...")
        
        with main_db.get_connection() as conn:
            # è·å–æ•°æ®é›†ç»Ÿè®¡
            cursor = conn.execute("""
                SELECT dataset_id, region, step, COUNT(*) as count
                FROM factor_expressions 
                GROUP BY dataset_id, region, step
                ORDER BY dataset_id, region, step
            """)
            
            dataset_stats = {}
            total_records = 0
            
            for row in cursor.fetchall():
                dataset_id, region, step, count = row
                if dataset_id not in dataset_stats:
                    dataset_stats[dataset_id] = {}
                if region not in dataset_stats[dataset_id]:
                    dataset_stats[dataset_id][region] = {}
                dataset_stats[dataset_id][region][step] = count
                total_records += count
        
        print(f"ğŸ“ˆ æ€»è®°å½•æ•°: {total_records:,}")
        print("ğŸ“‹ æ•°æ®é›†åˆ†å¸ƒ:")
        
        target_datasets = args.datasets if args.datasets else list(dataset_stats.keys())
        
        for dataset_id in sorted(dataset_stats.keys()):
            if dataset_id in target_datasets:
                dataset_total = sum(
                    sum(steps.values()) 
                    for steps in dataset_stats[dataset_id].values()
                )
                print(f"  âœ… {dataset_id}: {dataset_total:,} æ¡è®°å½•")
                
                for region in sorted(dataset_stats[dataset_id].keys()):
                    for step in sorted(dataset_stats[dataset_id][region].keys()):
                        count = dataset_stats[dataset_id][region][step]
                        print(f"    ğŸ“ {region}-Step{step}: {count:,} æ¡")
            else:
                dataset_total = sum(
                    sum(steps.values()) 
                    for steps in dataset_stats[dataset_id].values()
                )
                print(f"  â­ï¸  {dataset_id}: {dataset_total:,} æ¡è®°å½• (è·³è¿‡)")
        
        if args.dry_run:
            print("\nğŸ” æ¨¡æ‹Ÿè¿è¡Œæ¨¡å¼ - ä¸æ‰§è¡Œå®é™…è¿ç§»")
            return 0
        
        # 2. æ€§èƒ½æµ‹è¯•ï¼ˆè¿ç§»å‰ï¼‰
        if args.test_performance and target_datasets:
            print("\nğŸ”¬ è¿ç§»å‰æ€§èƒ½æµ‹è¯•...")
            test_dataset = target_datasets[0]
            test_regions = list(dataset_stats[test_dataset].keys())
            test_region = test_regions[0] if test_regions else 'USA'
            test_steps = list(dataset_stats[test_dataset][test_region].keys())
            test_step = test_steps[0] if test_steps else 1
            
            old_time = performance_test(main_db, test_dataset, test_region, test_step, "ä¸»æ•°æ®åº“")
        
        # 3. æ‰§è¡Œè¿ç§»
        print(f"\nğŸ”„ å¼€å§‹è¿ç§» {len(target_datasets)} ä¸ªæ•°æ®é›†...")
        
        migration_stats = partitioned_db.migrate_from_main_db(target_datasets)
        
        print("\nâœ… è¿ç§»å®Œæˆ!")
        print("ğŸ“Š è¿ç§»ç»Ÿè®¡:")
        total_migrated = 0
        for dataset_id, count in migration_stats.items():
            print(f"  {dataset_id}: {count:,} æ¡è®°å½•")
            total_migrated += count
        print(f"ğŸ¯ æ€»è®¡è¿ç§»: {total_migrated:,} æ¡è®°å½•")
        
        # 4. éªŒè¯è¿ç§»ç»“æœ
        print("\nğŸ” éªŒè¯è¿ç§»ç»“æœ...")
        partition_stats = partitioned_db.get_partition_stats()
        
        for dataset_id in target_datasets:
            if dataset_id in partition_stats:
                info = partition_stats[dataset_id]
                if 'error' not in info:
                    print(f"  âœ… {dataset_id}: {info['total_expressions']:,} æ¡è®°å½•, {info['db_size_mb']} MB")
                else:
                    print(f"  âŒ {dataset_id}: {info['error']}")
        
        # 5. æ€§èƒ½æµ‹è¯•ï¼ˆè¿ç§»åï¼‰
        if args.test_performance and target_datasets:
            print("\nğŸ”¬ è¿ç§»åæ€§èƒ½æµ‹è¯•...")
            new_time = performance_test(partitioned_db, test_dataset, test_region, test_step, "åˆ†åº“æ•°æ®åº“")
            
            if old_time > 0:
                improvement = ((old_time - new_time) / old_time) * 100
                print(f"\nğŸ“ˆ æ€§èƒ½æå‡: {improvement:.1f}% (ä» {old_time:.4f}s åˆ° {new_time:.4f}s)")
        
        # 6. æ¸…ç†ä¸»æ•°æ®åº“ï¼ˆå¯é€‰ï¼‰
        if args.cleanup_main:
            print("\nğŸ§¹ æ¸…ç†ä¸»æ•°æ®åº“...")
            
            response = input("âš ï¸  ç¡®è®¤è¦ä»ä¸»æ•°æ®åº“åˆ é™¤å·²è¿ç§»çš„factor_expressionsæ•°æ®å—? (y/N): ")
            if response.lower() == 'y':
                deleted_count = partitioned_db.cleanup_main_db_expressions(target_datasets)
                print(f"âœ… æ¸…ç†å®Œæˆ: åˆ é™¤ {deleted_count:,} æ¡è®°å½•")
            else:
                print("â­ï¸  è·³è¿‡æ¸…ç†æ­¥éª¤")
        
        print("\nğŸ‰ è¿ç§»æµç¨‹å®Œæˆ!")
        print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        print("  1. ä¿®æ”¹ä»£ç ä½¿ç”¨ PartitionedFactorManager æ›¿ä»£ FactorDatabaseManager")
        print("  2. ç›‘æ§åˆ†åº“æ€§èƒ½å’Œå­˜å‚¨ä½¿ç”¨æƒ…å†µ")
        print("  3. è€ƒè™‘å®šæœŸå‹ç¼©æ•°æ®åº“æ–‡ä»¶")
        
    except Exception as e:
        print(f"\nâŒ è¿ç§»å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    finally:
        # æ¸…ç†è¿æ¥
        partitioned_db.close_all_connections()
    
    return 0


if __name__ == "__main__":
    exit(main())
