#!/usr/bin/env python3
"""
ä½œè€…ï¼še.e.
æ—¥æœŸï¼š2025.09.10
åŠŸèƒ½ï¼šå°†SQLiteæ•°æ®åº“æ•°æ®å¯¼å‡ºåˆ°æ–‡æœ¬æ–‡ä»¶æ ¼å¼
"""

import os
import sys
import sqlite3
import pandas as pd
import json
from datetime import datetime
from pathlib import Path

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from config import RECORDS_PATH, ROOT_PATH

class FactorDataExporter:
    def __init__(self, db_path='database/factors.db'):
        """åˆå§‹åŒ–å¯¼å‡ºå™¨"""
        self.db_path = os.path.join(ROOT_PATH, db_path)
        self.records_path = RECORDS_PATH
        self.conn = None
        
        # ç¡®ä¿å¯¼å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.records_path, exist_ok=True)
        
    def connect_db(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.conn = sqlite3.connect(self.db_path)
            self.conn.execute("PRAGMA foreign_keys = ON")
            print(f"âœ… æˆåŠŸè¿æ¥æ•°æ®åº“: {self.db_path}")
            return True
        except Exception as e:
            print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return False
            
    def export_factor_expressions(self):
        """å¯¼å‡ºå› å­è¡¨è¾¾å¼æ•°æ®"""
        print("\nğŸ”„ å¼€å§‹å¯¼å‡ºå› å­è¡¨è¾¾å¼æ•°æ®...")
        total_exported = 0
        
        try:
            # æŸ¥è¯¢æ‰€æœ‰è¡¨è¾¾å¼æ•°æ®ï¼ŒæŒ‰æ•°æ®é›†åˆ†ç»„
            cursor = self.conn.execute("""
                SELECT dataset_id, region, step, expression 
                FROM factor_expressions 
                ORDER BY dataset_id, region, step, created_at
            """)
            
            # æŒ‰æ–‡ä»¶åˆ†ç»„æ•°æ®
            file_groups = {}
            for row in cursor.fetchall():
                dataset_id, region, step, expression = row
                filename = f"{dataset_id}_{region.lower()}_{step}step_simulated_alpha_expression.txt"
                
                if filename not in file_groups:
                    file_groups[filename] = []
                file_groups[filename].append(expression)
            
            # å†™å…¥æ–‡ä»¶
            for filename, expressions in file_groups.items():
                file_path = os.path.join(self.records_path, filename)
                try:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        for expression in expressions:
                            f.write(expression + '\n')
                    
                    count = len(expressions)
                    total_exported += count
                    print(f"âœ… {filename}: å¯¼å‡º {count} æ¡è¡¨è¾¾å¼")
                    
                except Exception as e:
                    print(f"âŒ å¯¼å‡ºæ–‡ä»¶ {filename} å¤±è´¥: {e}")
            
            print(f"ğŸ“Š å› å­è¡¨è¾¾å¼å¯¼å‡ºå®Œæˆï¼Œæ€»è®¡: {total_exported} æ¡")
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºå› å­è¡¨è¾¾å¼å¤±è´¥: {e}")
        
    def export_checked_alphas(self):
        """å¯¼å‡ºå·²æ£€æŸ¥å› å­æ•°æ®"""
        print("\nğŸ”„ å¼€å§‹å¯¼å‡ºå·²æ£€æŸ¥å› å­æ•°æ®...")
        total_exported = 0
        
        try:
            # æŸ¥è¯¢æ‰€æœ‰å·²æ£€æŸ¥æ•°æ®ï¼ŒæŒ‰æ•°æ®é›†åˆ†ç»„
            cursor = self.conn.execute("""
                SELECT dataset_id, region, step, alpha_id 
                FROM checked_alphas 
                ORDER BY dataset_id, region, step, checked_at
            """)
            
            # æŒ‰æ–‡ä»¶åˆ†ç»„æ•°æ®
            file_groups = {}
            for row in cursor.fetchall():
                dataset_id, region, step, alpha_id = row
                filename = f"{dataset_id}_{region.lower()}_{step}step_checked_alpha_id.txt"
                
                if filename not in file_groups:
                    file_groups[filename] = []
                file_groups[filename].append(alpha_id)
            
            # å†™å…¥æ–‡ä»¶
            for filename, alpha_ids in file_groups.items():
                file_path = os.path.join(self.records_path, filename)
                try:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        for alpha_id in alpha_ids:
                            f.write(alpha_id + '\n')
                    
                    count = len(alpha_ids)
                    total_exported += count
                    print(f"âœ… {filename}: å¯¼å‡º {count} ä¸ªAlpha ID")
                    
                except Exception as e:
                    print(f"âŒ å¯¼å‡ºæ–‡ä»¶ {filename} å¤±è´¥: {e}")
            
            print(f"ğŸ“Š å·²æ£€æŸ¥å› å­å¯¼å‡ºå®Œæˆï¼Œæ€»è®¡: {total_exported} æ¡")
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºå·²æ£€æŸ¥å› å­å¤±è´¥: {e}")
        
    def export_submitable_alphas(self):
        """å¯¼å‡ºå¯æäº¤å› å­æ•°æ®"""
        print("\nğŸ”„ å¼€å§‹å¯¼å‡ºå¯æäº¤å› å­æ•°æ®...")
        
        try:
            # æŸ¥è¯¢æ‰€æœ‰å¯æäº¤å› å­æ•°æ®
            df = pd.read_sql_query("""
                SELECT * FROM submitable_alphas 
                ORDER BY created_at
            """, self.conn)
            
            if df.empty:
                print("âš ï¸  æ•°æ®åº“ä¸­æ²¡æœ‰å¯æäº¤å› å­æ•°æ®")
                return
            
            # è½¬æ¢åˆ—åï¼šä¸‹åˆ’çº¿è½¬é©¼å³°ï¼ˆåå‘æ˜ å°„ï¼‰
            column_mapping = {}
            for col in df.columns:
                # å°†ä¸‹åˆ’çº¿å‘½åè½¬æ¢ä¸ºé©¼å³°å‘½å
                camel_case = ''.join(word.capitalize() if i > 0 else word for i, word in enumerate(col.split('_')))
                column_mapping[col] = camel_case
            
            df = df.rename(columns=column_mapping)
            
            # ç‰¹æ®Šå¤„ç†ï¼šalpha_id -> id
            if 'alpha_id' in df.columns:
                df = df.rename(columns={'alpha_id': 'id'})
            
            # å¤„ç†å¤æ‚å­—æ®µï¼šJSONå­—ç¬¦ä¸²è½¬å›å¯¹è±¡
            complex_fields = ['tags', 'checks', 'os', 'train', 'test', 'prod', 
                           'competitions', 'themes', 'team', 'pyramids', 'classifications']
            
            for field in complex_fields:
                if field in df.columns:
                    # å°è¯•è§£æJSONå­—ç¬¦ä¸²
                    def parse_json(value):
                        if pd.isna(value) or value == '':
                            return []
                        try:
                            return json.loads(value)
                        except:
                            return value
                    
                    df[field] = df[field].apply(parse_json)
            
            # å†™å…¥CSVæ–‡ä»¶
            csv_path = os.path.join(self.records_path, 'submitable_alpha.csv')
            df.to_csv(csv_path, index=False)
            
            count = len(df)
            print(f"âœ… å¯æäº¤å› å­å¯¼å‡ºå®Œæˆ: {count} æ¡ -> submitable_alpha.csv")
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¯æäº¤å› å­å¤±è´¥: {e}")
            
    def export_config(self):
        """å¯¼å‡ºé…ç½®æ•°æ®"""
        print("\nğŸ”„ å¼€å§‹å¯¼å‡ºé…ç½®æ•°æ®...")
        
        try:
            # æŸ¥è¯¢ç³»ç»Ÿé…ç½®
            cursor = self.conn.execute("""
                SELECT config_key, config_value FROM system_config
            """)
            
            configs = dict(cursor.fetchall())
            
            # å¯¼å‡ºå¼€å§‹æ—¥æœŸ
            if 'start_date' in configs:
                start_date_path = os.path.join(self.records_path, 'start_date.txt')
                with open(start_date_path, 'w', encoding='utf-8') as f:
                    f.write(configs['start_date'])
                print(f"âœ… å¼€å§‹æ—¥æœŸé…ç½®å¯¼å‡ºå®Œæˆ: {configs['start_date']}")
            
            # å¯¼å‡ºå…¶ä»–é…ç½®ï¼ˆå¯é€‰ï¼‰
            for key, value in configs.items():
                if key != 'start_date':
                    config_file = os.path.join(self.records_path, f'{key}.txt')
                    with open(config_file, 'w', encoding='utf-8') as f:
                        f.write(str(value))
                    print(f"âœ… é…ç½® {key} å¯¼å‡ºå®Œæˆ")
                    
        except Exception as e:
            print(f"âŒ å¯¼å‡ºé…ç½®æ•°æ®å¤±è´¥: {e}")
        
    def export_database_stats(self):
        """å¯¼å‡ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯:")
        
        try:
            # æŸ¥è¯¢å„è¡¨æ•°æ®é‡
            tables = ['factor_expressions', 'checked_alphas', 'submitable_alphas', 'system_config']
            
            for table in tables:
                cursor = self.conn.execute(f"SELECT COUNT(*) FROM {table}")
                count = cursor.fetchone()[0]
                print(f"  {table}: {count} æ¡è®°å½•")
            
            # æ˜¾ç¤ºæ•°æ®åˆ†å¸ƒç»Ÿè®¡
            print("\nğŸ“ˆ æ•°æ®åˆ†å¸ƒç»Ÿè®¡:")
            cursor = self.conn.execute("SELECT * FROM system_overview")
            for row in cursor.fetchall():
                print(f"  {row[0]}: {row[1]} æ¡è®°å½•ï¼Œæœ€æ–°æ›´æ–°: {row[2]}")
                
        except Exception as e:
            print(f"âŒ è·å–æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {e}")
            
    def create_backup_info(self):
        """åˆ›å»ºå¤‡ä»½ä¿¡æ¯æ–‡ä»¶"""
        try:
            backup_info = {
                "export_time": datetime.now().isoformat(),
                "database_path": self.db_path,
                "records_path": self.records_path,
                "export_type": "database_to_files",
                "description": "ä»SQLiteæ•°æ®åº“å¯¼å‡ºåˆ°æ–‡æœ¬æ–‡ä»¶æ ¼å¼"
            }
            
            backup_info_path = os.path.join(self.records_path, 'export_info.json')
            with open(backup_info_path, 'w', encoding='utf-8') as f:
                json.dump(backup_info, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… å¯¼å‡ºä¿¡æ¯å·²ä¿å­˜åˆ°: export_info.json")
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºå¤‡ä»½ä¿¡æ¯å¤±è´¥: {e}")
            
    def run_export(self):
        """æ‰§è¡Œå®Œæ•´å¯¼å‡ºæµç¨‹"""
        print("ğŸš€ å¼€å§‹æ•°æ®å¯¼å‡º...")
        
        if not self.connect_db():
            return False
            
        try:
            # 1. å¯¼å‡ºå„ç±»æ•°æ®
            self.export_factor_expressions()
            self.export_checked_alphas()
            self.export_submitable_alphas()
            self.export_config()
            
            # 2. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            self.export_database_stats()
            
            # 3. åˆ›å»ºå¤‡ä»½ä¿¡æ¯
            self.create_backup_info()
            
            print("\nâœ… æ•°æ®å¯¼å‡ºå®Œæˆï¼")
            print(f"ğŸ“ å¯¼å‡ºä½ç½®: {self.records_path}")
            print("ğŸ“ æ³¨æ„: å¯¼å‡ºçš„æ–‡ä»¶å¯ä»¥ç”¨äºæœåŠ¡å™¨è¿ç§»æˆ–æ•°æ®å¤‡ä»½")
            
            return True
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            return False
            
        finally:
            if self.conn:
                self.conn.close()

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("  WorldQuant å› å­ç³»ç»Ÿæ•°æ®å¯¼å‡ºå·¥å…·")
    print("  ä»SQLiteæ•°æ®åº“å¯¼å‡ºåˆ°æ–‡æœ¬æ–‡ä»¶æ ¼å¼")
    print("=" * 60)
    
    exporter = FactorDataExporter()
    success = exporter.run_export()
    
    if success:
        print("\nğŸ‰ å¯¼å‡ºæˆåŠŸï¼æ•°æ®å·²ä¿å­˜åˆ°æ–‡æœ¬æ–‡ä»¶æ ¼å¼ã€‚")
        print("ğŸ“‹ å¯¼å‡ºæ–‡ä»¶åŒ…æ‹¬:")
        print("  â€¢ *_simulated_alpha_expression.txt - å› å­è¡¨è¾¾å¼")
        print("  â€¢ *_checked_alpha_id.txt - å·²æ£€æŸ¥å› å­")
        print("  â€¢ submitable_alpha.csv - å¯æäº¤å› å­")
        print("  â€¢ start_date.txt - å¼€å§‹æ—¥æœŸé…ç½®")
        print("  â€¢ export_info.json - å¯¼å‡ºä¿¡æ¯")
    else:
        print("\nğŸ’¥ å¯¼å‡ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶é‡è¯•ã€‚")
        sys.exit(1)

if __name__ == "__main__":
    main() 